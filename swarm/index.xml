<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Swarms on Docker Docs</title>
    <link>http://docs.docker.com/swarm/</link>
    <description>Recent content in Swarms on Docker Docs</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://docs.docker.com/swarm/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Create a swarm for development</title>
      <link>http://docs.docker.com/swarm/install-manual/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://docs.docker.com/swarm/install-manual/</guid>
      <description>

&lt;h1 id=&#34;create-a-swarm-for-development&#34;&gt;Create a swarm for development&lt;/h1&gt;

&lt;p&gt;This section tells you how to create a Docker Swarm on your network to use only for debugging, testing, or development purposes. You can also use this type of installation if you are developing custom applications for Docker Swarm or contributing to it.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Caution&lt;/strong&gt;: Only use this set up if your network environment is secured by a firewall or other measures.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;p&gt;You install Docker Swarm on a single system which is known as your Docker Swarm
manager. You create the cluster, or swarm, on one or more additional nodes on
your network.  Each node in your swarm must:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;be accessible by the swarm manager across your network&lt;/li&gt;
&lt;li&gt;have Docker Engine 1.6.0+ installed&lt;/li&gt;
&lt;li&gt;open a TCP port to listen for the manager&lt;/li&gt;
&lt;li&gt;&lt;em&gt;do not&lt;/em&gt; install on a VM or from an image created through cloning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Docker generates a unique ID for the Engine that is located in the
&lt;code&gt;/etc/docker/key.json&lt;/code&gt; file. If a VM is cloned from an instance where a
Docker daemon was previously pre-installed, Swarm will be unable to differentiate
among the remote Docker engines. This is because the cloning process copied the
the identical ID to each image and the ID is no longer unique.&lt;/p&gt;

&lt;p&gt;If you forget this restriction and create a node anyway, Swarm displays a single
Docker engine as registered. To workaround this problem, you can generate a new
ID for each node with affected by this issue. To do this stop the daemon on a node,
delete its &lt;code&gt;/etc/docker/key.json&lt;/code&gt; file, and restart the daemon.&lt;/p&gt;

&lt;p&gt;You can run Docker Swarm on Linux 64-bit architectures. You can also install and
run it on 64-bit Windows and Max OSX but these architectures are &lt;em&gt;not&lt;/em&gt; regularly
tested for compatibility.&lt;/p&gt;

&lt;p&gt;Take a moment and identify the systems on your network that you intend to use.
Ensure each node meets the requirements listed above.&lt;/p&gt;

&lt;h2 id=&#34;pull-the-swarm-image-and-create-a-cluster&#34;&gt;Pull the swarm image and create a cluster.&lt;/h2&gt;

&lt;p&gt;The easiest way to get started with Swarm is to use the
&lt;a href=&#34;https://registry.hub.docker.com/_/swarm/&#34;&gt;official Docker image&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Pull the swarm image.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker pull swarm
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create a Swarm cluster using the &lt;code&gt;docker&lt;/code&gt; command.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run --rm swarm create
6856663cdefdec325839a4b7e1de38e8 # 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;create&lt;/code&gt; command returns a unique cluster ID (&lt;code&gt;cluster_id&lt;/code&gt;). You&amp;rsquo;ll need
this ID when starting the Docker Swarm agent on a node.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;create-swarm-nodes&#34;&gt;Create swarm nodes&lt;/h2&gt;

&lt;p&gt;Each Swarm node will run a Swarm node agent. The agent registers the referenced
Docker daemon, monitors it, and updates the discovery backend with the node&amp;rsquo;s status.&lt;/p&gt;

&lt;p&gt;This example uses the Docker Hub based &lt;code&gt;token&lt;/code&gt; discovery service (only for testing/dev, not for production).
Log into &lt;strong&gt;each node&lt;/strong&gt; and do the following.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Start the Docker daemon with the &lt;code&gt;-H&lt;/code&gt; flag. This ensures that the
Docker remote API on &lt;em&gt;Swarm Agents&lt;/em&gt; is available over TCP for the
&lt;em&gt;Swarm Manager&lt;/em&gt;, as well as the standard unix socket which is
available in default docker installs.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker daemon -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: versions of docker prior to 1.8 used the &lt;code&gt;-d&lt;/code&gt; flag instead of the &lt;code&gt;docker daemon&lt;/code&gt; subcommand.&lt;/p&gt;
&lt;/blockquote&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Register the Swarm agents to the discovery service. The node&amp;rsquo;s IP must be accessible from the Swarm Manager. Use the following command and replace with the proper &lt;code&gt;node_ip&lt;/code&gt; and &lt;code&gt;cluster_id&lt;/code&gt; to start an agent:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d swarm join --addr=&amp;lt;node_ip:2375&amp;gt; token://&amp;lt;cluster_id&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d swarm join --addr=172.31.40.100:2375 token://6856663cdefdec325839a4b7e1de38e8
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;configure-a-manager&#34;&gt;Configure a manager&lt;/h2&gt;

&lt;p&gt;Once you have your nodes established, set up a manager to control the swarm.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Start the Swarm manager on any machine or your laptop.&lt;/p&gt;

&lt;p&gt;The following command illustrates how to do this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d -p &amp;lt;manager_port&amp;gt;:2375 swarm manage token://&amp;lt;cluster_id&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The manager is exposed and listening on &lt;code&gt;&amp;lt;manager_port&amp;gt;&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Once the manager is running, check your configuration by running &lt;code&gt;docker info&lt;/code&gt; as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker -H tcp://&amp;lt;manager_ip:manager_port&amp;gt; info
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For example, if you run the manager locally on your machine:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H tcp://0.0.0.0:2375 info
Containers: 0
Nodes: 3
 agent-2: 172.31.40.102:2375
    └ Containers: 0
    └ Reserved CPUs: 0 / 1
    └ Reserved Memory: 0 B / 514.5 MiB
 agent-1: 172.31.40.101:2375
    └ Containers: 0
    └ Reserved CPUs: 0 / 1
    └ Reserved Memory: 0 B / 514.5 MiB
 agent-0: 172.31.40.100:2375
    └ Containers: 0
    └ Reserved CPUs: 0 / 1
    └ Reserved Memory: 0 B / 514.5 MiB
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you are running a test cluster without TLS enabled, you may get an error.
  In that case, be sure to unset &lt;code&gt;DOCKER_TLS_VERIFY&lt;/code&gt; with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ unset DOCKER_TLS_VERIFY
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;using-the-docker-cli&#34;&gt;Using the docker CLI&lt;/h2&gt;

&lt;p&gt;You can now use the regular Docker CLI to access your nodes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker -H tcp://&amp;lt;manager_ip:manager_port&amp;gt; info
docker -H tcp://&amp;lt;manager_ip:manager_port&amp;gt; run ...
docker -H tcp://&amp;lt;manager_ip:manager_port&amp;gt; ps
docker -H tcp://&amp;lt;manager_ip:manager_port&amp;gt; logs ...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;list-nodes-in-your-cluster&#34;&gt;List nodes in your cluster&lt;/h2&gt;

&lt;p&gt;You can get a list of all your running nodes using the &lt;code&gt;swarm list&lt;/code&gt; command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run --rm swarm list token://&amp;lt;cluster_id&amp;gt;
&amp;lt;node_ip:2375&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run --rm swarm list token://6856663cdefdec325839a4b7e1de38e8
172.31.40.100:2375
172.31.40.101:2375
172.31.40.102:2375
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;tls&#34;&gt;TLS&lt;/h2&gt;

&lt;p&gt;Swarm supports TLS authentication between the CLI and Swarm but also between
Swarm and the Docker nodes. &lt;em&gt;However&lt;/em&gt;, all the Docker daemon certificates and client
certificates &lt;strong&gt;must&lt;/strong&gt; be signed using the same CA-certificate.&lt;/p&gt;

&lt;p&gt;In order to enable TLS for both client and server, the same command line options
as Docker can be specified:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm manage --tlsverify --tlscacert=&amp;lt;CACERT&amp;gt; --tlscert=&amp;lt;CERT&amp;gt; --tlskey=&amp;lt;KEY&amp;gt; [...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Please refer to the &lt;a href=&#34;https://docs.docker.com/articles/https/&#34;&gt;Docker documentation&lt;/a&gt;
for more information on how to set up TLS authentication on Docker and generating
the certificates.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Swarm certificates must be generated with &lt;code&gt;extendedKeyUsage = clientAuth,serverAuth&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Docker Swarm</title>
      <link>http://docs.docker.com/swarm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://docs.docker.com/swarm/</guid>
      <description>

&lt;h1 id=&#34;docker-swarm-overview&#34;&gt;Docker Swarm overview&lt;/h1&gt;

&lt;p&gt;Docker Swarm is native clustering for Docker. It turns a pool of Docker hosts
into a single, virtual Docker host. Because Docker Swarm serves the standard
Docker API, any tool that already communicates with a Docker daemon can use
Swarm to transparently scale to multiple hosts. Supported tools include, but
are not limited to, the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Dokku&lt;/li&gt;
&lt;li&gt;Docker Compose&lt;/li&gt;
&lt;li&gt;Krane&lt;/li&gt;
&lt;li&gt;Jenkins&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And of course, the Docker client itself is also supported.&lt;/p&gt;

&lt;p&gt;Like other Docker projects, Docker Swarm follows the &amp;ldquo;swap, plug, and play&amp;rdquo;
principle. As initial development settles, an API will develop to enable
pluggable backends.  This means you can swap out the scheduling backend
Docker Swarm uses out-of-the-box with a backend you prefer. Swarm&amp;rsquo;s swappable design provides a smooth out-of-box experience for most use cases, and allows large-scale production deployments to swap for more powerful backends, like Mesos.&lt;/p&gt;

&lt;h2 id=&#34;understand-swarm-creation&#34;&gt;Understand swarm creation&lt;/h2&gt;

&lt;p&gt;The first step to creating a swarm on your network is to pull the Docker Swarm image. Then, using Docker, you configure the swarm manager and all the nodes to run Docker Swarm. This method requires that you:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;open a TCP port on each node for communication with the swarm manager&lt;/li&gt;
&lt;li&gt;install Docker on each node&lt;/li&gt;
&lt;li&gt;create and manage TLS certificates to secure your swarm&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As a starting point, the manual method is best suited for experienced administrators or programmers contributing to Docker Swarm. The alternative is to use &lt;code&gt;docker-machine&lt;/code&gt; to install a swarm.&lt;/p&gt;

&lt;p&gt;Using Docker Machine, you can quickly install a Docker Swarm on cloud providers or inside your own data center. If you have VirtualBox installed on your local machine, you can quickly build and explore Docker Swarm in your local environment. This method automatically generates a certificate to secure your swarm.&lt;/p&gt;

&lt;p&gt;Using Docker Machine is the best method for users getting started with Swarm for the first time. To try the recommended method of getting started, see &lt;a href=&#34;../swarm/install-w-machine/&#34;&gt;Get Started with Docker Swarm&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you are interested manually installing or interested in contributing, see &lt;a href=&#34;../swarm/install-manual/&#34;&gt;Create a swarm for development&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;discovery-services&#34;&gt;Discovery services&lt;/h2&gt;

&lt;p&gt;To dynamically configure and manage the services in your containers, you use a discovery backend with Docker Swarm. For information on which backends are available, see the &lt;a href=&#34;../swarm/discovery/&#34;&gt;Discovery service&lt;/a&gt; documentation.&lt;/p&gt;

&lt;h2 id=&#34;advanced-scheduling&#34;&gt;Advanced Scheduling&lt;/h2&gt;

&lt;p&gt;To learn more about advanced scheduling, see the
&lt;a href=&#34;../swarm/scheduler/strategy/&#34;&gt;strategies&lt;/a&gt; and &lt;a href=&#34;../swarm/scheduler/filter/&#34;&gt;filters&lt;/a&gt;
documents.&lt;/p&gt;

&lt;h2 id=&#34;swarm-api&#34;&gt;Swarm API&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&#34;../swarm/api/swarm-api/&#34;&gt;Docker Swarm API&lt;/a&gt; is compatible with
the &lt;a href=&#34;http://docs.docker.com/reference/api/docker_remote_api/&#34;&gt;Docker remote
API&lt;/a&gt;, and extends it
with some new endpoints.&lt;/p&gt;

&lt;h1 id=&#34;getting-help&#34;&gt;Getting help&lt;/h1&gt;

&lt;p&gt;Docker Swarm is still in its infancy and under active development. If you need
help, would like to contribute, or simply want to talk about the project with
like-minded individuals, we have a number of open channels for communication.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To report bugs or file feature requests: please use the &lt;a href=&#34;https://github.com/docker/swarm/issues&#34;&gt;issue tracker on Github&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;To talk about the project with people in real time: please join the &lt;code&gt;#docker-swarm&lt;/code&gt; channel on IRC.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;To contribute code or documentation changes: please submit a &lt;a href=&#34;https://github.com/docker/swarm/pulls&#34;&gt;pull request on Github&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more information and resources, please visit the &lt;a href=&#34;https://docs.docker.com/project/get-help/&#34;&gt;Getting Help project page&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Docker Swarm</title>
      <link>http://docs.docker.com/swarm/install-w-machine/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://docs.docker.com/swarm/install-w-machine/</guid>
      <description>

&lt;h1 id=&#34;install-and-create-a-docker-swarm&#34;&gt;Install and Create a Docker Swarm&lt;/h1&gt;

&lt;p&gt;You use Docker Swarm to host and schedule a cluster of Docker containers. This section introduces you to Docker Swarm by teaching you how to create a swarm
on your local machine using Docker Machine and VirtualBox.&lt;/p&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;p&gt;Make sure your local system has VirtualBox installed. If you are using Mac OS X
or Windows and have installed Docker, you should have VirtualBox already
installed.&lt;/p&gt;

&lt;p&gt;Using the instructions appropriate to your system architecture, &lt;a href=&#34;http://docs.docker.com/machine/install-machine&#34;&gt;install Docker
Machine&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;create-a-docker-swarm&#34;&gt;Create a Docker Swarm&lt;/h2&gt;

&lt;p&gt;Docker Machine gets hosts ready to run Docker containers. Each node in your
Docker Swarm must have access to Docker to pull images and run them in
containers. Docker Machine manages all this provisioning for your swarm.&lt;/p&gt;

&lt;p&gt;Before you create a swarm with &lt;code&gt;docker-machine&lt;/code&gt;, you associate each
node with a discovery service. This example uses the token discovery
service hosted by Docker Hub (only for testing/dev, not for production).
This discovery service associates a token with instances of the Docker
Daemon running on each node. Other discovery service backends such as
&lt;code&gt;etcd&lt;/code&gt;, &lt;code&gt;consul&lt;/code&gt;, and &lt;code&gt;zookeeper&lt;/code&gt; are &lt;a href=&#34;../swarm/discovery/&#34;&gt;available&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;List the machines on your system.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine ls
NAME         ACTIVE   DRIVER       STATE     URL                         SWARM
docker-vm    *        virtualbox   Running   tcp://192.168.99.100:2376   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This example was run a Mac OSX system with Docker Toolbox installed. So, the
    &lt;code&gt;docker-vm&lt;/code&gt; virtual machine is in the list.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create a VirtualBox machine called &lt;code&gt;local&lt;/code&gt; on your system.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine create -d virtualbox local
INFO[0000] Creating SSH key...                          
INFO[0000] Creating VirtualBox VM...                    
INFO[0005] Starting VirtualBox VM...                    
INFO[0005] Waiting for VM to start...                   
INFO[0050] &amp;quot;local&amp;quot; has been created and is now the active machine.
INFO[0050] To point your Docker client at it, run this in your shell: eval &amp;quot;$(docker-machine env local)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Load the &lt;code&gt;local&lt;/code&gt; machine configuration into your shell.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ eval &amp;quot;$(docker-machine env local)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Generate a discovery token using the Docker Swarm image.&lt;/p&gt;

&lt;p&gt;The command below runs the &lt;code&gt;swarm create&lt;/code&gt; command in a container. If you
haven&amp;rsquo;t got the &lt;code&gt;swarm:latest&lt;/code&gt; image on your local machine, Docker pulls it
for you.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run swarm create
Unable to find image &#39;swarm:latest&#39; locally
latest: Pulling from swarm
de939d6ed512: Pull complete
79195899a8a4: Pull complete
79ad4f2cc8e0: Pull complete
0db1696be81b: Pull complete
ae3b6728155e: Pull complete
57ec2f5f3e06: Pull complete
73504b2882a3: Already exists
swarm:latest: The image you are pulling has been verified. Important: image verification is a tech preview feature and should not be relied on to provide security.
Digest: sha256:aaaf6c18b8be01a75099cc554b4fb372b8ec677ae81764dcdf85470279a61d6f
Status: Downloaded newer image for swarm:latest
fe0cc96a72cf04dba8c1c4aa79536ec3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;swarm create&lt;/code&gt; command returned the  &lt;code&gt;fe0cc96a72cf04dba8c1c4aa79536ec3&lt;/code&gt;
token.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Save the token in a safe place.&lt;/p&gt;

&lt;p&gt;You&amp;rsquo;ll use this token in the next step to create a Docker Swarm.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;launch-the-swarm-manager&#34;&gt;Launch the Swarm manager&lt;/h2&gt;

&lt;p&gt;A single system in your network is known as your Docker Swarm manager. The swarm
manager orchestrates and schedules containers on the entire cluster. The swarm
manager rules a set of agents (also called nodes or Docker nodes).&lt;/p&gt;

&lt;p&gt;Swarm agents are responsible for hosting containers. They are regular docker
daemons and you can communicate with them using the Docker remote API.&lt;/p&gt;

&lt;p&gt;In this section, you create a swarm manager and two nodes.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Create a swarm manager under VirtualBox.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker-machine create \
        -d virtualbox \
        --swarm \
        --swarm-master \
        --swarm-discovery token://&amp;lt;TOKEN-FROM-ABOVE&amp;gt; \
        swarm-master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine create -d virtualbox --swarm --swarm-master --swarm-discovery token://fe0cc96a72cf04dba8c1c4aa79536ec3 swarm-master
INFO[0000] Creating SSH key...                          
INFO[0000] Creating VirtualBox VM...                    
INFO[0005] Starting VirtualBox VM...                    
INFO[0005] Waiting for VM to start...                   
INFO[0060] &amp;quot;swarm-master&amp;quot; has been created and is now the active machine.
INFO[0060] To point your Docker client at it, run this in your shell: eval &amp;quot;$(docker-machine env swarm-master)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Open your VirtualBox Manager, it should contain the &lt;code&gt;local&lt;/code&gt; machine and the
new &lt;code&gt;swarm-master&lt;/code&gt; machine.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/virtual-box.png&#34; alt=&#34;VirtualBox&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create a swarm node.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    docker-machine create \
    -d virtualbox \
    --swarm \
    --swarm-discovery token://&amp;lt;TOKEN-FROM-ABOVE&amp;gt; \
    swarm-agent-00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine create -d virtualbox --swarm --swarm-discovery token://fe0cc96a72cf04dba8c1c4aa79536ec3 swarm-agent-00
INFO[0000] Creating SSH key...                          
INFO[0000] Creating VirtualBox VM...                    
INFO[0005] Starting VirtualBox VM...                    
INFO[0006] Waiting for VM to start...                   
INFO[0066] &amp;quot;swarm-agent-00&amp;quot; has been created and is now the active machine.
INFO[0066] To point your Docker client at it, run this in your shell: eval &amp;quot;$(docker-machine env swarm-agent-00)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Add another agent called &lt;code&gt;swarm-agent-01&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine create -d virtualbox --swarm --swarm-discovery token://fe0cc96a72cf04dba8c1c4aa79536ec3 swarm-agent-01
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should see the two agents in your VirtualBox Manager.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;direct-your-swarm&#34;&gt;Direct your swarm&lt;/h2&gt;

&lt;p&gt;In this step, you connect to the swarm machine, display information related to
your swarm, and start an image on your swarm.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Point your Docker environment to the machine running the swarm master.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ eval $(docker-machine env --swarm swarm-master)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Get information on your new swarm using the &lt;code&gt;docker&lt;/code&gt; command.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker info
Containers: 4
Strategy: spread
Filters: affinity, health, constraint, port, dependency
Nodes: 3
 swarm-agent-00: 192.168.99.105:2376
    └ Containers: 1
    └ Reserved CPUs: 0 / 8
    └ Reserved Memory: 0 B / 1.023 GiB
 swarm-agent-01: 192.168.99.106:2376
    └ Containers: 1
    └ Reserved CPUs: 0 / 8
    └ Reserved Memory: 0 B / 1.023 GiB
 swarm-master: 192.168.99.104:2376
    └ Containers: 2
    └ Reserved CPUs: 0 / 8
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can see that each agent and the master all have port &lt;code&gt;2376&lt;/code&gt; exposed. When you create a swarm, you can use any port you like and even different ports on different nodes. Each swarm node runs the swarm agent container.&lt;/p&gt;

&lt;p&gt;The master is running both the swarm manager and a swarm agent container. This isn&amp;rsquo;t recommended in a production environment because it can cause problems with agent failover. However, it is perfectly fine to do this in a learning environment like this one.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Check the images currently running on your swarm.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker ps  -a
CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS                                     NAMES
78be991b58d1        swarm:latest        &amp;quot;/swarm join --addr    3 minutes ago       Up 2 minutes        2375/tcp                                  swarm-agent-01/swarm-agent        
da5127e4f0f9        swarm:latest        &amp;quot;/swarm join --addr    6 minutes ago       Up 6 minutes        2375/tcp                                  swarm-agent-00/swarm-agent        
ef395f316c59        swarm:latest        &amp;quot;/swarm join --addr    16 minutes ago      Up 16 minutes       2375/tcp                                  swarm-master/swarm-agent          
45821ca5208e        swarm:latest        &amp;quot;/swarm manage --tls   16 minutes ago      Up 16 minutes       2375/tcp, 192.168.99.104:3376-&amp;gt;3376/tcp   swarm-master/swarm-agent-master   
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Run the Docker &lt;code&gt;hello-world&lt;/code&gt; test image on your swarm.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run hello-world
Hello from Docker.
This message shows that your installation appears to be working correctly.


To generate this message, Docker took the following steps:
 1. The Docker client contacted the Docker daemon.
 2. The Docker daemon pulled the &amp;quot;hello-world&amp;quot; image from the Docker Hub.
        (Assuming it was not already locally available.)
 3. The Docker daemon created a new container from that image which runs the
        executable that produces the output you are currently reading.
 4. The Docker daemon streamed that output to the Docker client, which sent it
        to your terminal.


To try something more ambitious, you can run an Ubuntu container with:
 $ docker run -it ubuntu bash


For more examples and ideas, visit:
 http://docs.docker.com/userguide/
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;docker ps&lt;/code&gt; command to find out which node the container ran on.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker ps -a
CONTAINER ID        IMAGE                COMMAND                CREATED             STATUS                     PORTS                                     NAMES
54a8690043dd        hello-world:latest   &amp;quot;/hello&amp;quot;               22 seconds ago      Exited (0) 3 seconds ago                                             swarm-agent-00/modest_goodall     
78be991b58d1        swarm:latest         &amp;quot;/swarm join --addr    5 minutes ago       Up 4 minutes               2375/tcp                                  swarm-agent-01/swarm-agent        
da5127e4f0f9        swarm:latest         &amp;quot;/swarm join --addr    8 minutes ago       Up 8 minutes               2375/tcp                                  swarm-agent-00/swarm-agent        
ef395f316c59        swarm:latest         &amp;quot;/swarm join --addr    18 minutes ago      Up 18 minutes              2375/tcp                                  swarm-master/swarm-agent          
45821ca5208e        swarm:latest         &amp;quot;/swarm manage --tls   18 minutes ago      Up 18 minutes              2375/tcp, 192.168.99.104:3376-&amp;gt;3376/tcp   swarm-master/swarm-agent-master   
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;where-to-go-next&#34;&gt;Where to go next&lt;/h2&gt;

&lt;p&gt;At this point, you&amp;rsquo;ve installed Docker Swarm by pulling the latest image of
it from Docker Hub. Then, you built and ran a swarm on your local machine
using VirtualBox. If you want, you can onto read an &lt;a href=&#34;../swarm/&#34;&gt;overview of Docker Swarm
features&lt;/a&gt;. Alternatively, you can develop a more in-depth view of Swarm by
&lt;a href=&#34;../swarm/install-manual/&#34;&gt;manually installing Swarm&lt;/a&gt; on a network.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Docker Swarm API</title>
      <link>http://docs.docker.com/swarm/api/swarm-api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://docs.docker.com/swarm/api/swarm-api/</guid>
      <description>

&lt;h1 id=&#34;docker-swarm-api&#34;&gt;Docker Swarm API&lt;/h1&gt;

&lt;p&gt;The Docker Swarm API is mostly compatible with the &lt;a href=&#34;https://docs.docker.com/reference/api/docker_remote_api/&#34;&gt;Docker Remote API&lt;/a&gt;. This document is an overview of the differences between the Swarm API and the Docker Remote API.&lt;/p&gt;

&lt;h2 id=&#34;missing-endpoints&#34;&gt;Missing endpoints&lt;/h2&gt;

&lt;p&gt;Some endpoints have not yet been implemented and will return a 404 error.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;POST &amp;quot;/images/create&amp;quot; : &amp;quot;docker import&amp;quot; flow not implement
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;endpoints-which-behave-differently&#34;&gt;Endpoints which behave differently&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;GET &amp;quot;/containers/{name:.*}/json&amp;quot;&lt;/code&gt;: New field &lt;code&gt;Node&lt;/code&gt; added:&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;&amp;quot;Node&amp;quot;: {
	&amp;quot;Id&amp;quot;: &amp;quot;ODAI:IC6Q:MSBL:TPB5:HIEE:6IKC:VCAM:QRNH:PRGX:ERZT:OK46:PMFX&amp;quot;,
	&amp;quot;Ip&amp;quot;: &amp;quot;0.0.0.0&amp;quot;,
	&amp;quot;Addr&amp;quot;: &amp;quot;http://0.0.0.0:4243&amp;quot;,
	&amp;quot;Name&amp;quot;: &amp;quot;vagrant-ubuntu-saucy-64&amp;quot;,
    },
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;GET &amp;quot;/containers/{name:.*}/json&amp;quot;&lt;/code&gt;: &lt;code&gt;HostIP&lt;/code&gt; replaced by the the actual Node&amp;rsquo;s IP if &lt;code&gt;HostIP&lt;/code&gt; is &lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;GET &amp;quot;/containers/json&amp;quot;&lt;/code&gt;: Node&amp;rsquo;s name prepended to the container name.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;GET &amp;quot;/containers/json&amp;quot;&lt;/code&gt;: &lt;code&gt;HostIP&lt;/code&gt; replaced by the the actual Node&amp;rsquo;s IP if &lt;code&gt;HostIP&lt;/code&gt; is &lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;GET &amp;quot;/containers/json&amp;quot;&lt;/code&gt; : Containers started from the &lt;code&gt;swarm&lt;/code&gt; official image are hidden by default, use &lt;code&gt;all=1&lt;/code&gt; to display them.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;GET &amp;quot;/images/json&amp;quot;&lt;/code&gt; : Use &amp;lsquo;&amp;ndash;filter node=&amp;lt;Node name&amp;gt;&amp;rsquo; to show images of the specific node.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;POST &amp;quot;/containers/create&amp;quot;&lt;/code&gt;: &lt;code&gt;CpuShares&lt;/code&gt; in &lt;code&gt;HostConfig&lt;/code&gt; sets the number of CPU cores allocated to the container.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;docker-swarm-documentation-index&#34;&gt;Docker Swarm documentation index&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/swarm/&#34;&gt;Docker Swarm overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/swarm/discovery/&#34;&gt;Discovery options&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/swarm/scheduler/strategy/&#34;&gt;Scheduler strategies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/swarm/scheduler/filter/&#34;&gt;Scheduler filters&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Docker Swarm Networking</title>
      <link>http://docs.docker.com/swarm/networking/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://docs.docker.com/swarm/networking/</guid>
      <description>

&lt;h1 id=&#34;networking&#34;&gt;Networking&lt;/h1&gt;

&lt;p&gt;Docker Swarm is fully compatible with Docker&amp;rsquo;s networking features. This
includes the multi-host networking feature which allows creation of custom
container networks that span multiple Docker hosts.&lt;/p&gt;

&lt;p&gt;Before using Swarm with a custom network, read through the conceptual
information in &lt;a href=&#34;https://docs.docker.com/engine/userguide/networking/dockernetworks/&#34;&gt;Docker container
networking&lt;/a&gt;.
You should also have walked through the &lt;a href=&#34;https://docs.docker.com/engine/userguide/networking/get-started-overlay/&#34;&gt;Get started with multi-host
networking&lt;/a&gt;
example.&lt;/p&gt;

&lt;h2 id=&#34;create-a-custom-network-in-a-swarm-cluster&#34;&gt;Create a custom network in a Swarm cluster&lt;/h2&gt;

&lt;p&gt;Multi-host networks require a key-value store. The key-value store holds
information about the network state which includes discovery, networks,
endpoints, IP addresses, and more. Through the Docker&amp;rsquo;s libkv project, Docker
supports Consul, Etcd, and ZooKeeper key-value store backends. For details about
the supported backends, refer to the &lt;a href=&#34;https://github.com/docker/libkv&#34;&gt;libkv
project&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To create a custom network, you must choose a key-value store backend and
implement it on your network. Then, you configure the Docker Engine daemon to
use this store. Two required parameters,  &lt;code&gt;--cluster-store&lt;/code&gt; and
&lt;code&gt;--cluster-advertise&lt;/code&gt;, refer to your key-value store server.&lt;/p&gt;

&lt;p&gt;Once you&amp;rsquo;ve configured and restarted the daemon on each Swarm node, you are
ready to create a network.&lt;/p&gt;

&lt;h2 id=&#34;list-networks&#34;&gt;List networks&lt;/h2&gt;

&lt;p&gt;This example assumes there are two nodes &lt;code&gt;node-0&lt;/code&gt; and &lt;code&gt;node-1&lt;/code&gt; in the cluster.
From a swarm node, list the networks:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker network ls
NETWORK ID          NAME                   DRIVER
3dd50db9706d        node-0/host            host
09138343e80e        node-0/bridge          bridge
8834dbd552e5        node-0/none            null
45782acfe427        node-1/host            host
8926accb25fd        node-1/bridge          bridge
6382abccd23d        node-1/none            null
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, each network name is prefixed by the node name.&lt;/p&gt;

&lt;h2 id=&#34;create-a-network&#34;&gt;Create a network&lt;/h2&gt;

&lt;p&gt;By default, Swarm is using the &lt;code&gt;overlay&lt;/code&gt; network driver, a global-scope network
driver. A global-scope network driver creates a network across an entire swarm.
When you create an &lt;code&gt;overlay&lt;/code&gt; network under Swarm, you can omit the &lt;code&gt;-d&lt;/code&gt; option:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker network create swarm_network
42131321acab3233ba342443Ba4312
$ docker network ls
NETWORK ID          NAME                   DRIVER
3dd50db9706d        node-0/host            host
09138343e80e        node-0/bridge          bridge
8834dbd552e5        node-0/none            null
42131321acab        node-0/swarm_network   overlay
45782acfe427        node-1/host            host
8926accb25fd        node-1/bridge          bridge
6382abccd23d        node-1/none            null
42131321acab        node-1/swarm_network   overlay
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see here, both the &lt;code&gt;node-0/swarm_network&lt;/code&gt; and the
&lt;code&gt;node-1/swarm_network&lt;/code&gt; have the same ID.  This is because when you create a
network on the swarm, it is accessible from all the nodes.&lt;/p&gt;

&lt;p&gt;To create a local scope network (for example with the &lt;code&gt;bridge&lt;/code&gt; network driver) you
should use &lt;code&gt;&amp;lt;node&amp;gt;/&amp;lt;name&amp;gt;&lt;/code&gt; otherwise your network is created on a random node.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker network create node-0/bridge2 -b bridge
921817fefea521673217123abab223
$ docker network create node-1/bridge2 -b bridge
5262bbfe5616fef6627771289aacc2
$ docker network ls
NETWORK ID          NAME                   DRIVER
3dd50db9706d        node-0/host            host
09138343e80e        node-0/bridge          bridge
8834dbd552e5        node-0/none            null
42131321acab        node-0/swarm_network   overlay
921817fefea5        node-0/bridge2         brige
45782acfe427        node-1/host            host
8926accb25fd        node-1/bridge          bridge
6382abccd23d        node-1/none            null
42131321acab        node-1/swarm_network   overlay
5262bbfe5616        node-1/bridge2         bridge
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;remove-a-network&#34;&gt;Remove a network&lt;/h2&gt;

&lt;p&gt;To remove a network you can use its ID or its name.
If two different networks have the same name, you may use &lt;code&gt;&amp;lt;node&amp;gt;/&amp;lt;name&amp;gt;&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker network rm swarm_network
42131321acab3233ba342443Ba4312
$ docker network rm node-0/bridge2
921817fefea521673217123abab223
$ docker network ls
NETWORK ID          NAME                   DRIVER
3dd50db9706d        node-0/host            host
09138343e80e        node-0/bridge          bridge
8834dbd552e5        node-0/none            null
45782acfe427        node-1/host            host
8926accb25fd        node-1/bridge          bridge
6382abccd23d        node-1/none            null
5262bbfe5616        node-1/bridge2         bridge
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;swarm_network&lt;/code&gt; was removed from every node, &lt;code&gt;bridge2&lt;/code&gt; was removed only
from &lt;code&gt;node-0&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;docker-swarm-documentation-index&#34;&gt;Docker Swarm documentation index&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/&#34;&gt;Docker Swarm overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/scheduler/strategy/&#34;&gt;Scheduler strategies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/scheduler/filter/&#34;&gt;Scheduler filters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/api/swarm-api/&#34;&gt;Swarm API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Docker Swarm Scheduling</title>
      <link>http://docs.docker.com/swarm/scheduler/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://docs.docker.com/swarm/scheduler/</guid>
      <description>

&lt;h2 id=&#34;advanced-scheduling&#34;&gt;Advanced Scheduling&lt;/h2&gt;

&lt;p&gt;To learn more about advanced scheduling, see the
&lt;a href=&#34;../swarm/scheduler/strategy/&#34;&gt;strategies&lt;/a&gt; and &lt;a href=&#34;../swarm/scheduler/filter/&#34;&gt;filters&lt;/a&gt;
documents.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Docker Swarm discovery</title>
      <link>http://docs.docker.com/swarm/discovery/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://docs.docker.com/swarm/discovery/</guid>
      <description>

&lt;h1 id=&#34;discovery&#34;&gt;Discovery&lt;/h1&gt;

&lt;p&gt;Docker Swarm comes with multiple Discovery backends.&lt;/p&gt;

&lt;h2 id=&#34;backends&#34;&gt;Backends&lt;/h2&gt;

&lt;p&gt;You use a hosted discovery service with Docker Swarm. The service
maintains a list of IPs in your swarm. There are several available
services, such as &lt;code&gt;etcd&lt;/code&gt;, &lt;code&gt;consul&lt;/code&gt; and &lt;code&gt;zookeeper&lt;/code&gt; depending on what
is best suited for your environment. You can even use a static
file. Docker Hub also provides a hosted discovery service which you
can use.&lt;/p&gt;

&lt;h3 id=&#34;hosted-discovery-with-docker-hub&#34;&gt;Hosted Discovery with Docker Hub&lt;/h3&gt;

&lt;p&gt;#####The Hosted Discovery Service is not recommended for production use.
#####It&amp;rsquo;s intended to be used for testing/development.&lt;/p&gt;

&lt;p&gt;#####See other discovery backends for production use.&lt;/p&gt;

&lt;p&gt;This example uses the hosted discovery service on Docker Hub. Using
Docker Hub&amp;rsquo;s hosted discovery service requires that each node in the
swarm is connected to the internet. To create your swarm:&lt;/p&gt;

&lt;p&gt;First we create a cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# create a cluster
$ swarm create
6856663cdefdec325839a4b7e1de38e8 # &amp;lt;- this is your unique &amp;lt;cluster_id&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we create each node and join them to the cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# on each of your nodes, start the swarm agent
#  &amp;lt;node_ip&amp;gt; doesn&#39;t have to be public (eg. 192.168.0.X),
#  as long as the swarm manager can access it.
$ swarm join --advertise=&amp;lt;node_ip:2375&amp;gt; token://&amp;lt;cluster_id&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we start the Swarm manager. This can be on any machine or even
your laptop.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ swarm manage -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; token://&amp;lt;cluster_id&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can then use regular Docker commands to interact with your swarm.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; info
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; run ...
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; ps
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; logs ...
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also list the nodes in your cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm list token://&amp;lt;cluster_id&amp;gt;
&amp;lt;node_ip:2375&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;using-a-static-file-describing-the-cluster&#34;&gt;Using a static file describing the cluster&lt;/h3&gt;

&lt;p&gt;For each of your nodes, add a line to a file. The node IP address
doesn&amp;rsquo;t need to be public as long the Swarm manager can access it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo &amp;lt;node_ip1:2375&amp;gt; &amp;gt;&amp;gt; /tmp/my_cluster
echo &amp;lt;node_ip2:2375&amp;gt; &amp;gt;&amp;gt; /tmp/my_cluster
echo &amp;lt;node_ip3:2375&amp;gt; &amp;gt;&amp;gt; /tmp/my_cluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then start the Swarm manager on any machine.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm manage -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; file:///tmp/my_cluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And then use the regular Docker commands.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; info
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; run ...
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; ps
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; logs ...
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can list the nodes in your cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ swarm list file:///tmp/my_cluster
&amp;lt;node_ip1:2375&amp;gt;
&amp;lt;node_ip2:2375&amp;gt;
&amp;lt;node_ip3:2375&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;using-etcd&#34;&gt;Using etcd&lt;/h3&gt;

&lt;p&gt;On each of your nodes, start the Swarm agent. The node IP address
doesn&amp;rsquo;t have to be public as long as the swarm manager can access it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm join --advertise=&amp;lt;node_ip:2375&amp;gt; etcd://&amp;lt;etcd_addr1&amp;gt;,&amp;lt;etcd_addr2&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Start the manager on any machine or your laptop.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm manage -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; etcd://&amp;lt;etcd_addr1&amp;gt;,&amp;lt;etcd_addr2&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And then use the regular Docker commands.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; info
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; run ...
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; ps
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; logs ...
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can list the nodes in your cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm list etcd://&amp;lt;etcd_addr1&amp;gt;,&amp;lt;etcd_addr2&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&amp;lt;node_ip:2375&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;using-consul&#34;&gt;Using consul&lt;/h3&gt;

&lt;p&gt;On each of your nodes, start the Swarm agent. The node IP address
doesn&amp;rsquo;t need to be public as long as the Swarm manager can access it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm join --advertise=&amp;lt;node_ip:2375&amp;gt; consul://&amp;lt;consul_addr&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Start the manager on any machine or your laptop.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm manage -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; consul://&amp;lt;consul_addr&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And then use the regular Docker commands.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; info
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; run ...
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; ps
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; logs ...
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can list the nodes in your cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm list consul://&amp;lt;consul_addr&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&amp;lt;node_ip:2375&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;using-zookeeper&#34;&gt;Using zookeeper&lt;/h3&gt;

&lt;p&gt;On each of your nodes, start the Swarm agent. The node IP doesn&amp;rsquo;t have
to be public as long as the swarm manager can access it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm join --advertise=&amp;lt;node_ip:2375&amp;gt; zk://&amp;lt;zookeeper_addr1&amp;gt;,&amp;lt;zookeeper_addr2&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Start the manager on any machine or your laptop.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm manage -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; zk://&amp;lt;zookeeper_addr1&amp;gt;,&amp;lt;zookeeper_addr2&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can then use the regular Docker commands.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; info
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; run ...
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; ps
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; logs ...
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can list the nodes in the cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm list zk://&amp;lt;zookeeper_addr1&amp;gt;,&amp;lt;zookeeper_addr2&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&amp;lt;node_ip:2375&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;using-a-static-list-of-ip-addresses&#34;&gt;Using a static list of IP addresses&lt;/h3&gt;

&lt;p&gt;Start the manager on any machine or your laptop&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm manage -H &amp;lt;swarm_ip:swarm_port&amp;gt; nodes://&amp;lt;node_ip1:2375&amp;gt;,&amp;lt;node_ip2:2375&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm manage -H &amp;lt;swarm_ip:swarm_port&amp;gt; &amp;lt;node_ip1:2375&amp;gt;,&amp;lt;node_ip2:2375&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then use the regular Docker commands.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker -H &amp;lt;swarm_ip:swarm_port&amp;gt; info
docker -H &amp;lt;swarm_ip:swarm_port&amp;gt; run ...
docker -H &amp;lt;swarm_ip:swarm_port&amp;gt; ps
docker -H &amp;lt;swarm_ip:swarm_port&amp;gt; logs ...
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;range-pattern-for-ip-addresses&#34;&gt;Range pattern for IP addresses&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;file&lt;/code&gt; and &lt;code&gt;nodes&lt;/code&gt; discoveries support a range pattern to specify IP
addresses, i.e., &lt;code&gt;10.0.0.[10:200]&lt;/code&gt; will be a list of nodes starting from
&lt;code&gt;10.0.0.10&lt;/code&gt; to &lt;code&gt;10.0.0.200&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;For example for the &lt;code&gt;file&lt;/code&gt; discovery method.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ echo &amp;quot;10.0.0.[11:100]:2375&amp;quot;   &amp;gt;&amp;gt; /tmp/my_cluster
$ echo &amp;quot;10.0.1.[15:20]:2375&amp;quot;    &amp;gt;&amp;gt; /tmp/my_cluster
$ echo &amp;quot;192.168.1.2:[2:20]375&amp;quot;  &amp;gt;&amp;gt; /tmp/my_cluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then start the manager.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm manage -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; file:///tmp/my_cluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And for the &lt;code&gt;nodes&lt;/code&gt; discovery method.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm manage -H &amp;lt;swarm_ip:swarm_port&amp;gt; &amp;quot;nodes://10.0.0.[10:200]:2375,10.0.1.[2:250]:2375&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;contributing-a-new-discovery-backend&#34;&gt;Contributing a new discovery backend&lt;/h2&gt;

&lt;p&gt;You can contribute a new discovery backend to Swarm. For information on how to
do this, see &lt;a
href=&#34;https://github.com/docker/swarm/blob/master/discovery/README.md&#34;&gt;our
discovery README in the Docker Swarm repository&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;docker-swarm-documentation-index&#34;&gt;Docker Swarm documentation index&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/&#34;&gt;Docker Swarm overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/scheduler/strategy/&#34;&gt;Scheduler strategies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/scheduler/filter/&#34;&gt;Scheduler filters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/api/swarm-api/&#34;&gt;Swarm API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/&#34;&gt;Docker Swarm overview&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Docker Swarm filters</title>
      <link>http://docs.docker.com/swarm/scheduler/filter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://docs.docker.com/swarm/scheduler/filter/</guid>
      <description>

&lt;h1 id=&#34;filters&#34;&gt;Filters&lt;/h1&gt;

&lt;p&gt;Filters tell Docker Swarm scheduler which nodes to use when creating and running
a container.&lt;/p&gt;

&lt;h2 id=&#34;configure-the-available-filters&#34;&gt;Configure the available filters&lt;/h2&gt;

&lt;p&gt;Filters are divided into two categories, node filters and container configuration
filters. Node filters operate on characteristics of the Docker host or on the
configuration of the Docker daemon. Container configuration filters operate on
characteristics of containers, or on the availability of images on a host.&lt;/p&gt;

&lt;p&gt;Each filter has a name that identifies it. The node filters are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;constraint&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;health&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The container configuration filters are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;affinity&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dependency&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;port&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When you start a Swarm manager with the &lt;code&gt;swarm manage&lt;/code&gt; command, all the filters
are enabled. If you want to limit the filters available to your Swarm, specify a subset
of filters by passing the &lt;code&gt;--filter&lt;/code&gt; flag and the name:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ swarm manage --filter=health --filter=dependency
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Container configuration filters match all containers, including stopped
containers, when applying the filter. To release a node used by a container, you
must remove the container from the node.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;node-filters&#34;&gt;Node filters&lt;/h2&gt;

&lt;p&gt;When creating a container or building an image, you use a &lt;code&gt;constraint&lt;/code&gt; or
&lt;code&gt;health&lt;/code&gt; filter to select a subset of nodes to consider for scheduling.&lt;/p&gt;

&lt;h3 id=&#34;use-a-constraint-filter&#34;&gt;Use a constraint filter&lt;/h3&gt;

&lt;p&gt;Node constraints can refer to Docker&amp;rsquo;s default tags or to custom labels. Default
tags are sourced from &lt;code&gt;docker info&lt;/code&gt;. Often, they relate to properties of the Docker
host. Currently, the dafult tags include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;node&lt;/code&gt; to refer to the node by ID or name&lt;/li&gt;
&lt;li&gt;&lt;code&gt;storagedriver&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;executiondriver&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kernelversion&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;operatingsystem&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Custom node labels you apply when you start the &lt;code&gt;docker daemon&lt;/code&gt;, for example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker daemon --label com.example.environment=&amp;quot;production&amp;quot; --label
com.example.storage=&amp;quot;ssd&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, when you start a container on the cluster, you can set constraints using
these default tags or custom labels. The Swarm scheduler looks for matching node
on the cluster and starts the container there. This approach has several
practical applications:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Schedule based on specific host properties, for example,&lt;code&gt;storage=ssd&lt;/code&gt; schedules
containers on specific hardware.&lt;/li&gt;
&lt;li&gt;Force containers to run in a given location, for example region=us-east`.&lt;/li&gt;
&lt;li&gt;Create logical cluster partitions by splitting a cluster into
sub-clusters with different properties, for example &lt;code&gt;environment=production&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;example-node-constraints&#34;&gt;Example node constraints&lt;/h4&gt;

&lt;p&gt;To specify custom label for a node, pass a list of &lt;code&gt;--label&lt;/code&gt;
options at &lt;code&gt;docker&lt;/code&gt; startup time. For instance, to start &lt;code&gt;node-1&lt;/code&gt; with the
&lt;code&gt;storage=ssd&lt;/code&gt; label:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker daemon --label storage=ssd
$ swarm join --advertise=192.168.0.42:2375 token://XXXXXXXXXXXXXXXXXX
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You might start a different &lt;code&gt;node-2&lt;/code&gt; with &lt;code&gt;storage=disk&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker daemon --label storage=disk
$ swarm join --advertise=192.168.0.43:2375 token://XXXXXXXXXXXXXXXXXX
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once the nodes are joined to a cluster, the Swarm master pulls their respective
tags.  Moving forward, the master takes the tags into account when scheduling
new containers.&lt;/p&gt;

&lt;p&gt;Continuing the previous example, assuming your cluster with &lt;code&gt;node-1&lt;/code&gt; and
&lt;code&gt;node-2&lt;/code&gt;, you can run a MySQL server container on the cluster.  When you run the
container, you can use a &lt;code&gt;constraint&lt;/code&gt; to ensure the database gets good I/O
performance. You do this by by filter for nodes with flash drives:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run -d -P -e constraint:storage==ssd --name db mysql
f8b693db9cd6

$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NODE        NAMES
f8b693db9cd6        mysql:latest        &amp;quot;mysqld&amp;quot;            Less than a second ago   running             192.168.0.42:49178-&amp;gt;3306/tcp    node-1      db
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this example, the master selected all nodes that met the &lt;code&gt;storage=ssd&lt;/code&gt;
constraint and applied resource management on top of them.   Only &lt;code&gt;node-1&lt;/code&gt; was
selected because it&amp;rsquo;s the only host running flash.&lt;/p&gt;

&lt;p&gt;Suppose you want run an Nginx frontend in a cluster. In this case, you wouldn&amp;rsquo;t want flash drives because the frontend mostly writes logs to disk.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run -d -P -e constraint:storage==disk --name frontend nginx
963841b138d8

$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NODE        NAMES
963841b138d8        nginx:latest        &amp;quot;nginx&amp;quot;             Less than a second ago   running             192.168.0.43:49177-&amp;gt;80/tcp      node-2      frontend
f8b693db9cd6        mysql:latest        &amp;quot;mysqld&amp;quot;            Up About a minute        running             192.168.0.42:49178-&amp;gt;3306/tcp    node-1      db
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The scheduler selected &lt;code&gt;node-2&lt;/code&gt; since it was started with the &lt;code&gt;storage=disk&lt;/code&gt; label.&lt;/p&gt;

&lt;p&gt;Finally, build args can be used to apply node constraints to a &lt;code&gt;docker build&lt;/code&gt;.
Again, you&amp;rsquo;ll avoid flash drives.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ mkdir sinatra
$ cd sinatra
$ echo &amp;quot;FROM ubuntu:14.04&amp;quot; &amp;gt; Dockerfile
$ echo &amp;quot;MAINTAINER Kate Smith &amp;lt;ksmith@example.com&amp;gt;&amp;quot; &amp;gt;&amp;gt; Dockerfile
$ echo &amp;quot;RUN apt-get update &amp;amp;&amp;amp; apt-get install -y ruby ruby-dev&amp;quot; &amp;gt;&amp;gt; Dockerfile
$ echo &amp;quot;RUN gem install sinatra&amp;quot; &amp;gt;&amp;gt; Dockerfile
$ docker build --build-arg=constraint:storage==disk -t ouruser/sinatra:v2 .
Sending build context to Docker daemon 2.048 kB
Step 1 : FROM ubuntu:14.04
 ---&amp;gt; a5a467fddcb8
Step 2 : MAINTAINER Kate Smith &amp;lt;ksmith@example.com&amp;gt;
 ---&amp;gt; Running in 49e97019dcb8
 ---&amp;gt; de8670dcf80e
Removing intermediate container 49e97019dcb8
Step 3 : RUN apt-get update &amp;amp;&amp;amp; apt-get install -y ruby ruby-dev
 ---&amp;gt; Running in 26c9fbc55aeb
 ---&amp;gt; 30681ef95fff
Removing intermediate container 26c9fbc55aeb
Step 4 : RUN gem install sinatra
 ---&amp;gt; Running in 68671d4a17b0
 ---&amp;gt; cd70495a1514
Removing intermediate container 68671d4a17b0
Successfully built cd70495a1514

$ docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
dockerswarm/swarm   master              8c2c56438951        2 days ago          795.7 MB
ouruser/sinatra     v2                  cd70495a1514        35 seconds ago      318.7 MB
ubuntu              14.04               a5a467fddcb8        11 days ago         187.9 MB
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;use-the-health-filter&#34;&gt;Use the health filter&lt;/h3&gt;

&lt;p&gt;The node &lt;code&gt;health&lt;/code&gt; filter prevents the scheduler form running containers
on unhealthy nodes. A node is considered unhealthy if the node is down or it
can&amp;rsquo;t communicate with the cluster store.&lt;/p&gt;

&lt;h2 id=&#34;container-filters&#34;&gt;Container filters&lt;/h2&gt;

&lt;p&gt;When creating a container, you can use three types of container filters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#use-an-affinity-filter&#34;&gt;&lt;code&gt;affinity&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#use-a-depedency-filter&#34;&gt;&lt;code&gt;dependency&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#use-a-port-filter&#34;&gt;&lt;code&gt;port&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;use-an-affinity-filter&#34;&gt;Use an affinity filter&lt;/h3&gt;

&lt;p&gt;Use an &lt;code&gt;affinity&lt;/code&gt; filter to create &amp;ldquo;attractions&amp;rdquo; between containers. For
example, you can run a container and instruct Swarm to schedule it next to
another container based on these affinities:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;container name or id&lt;/li&gt;
&lt;li&gt;an image on the host&lt;/li&gt;
&lt;li&gt;a custom label applied to the container&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These affinities ensure that containers run on the same network node
&amp;mdash; without you having to know what each node is running.&lt;/p&gt;

&lt;h4 id=&#34;example-name-affinity&#34;&gt;Example name affinity&lt;/h4&gt;

&lt;p&gt;You can schedule a new container to run next to another based on a container
name or ID. For example, you can start a container called &lt;code&gt;frontend&lt;/code&gt; running
&lt;code&gt;nginx&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run -d -p 80:80 --name frontend nginx
 87c4376856a8


$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NODE        NAMES
87c4376856a8        nginx:latest        &amp;quot;nginx&amp;quot;             Less than a second ago   running             192.168.0.42:80-&amp;gt;80/tcp         node-1      frontend
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, using &lt;code&gt;-e affinity:container==frontend&lt;/code&gt; value to schedule a second
container to locate and run next to the container named &lt;code&gt;frontend&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run -d --name logger -e affinity:container==frontend logger
 87c4376856a8

$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NODE        NAMES
87c4376856a8        nginx:latest        &amp;quot;nginx&amp;quot;             Less than a second ago   running             192.168.0.42:80-&amp;gt;80/tcp         node-1      frontend
963841b138d8        logger:latest       &amp;quot;logger&amp;quot;            Less than a second ago   running                                             node-1      logger
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because of &lt;code&gt;name&lt;/code&gt; affinity, the  &lt;code&gt;logger&lt;/code&gt; container ends up on &lt;code&gt;node-1&lt;/code&gt; along
with the &lt;code&gt;frontend&lt;/code&gt; container. Instead of the &lt;code&gt;frontend&lt;/code&gt; name you could have
supplied its ID as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run -d --name logger -e affinity:container==87c4376856a8
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;example-image-affinity&#34;&gt;Example image affinity&lt;/h4&gt;

&lt;p&gt;You can schedule a container to run only on nodes where a specific image is
already pulled. For example, suppose you pull a &lt;code&gt;redis&lt;/code&gt; image to two hosts and a
&lt;code&gt;mysql&lt;/code&gt; image to a third.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker -H node-1:2375 pull redis
$ docker -H node-2:2375 pull mysql
$ docker -H node-3:2375 pull redis
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Only &lt;code&gt;node-1&lt;/code&gt; and &lt;code&gt;node-3&lt;/code&gt; have the &lt;code&gt;redis&lt;/code&gt; image. Specify a &lt;code&gt;-e
affinity:image==redis&lt;/code&gt; filter to schedule several additional containers to run
on these nodes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run -d --name redis1 -e affinity:image==redis redis
$ docker run -d --name redis2 -e affinity:image==redis redis
$ docker run -d --name redis3 -e affinity:image==redis redis
$ docker run -d --name redis4 -e affinity:image==redis redis
$ docker run -d --name redis5 -e affinity:image==redis redis
$ docker run -d --name redis6 -e affinity:image==redis redis
$ docker run -d --name redis7 -e affinity:image==redis redis
$ docker run -d --name redis8 -e affinity:image==redis redis

$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NODE        NAMES
87c4376856a8        redis:latest        &amp;quot;redis&amp;quot;             Less than a second ago   running                                             node-1      redis1
1212386856a8        redis:latest        &amp;quot;redis&amp;quot;             Less than a second ago   running                                             node-1      redis2
87c4376639a8        redis:latest        &amp;quot;redis&amp;quot;             Less than a second ago   running                                             node-3      redis3
1234376856a8        redis:latest        &amp;quot;redis&amp;quot;             Less than a second ago   running                                             node-1      redis4
86c2136253a8        redis:latest        &amp;quot;redis&amp;quot;             Less than a second ago   running                                             node-3      redis5
87c3236856a8        redis:latest        &amp;quot;redis&amp;quot;             Less than a second ago   running                                             node-3      redis6
87c4376856a8        redis:latest        &amp;quot;redis&amp;quot;             Less than a second ago   running                                             node-3      redis7
963841b138d8        redis:latest        &amp;quot;redis&amp;quot;             Less than a second ago   running                                             node-1      redis8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see here, the containers were only scheduled on nodes that had the
&lt;code&gt;redis&lt;/code&gt; image. Instead of the image name, you could have specified the image ID.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker images
REPOSITORY                         TAG                       IMAGE ID            CREATED             VIRTUAL SIZE
redis                              latest                    06a1f75304ba        2 days ago          111.1 MB

$ docker run -d --name redis1 -e affinity:image==06a1f75304ba redis
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;example-label-affinity&#34;&gt;Example label affinity&lt;/h4&gt;

&lt;p&gt;A label affinity allows you to filter based on a custom container label. For
example, you can run a &lt;code&gt;nginx&lt;/code&gt; container and apply the
&lt;code&gt;com.example.type=frontend&lt;/code&gt; custom label.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run -d -p 80:80 --label com.example.type=frontend nginx
 87c4376856a8

$ docker ps  --filter &amp;quot;label=com.example.type=frontend&amp;quot;
CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NODE        NAMES
87c4376856a8        nginx:latest        &amp;quot;nginx&amp;quot;             Less than a second ago   running             192.168.0.42:80-&amp;gt;80/tcp         node-1      trusting_yonath
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, use &lt;code&gt;-e affnity:com.example.type==frontend&lt;/code&gt; to schedule a container next
to the container with the &lt;code&gt;com.example.type==frontend&lt;/code&gt; label.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run -d -e affinity:com.example.type==frontend logger
 87c4376856a8

$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NODE        NAMES
87c4376856a8        nginx:latest        &amp;quot;nginx&amp;quot;             Less than a second ago   running             192.168.0.42:80-&amp;gt;80/tcp         node-1      trusting_yonath
963841b138d8        logger:latest       &amp;quot;logger&amp;quot;            Less than a second ago   running                                             node-1      happy_hawking
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;logger&lt;/code&gt; container ends up on &lt;code&gt;node-1&lt;/code&gt; because its affinity with the
&lt;code&gt;com.example.type==frontend&lt;/code&gt; label.&lt;/p&gt;

&lt;h3 id=&#34;use-a-dependency-filter&#34;&gt;Use a dependency filter&lt;/h3&gt;

&lt;p&gt;A container dependency filter co-schedules dependent containers on the same node.
Currently, dependencies are declared as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--volumes-from=dependency&lt;/code&gt; (shared volumes)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--link=dependency:alias&lt;/code&gt; (links)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--net=container:dependency&lt;/code&gt; (shared network stacks)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Swarm attempts to co-locate the dependent container on the same node. If it
cannot be done (because the dependent container doesn&amp;rsquo;t exist, or because the
node doesn&amp;rsquo;t have enough resources), it will prevent the container creation.&lt;/p&gt;

&lt;p&gt;The combination of multiple dependencies are honored if possible. For
instance, if you specify &lt;code&gt;--volumes-from=A --net=container:B&lt;/code&gt;,  the scheduler
attempts to co-locate the container on the same node as &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt;. If those
containers are running on different nodes, Swarm does not schedule the container.&lt;/p&gt;

&lt;h3 id=&#34;use-a-port-filter&#34;&gt;Use a port filter&lt;/h3&gt;

&lt;p&gt;When the &lt;code&gt;port&lt;/code&gt; filter is enabled, a container&amp;rsquo;s port configuration is used as a
unique constraint. Docker Swarm selects a node where a particular port is
available and unoccupied by another container or process. Required ports may be
specified by mapping a host port, or using the host networking an exposing a
port using the container configuration.&lt;/p&gt;

&lt;h4 id=&#34;example-in-bridge-mode&#34;&gt;Example in bridge mode&lt;/h4&gt;

&lt;p&gt;By default, containers run on Docker&amp;rsquo;s bridge network. To use the &lt;code&gt;port&lt;/code&gt; filter
with the bridge network, you run a container as follows.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run -d -p 80:80 nginx
87c4376856a8

$ docker ps
CONTAINER ID    IMAGE               COMMAND         PORTS                       NODE        NAMES
87c4376856a8    nginx:latest        &amp;quot;nginx&amp;quot;         192.168.0.42:80-&amp;gt;80/tcp     node-1      prickly_engelbart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Docker Swarm selects a node where port &lt;code&gt;80&lt;/code&gt; is available and unoccupied by another
container or process, in this case &lt;code&gt;node-1&lt;/code&gt;. Attempting to run another container
that uses the host port &lt;code&gt;80&lt;/code&gt; results in Swarm selecting a different node,
because port &lt;code&gt;80&lt;/code&gt; is already occupied on &lt;code&gt;node-1&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run -d -p 80:80 nginx
963841b138d8

$ docker ps
CONTAINER ID        IMAGE          COMMAND        PORTS                           NODE        NAMES
963841b138d8        nginx:latest   &amp;quot;nginx&amp;quot;        192.168.0.43:80-&amp;gt;80/tcp         node-2      dreamy_turing
87c4376856a8        nginx:latest   &amp;quot;nginx&amp;quot;        192.168.0.42:80-&amp;gt;80/tcp         node-1      prickly_engelbart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Again, repeating the same command will result in the selection of &lt;code&gt;node-3&lt;/code&gt;,
since port &lt;code&gt;80&lt;/code&gt; is neither available on &lt;code&gt;node-1&lt;/code&gt; nor &lt;code&gt;node-2&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run -d -p 80:80 nginx
963841b138d8

$ docker ps
CONTAINER ID   IMAGE               COMMAND        PORTS                           NODE        NAMES
f8b693db9cd6   nginx:latest        &amp;quot;nginx&amp;quot;        192.168.0.44:80-&amp;gt;80/tcp         node-3      stoic_albattani
963841b138d8   nginx:latest        &amp;quot;nginx&amp;quot;        192.168.0.43:80-&amp;gt;80/tcp         node-2      dreamy_turing
87c4376856a8   nginx:latest        &amp;quot;nginx&amp;quot;        192.168.0.42:80-&amp;gt;80/tcp         node-1      prickly_engelbart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, Docker Swarm will refuse to run another container that requires port
&lt;code&gt;80&lt;/code&gt;, because it is not available on any node in the cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run -d -p 80:80 nginx
2014/10/29 00:33:20 Error response from daemon: no resources available to schedule container
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each container occupies port &lt;code&gt;80&lt;/code&gt; on its residing node when the container
is created and releases the port when the container is deleted. A container in &lt;code&gt;exited&lt;/code&gt;
state still owns the port. If &lt;code&gt;prickly_engelbart&lt;/code&gt; on &lt;code&gt;node-1&lt;/code&gt; is stopped but not
deleted, trying to start another container on &lt;code&gt;node-1&lt;/code&gt; that requires port &lt;code&gt;80&lt;/code&gt; would fail
because port &lt;code&gt;80&lt;/code&gt; is associated with &lt;code&gt;prickly_engelbart&lt;/code&gt;. To increase running
instances of nginx, you can either restart &lt;code&gt;prickly_engelbart&lt;/code&gt;, or start another container
after deleting &lt;code&gt;prickly_englbart&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&#34;node-port-filter-with-host-networking&#34;&gt;Node port filter with host networking&lt;/h4&gt;

&lt;p&gt;A container running with &lt;code&gt;--net=host&lt;/code&gt; differs from the default
&lt;code&gt;bridge&lt;/code&gt; mode as the &lt;code&gt;host&lt;/code&gt; mode does not perform any port binding. Instead,
host mode requires that you  explicitly expose one or more port numbers.  You
expose a port using &lt;code&gt;EXPOSE&lt;/code&gt; in the &lt;code&gt;Dockerfile&lt;/code&gt; or &lt;code&gt;--expose&lt;/code&gt; on the command
line. Swarm makes use of this information in conjunction with the &lt;code&gt;host&lt;/code&gt; mode to
choose an available node for a new container.&lt;/p&gt;

&lt;p&gt;For example, the following commands start &lt;code&gt;nginx&lt;/code&gt; on 3-node cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run -d --expose=80 --net=host nginx
640297cb29a7
$ docker run -d --expose=80 --net=host nginx
7ecf562b1b3f
$ docker run -d --expose=80 --net=host nginx
09a92f582bc2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Port binding information is not available through the &lt;code&gt;docker ps&lt;/code&gt; command because
all the nodes were started with the &lt;code&gt;host&lt;/code&gt; network.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker ps
CONTAINER ID        IMAGE               COMMAND                CREATED                  STATUS              PORTS               NAMES
640297cb29a7        nginx:1             &amp;quot;nginx -g &#39;daemon of   Less than a second ago   Up 30 seconds                           box3/furious_heisenberg
7ecf562b1b3f        nginx:1             &amp;quot;nginx -g &#39;daemon of   Less than a second ago   Up 28 seconds                           box2/ecstatic_meitner
09a92f582bc2        nginx:1             &amp;quot;nginx -g &#39;daemon of   46 seconds ago           Up 27 seconds                           box1/mad_goldstine
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Swarm refuses the operation when trying to instantiate the 4th container.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$  docker run -d --expose=80 --net=host nginx
FATA[0000] Error response from daemon: unable to find a node with port 80/tcp available in the Host mode
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, port binding to the different value, for example  &lt;code&gt;81&lt;/code&gt;, is still allowed.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$  docker run -d -p 81:80 nginx:latest
832f42819adc
$  docker ps
CONTAINER ID        IMAGE               COMMAND                CREATED                  STATUS                  PORTS                                 NAMES
832f42819adc        nginx:1             &amp;quot;nginx -g &#39;daemon of   Less than a second ago   Up Less than a second   443/tcp, 192.168.136.136:81-&amp;gt;80/tcp   box3/thirsty_hawking
640297cb29a7        nginx:1             &amp;quot;nginx -g &#39;daemon of   8 seconds ago            Up About a minute                                             box3/furious_heisenberg
7ecf562b1b3f        nginx:1             &amp;quot;nginx -g &#39;daemon of   13 seconds ago           Up About a minute                                             box2/ecstatic_meitner
09a92f582bc2        nginx:1             &amp;quot;nginx -g &#39;daemon of   About a minute ago       Up About a minute                                             box1/mad_goldstine
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;how-to-write-filter-expressions&#34;&gt;How to write filter expressions&lt;/h2&gt;

&lt;p&gt;To apply a node &lt;code&gt;constraint&lt;/code&gt; or container &lt;code&gt;affinity&lt;/code&gt; filters you must set
environment variables on the container using filter expressions, for example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run -d --name redis1 -e affinity:image==~redis redis
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each expression must be in the form:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;filter-type&amp;gt;:&amp;lt;key&amp;gt;&amp;lt;operator&amp;gt;&amp;lt;value&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;&amp;lt;filter-type&amp;gt;&lt;/code&gt; is either the &lt;code&gt;affinity&lt;/code&gt; or the &lt;code&gt;container&lt;/code&gt; keyword. It
identifies the type filter you intend to use.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;&amp;lt;key&amp;gt;&lt;/code&gt; is an alpha-numeric and must start with a letter or underscore. The
&lt;code&gt;&amp;lt;key&amp;gt;&lt;/code&gt; corresponds to one of the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the &lt;code&gt;container&lt;/code&gt; keyword&lt;/li&gt;
&lt;li&gt;the &lt;code&gt;node&lt;/code&gt; keyword&lt;/li&gt;
&lt;li&gt;a default tag (node constraints)&lt;/li&gt;
&lt;li&gt;a custom metadata label (nodes or containers).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;code&gt;&amp;lt;operator&amp;gt;&lt;/code&gt;is either &lt;code&gt;==&lt;/code&gt; or &lt;code&gt;!=&lt;/code&gt;. By default, expression operators are
hard enforced. If an expression is not met exactly , the manager does not
schedule the container. You can use a &lt;code&gt;~&lt;/code&gt;(tilde) to create a &amp;ldquo;soft&amp;rdquo; expression.
The scheduler tries to match a soft expression. If the expression is not met,
the scheduler discards the filter and schedules the container according to the
scheduler&amp;rsquo;s strategy.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;&amp;lt;value&amp;gt;&lt;/code&gt; is an alpha-numeric string, dots, hyphens, and underscores making
up one of the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A globbing pattern, for example, &lt;code&gt;abc*&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;A regular expression in the form of &lt;code&gt;/regexp/&lt;/code&gt;. See
&lt;a href=&#34;https://github.com/google/re2/wiki/Syntax&#34;&gt;re2 syntax&lt;/a&gt; for the supported
regex syntax.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following examples illustrate some possible expressions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;constraint:node==node1&lt;/code&gt; matches node &lt;code&gt;node1&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;constraint:node!=node1&lt;/code&gt; matches all nodes, except &lt;code&gt;node1&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;constraint:region!=us*&lt;/code&gt; matches all nodes outside with a &lt;code&gt;region&lt;/code&gt; tag prefixed with &lt;code&gt;us&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;constraint:node==/node[12]/&lt;/code&gt; matches nodes &lt;code&gt;node1&lt;/code&gt; and &lt;code&gt;node2&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;constraint:node==/node\d/&lt;/code&gt; matches all nodes with &lt;code&gt;node&lt;/code&gt; + 1 digit.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;constraint:node!=/node-[01]/&lt;/code&gt; matches all nodes, except &lt;code&gt;node-0&lt;/code&gt; and &lt;code&gt;node-1&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;constraint:node!=/foo\[bar\]/&lt;/code&gt; matches all nodes, except &lt;code&gt;foo[bar]&lt;/code&gt;. You can see the use of escape characters here.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;constraint:node==/(?i)node1/&lt;/code&gt; matches node &lt;code&gt;node1&lt;/code&gt; case-insensitive. So &lt;code&gt;NoDe1&lt;/code&gt; or &lt;code&gt;NODE1&lt;/code&gt; also match.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;affinity:image==~redis&lt;/code&gt; tries to match for nodes running container with a &lt;code&gt;redis&lt;/code&gt; image&lt;/li&gt;
&lt;li&gt;&lt;code&gt;constraint:region==~us*&lt;/code&gt; searches for nodes in the cluster belongs to the&lt;/li&gt;
&lt;li&gt;&lt;code&gt;us&lt;/code&gt; region&lt;/li&gt;
&lt;li&gt;&lt;code&gt;affinity:container!=~redis*&lt;/code&gt; schedule a new &lt;code&gt;redis5&lt;/code&gt; container to a node
without a container that satisfies &lt;code&gt;redis*&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;related-information&#34;&gt;Related information&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/&#34;&gt;Docker Swarm overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/discovery/&#34;&gt;Discovery options&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/scheduler/strategy/&#34;&gt;Scheduler strategies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/api/swarm-api/&#34;&gt;Swarm API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Docker Swarm strategies</title>
      <link>http://docs.docker.com/swarm/scheduler/strategy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://docs.docker.com/swarm/scheduler/strategy/</guid>
      <description>

&lt;h1 id=&#34;strategies&#34;&gt;Strategies&lt;/h1&gt;

&lt;p&gt;The Docker Swarm scheduler features multiple strategies for ranking nodes. The
strategy you choose determines how Swarm computes ranking. When you run a new
container, Swarm chooses to place it on the node with the highest computed ranking
for your chosen strategy.&lt;/p&gt;

&lt;p&gt;To choose a ranking strategy, pass the &lt;code&gt;--strategy&lt;/code&gt; flag and a strategy value to
the &lt;code&gt;swarm manage&lt;/code&gt; command. Swarm currently supports these values:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;spread&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;binpack&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;random&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;code&gt;spread&lt;/code&gt; and &lt;code&gt;binpack&lt;/code&gt; strategies compute rank according to a node&amp;rsquo;s
available CPU, its RAM, and the number of containers it has. The &lt;code&gt;random&lt;/code&gt;
strategy uses no computation. It selects a node at random and is primarily
intended for debugging.&lt;/p&gt;

&lt;p&gt;Your goal in choosing a strategy is to best optimize your swarm according to
your company&amp;rsquo;s needs.&lt;/p&gt;

&lt;p&gt;Under the &lt;code&gt;spread&lt;/code&gt; strategy, Swarm optimizes for the node with the least number
of containers. The &lt;code&gt;binpack&lt;/code&gt; strategy causes Swarm to optimize for the
node which is most packed. Note that a container occupies resource during its life
cycle, including &lt;code&gt;exited&lt;/code&gt; state. Users should be aware of this condition to schedule
containers. For exmaple, &lt;code&gt;spread&lt;/code&gt; strategy only checks number of containers
disregarding their states. A node with no active containers but high number of
stopped containers may not be selected, defeating the purpose of load sharing.
User could either remove stopped containers, or start stopped containers to achieve
load spreading. The &lt;code&gt;random&lt;/code&gt; strategy, like it sounds, chooses nodes at random
regardless of their available CPU or RAM.&lt;/p&gt;

&lt;p&gt;Using the &lt;code&gt;spread&lt;/code&gt; strategy results in containers spread thinly over many
machines. The advantage of this strategy is that if a node goes down you only
lose a few containers.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;binpack&lt;/code&gt; strategy avoids fragmentation because it leaves room for bigger
containers on unused machines. The strategic advantage of &lt;code&gt;binpack&lt;/code&gt; is that you
use fewer machines as Swarm tries to pack as many containers as it can on a
node.&lt;/p&gt;

&lt;p&gt;If you do not specify a &lt;code&gt;--strategy&lt;/code&gt; Swarm uses &lt;code&gt;spread&lt;/code&gt; by default.&lt;/p&gt;

&lt;h2 id=&#34;spread-strategy-example&#34;&gt;Spread strategy example&lt;/h2&gt;

&lt;p&gt;In this example, your swarm is using the &lt;code&gt;spread&lt;/code&gt; strategy which optimizes for
nodes that have the fewest containers. In this swarm, both &lt;code&gt;node-1&lt;/code&gt; and &lt;code&gt;node-2&lt;/code&gt;
have 2G of RAM, 2 CPUs, and neither node is running a container. Under this strategy
&lt;code&gt;node-1&lt;/code&gt; and &lt;code&gt;node-2&lt;/code&gt; have the same ranking.&lt;/p&gt;

&lt;p&gt;When you run a new container, the system chooses &lt;code&gt;node-1&lt;/code&gt; at random from the swarm
of two equally ranked nodes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ docker run -d -P -m 1G --name db mysql
  f8b693db9cd6

  $ docker ps
  CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NODE        NAMES
  f8b693db9cd6        mysql:latest        &amp;quot;mysqld&amp;quot;            Less than a second ago   running             192.168.0.42:49178-&amp;gt;3306/tcp    node-1      db
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, we start another container and ask for 1G of RAM again.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d -P -m 1G --name frontend nginx
963841b138d8

$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NODE        NAMES
963841b138d8        nginx:latest        &amp;quot;nginx&amp;quot;             Less than a second ago   running             192.168.0.42:49177-&amp;gt;80/tcp      node-2      frontend
f8b693db9cd6        mysql:latest        &amp;quot;mysqld&amp;quot;            Up About a minute        running             192.168.0.42:49178-&amp;gt;3306/tcp    node-1      db
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The container &lt;code&gt;frontend&lt;/code&gt; was started on &lt;code&gt;node-2&lt;/code&gt; because it was the node the
least loaded already. If two nodes have the same amount of available RAM and
CPUs, the &lt;code&gt;spread&lt;/code&gt; strategy prefers the node with least containers.&lt;/p&gt;

&lt;h2 id=&#34;binpack-strategy-example&#34;&gt;BinPack strategy example&lt;/h2&gt;

&lt;p&gt;In this example, let&amp;rsquo;s says that both &lt;code&gt;node-1&lt;/code&gt; and &lt;code&gt;node-2&lt;/code&gt; have 2G of RAM and
neither is running a container. Again, the nodes are equal. When you run a new
container, the system chooses &lt;code&gt;node-1&lt;/code&gt; at random from the swarm:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d -P -m 1G --name db mysql
f8b693db9cd6

$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NODE        NAMES
f8b693db9cd6        mysql:latest        &amp;quot;mysqld&amp;quot;            Less than a second ago   running             192.168.0.42:49178-&amp;gt;3306/tcp    node-1      db
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, you start another container, asking for 1G of RAM again.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d -P -m 1G --name frontend nginx
963841b138d8

$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NODE        NAMES
963841b138d8        nginx:latest        &amp;quot;nginx&amp;quot;             Less than a second ago   running             192.168.0.42:49177-&amp;gt;80/tcp      node-1      frontend
f8b693db9cd6        mysql:latest        &amp;quot;mysqld&amp;quot;            Up About a minute        running             192.168.0.42:49178-&amp;gt;3306/tcp    node-1      db
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The system starts the new &lt;code&gt;frontend&lt;/code&gt; container on &lt;code&gt;node-1&lt;/code&gt; because it was the
node the most packed already. This allows us to start a container requiring 2G
of RAM on &lt;code&gt;node-2&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If two nodes have the same amount of available RAM and CPUs, the &lt;code&gt;binpack&lt;/code&gt;
strategy prefers the node with most containers.&lt;/p&gt;

&lt;h2 id=&#34;docker-swarm-documentation-index&#34;&gt;Docker Swarm documentation index&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/&#34;&gt;Docker Swarm overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/discovery/&#34;&gt;Discovery options&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/scheduler/filter/&#34;&gt;Scheduler filters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/api/swarm-api/&#34;&gt;Swarm API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>High availability in Docker Swarm</title>
      <link>http://docs.docker.com/swarm/multi-manager-setup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://docs.docker.com/swarm/multi-manager-setup/</guid>
      <description>

&lt;h1 id=&#34;high-availability-in-docker-swarm&#34;&gt;High availability in Docker Swarm&lt;/h1&gt;

&lt;p&gt;In Docker Swarm, the &lt;strong&gt;swarm manager&lt;/strong&gt; is responsible for the entire cluster and manages the resources of multiple &lt;em&gt;Docker hosts&lt;/em&gt; at scale. If the swarm manager dies, you must create a new one and deal with an interruption of service.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;High Availability&lt;/em&gt; feature allows a Docker Swarm to gracefully handle the failover of a manager instance. Using this feature, you can create a single &lt;strong&gt;primary manager&lt;/strong&gt; instance and multiple &lt;strong&gt;replica&lt;/strong&gt; instances.&lt;/p&gt;

&lt;p&gt;A primary manager is the main point of contact with the Docker Swarm cluster. You can also create and talk to replica instances that will act as backups. Requests issued on a replica are automatically proxied to the primary manager. If the primary manager fails, a replica takes away the lead. In this way, you always keep a point of contact with the cluster.&lt;/p&gt;

&lt;h2 id=&#34;setup-primary-and-replicas&#34;&gt;Setup primary and replicas&lt;/h2&gt;

&lt;p&gt;This section explains how to set up Docker Swarm using multiple &lt;strong&gt;managers&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&#34;assumptions&#34;&gt;Assumptions&lt;/h3&gt;

&lt;p&gt;You need either a &lt;code&gt;Consul&lt;/code&gt;, &lt;code&gt;etcd&lt;/code&gt;, or &lt;code&gt;Zookeeper&lt;/code&gt; cluster. This procedure is written assuming a &lt;code&gt;Consul&lt;/code&gt; server running on address &lt;code&gt;192.168.42.10:8500&lt;/code&gt;. All hosts will have a Docker Engine configured to listen on port 2375.  We will be configuring the Managers to operate on port 4000. The sample swarm configuration has three machines:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;manager-1&lt;/code&gt; on &lt;code&gt;192.168.42.200&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;manager-2&lt;/code&gt; on &lt;code&gt;192.168.42.201&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;manager-3&lt;/code&gt; on &lt;code&gt;192.168.42.202&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;create-the-primary-manager&#34;&gt;Create the primary manager&lt;/h3&gt;

&lt;p&gt;You use the &lt;code&gt;swarm manage&lt;/code&gt; command with the &lt;code&gt;--replication&lt;/code&gt; and &lt;code&gt;--advertise&lt;/code&gt; flags to create a primary manager.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  user@manager-1 $ swarm manage -H :4000 &amp;lt;tls-config-flags&amp;gt; --replication --advertise 192.168.42.200:4000 consul://192.168.42.10:8500/nodes
  INFO[0000] Listening for HTTP addr=:4000 proto=tcp
  INFO[0000] Cluster leadership acquired
  INFO[0000] New leader elected: 192.168.42.200:4000
  [...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The  &lt;code&gt;--replication&lt;/code&gt; flag tells swarm that the manager is part of a a multi-manager configuration and that this primary manager competes with other manager instances for the primary role. The primary manager has the authority to manage cluster, replicate logs, and replicate events happening inside the cluster.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;--advertise&lt;/code&gt; option specifies the primary manager address. Swarm uses this address to advertise to the cluster when the node is elected as the primary. As you see in the command&amp;rsquo;s output, the address you provided now appears to be the one of the elected Primary manager.&lt;/p&gt;

&lt;h3 id=&#34;create-two-replicas&#34;&gt;Create two replicas&lt;/h3&gt;

&lt;p&gt;Now that you have a primary manager, you can create replicas.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;user@manager-2 $ swarm manage -H :4000 &amp;lt;tls-config-flags&amp;gt; --replication --advertise 192.168.42.201:4000 consul://192.168.42.10:8500/nodes
INFO[0000] Listening for HTTP                            addr=:4000 proto=tcp
INFO[0000] Cluster leadership lost
INFO[0000] New leader elected: 192.168.42.200:4000
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command creates a replica manager on &lt;code&gt;192.168.42.201:4000&lt;/code&gt; which is looking at &lt;code&gt;192.168.42.200:4000&lt;/code&gt; as the primary manager.&lt;/p&gt;

&lt;p&gt;Create an additional, third &lt;em&gt;manager&lt;/em&gt; instance:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;user@manager-3 $ swarm manage -H :4000 &amp;lt;tls-config-flags&amp;gt; --replication --advertise 192.168.42.202:4000 consul://192.168.42.10:8500/nodes
INFO[0000] Listening for HTTP                            addr=:4000 proto=tcp
INFO[0000] Cluster leadership lost
INFO[0000] New leader elected: 192.168.42.200:4000
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once you have established your primary manager and the replicas, create &lt;strong&gt;swarm agents&lt;/strong&gt; as you normally would.&lt;/p&gt;

&lt;h3 id=&#34;list-machines-in-the-cluster&#34;&gt;List machines in the cluster&lt;/h3&gt;

&lt;p&gt;Typing &lt;code&gt;docker info&lt;/code&gt; should give you an output similar to the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;user@my-machine $ export DOCKER_HOST=192.168.42.200:4000 # Points to manager-1
user@my-machine $ docker info
Containers: 0
Images: 25
Storage Driver:
Role: Primary  &amp;lt;--------- manager-1 is the Primary manager
Primary: 192.168.42.200
Strategy: spread
Filters: affinity, health, constraint, port, dependency
Nodes: 3
 swarm-agent-0: 192.168.42.100:2375
  └ Containers: 0
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 2.053 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=3.13.0-49-generic, operatingsystem=Ubuntu 14.04.2 LTS, storagedriver=aufs
 swarm-agent-1: 192.168.42.101:2375
  └ Containers: 0
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 2.053 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=3.13.0-49-generic, operatingsystem=Ubuntu 14.04.2 LTS, storagedriver=aufs
 swarm-agent-2: 192.168.42.102:2375
  └ Containers: 0
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 2.053 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=3.13.0-49-generic, operatingsystem=Ubuntu 14.04.2 LTS, storagedriver=aufs
Execution Driver:
Kernel Version:
Operating System:
CPUs: 3
Total Memory: 6.158 GiB
Name:
ID:
Http Proxy:
Https Proxy:
No Proxy:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This information shows that &lt;code&gt;manager-1&lt;/code&gt; is the current primary and supplies the address to use to contact this primary.&lt;/p&gt;

&lt;h2 id=&#34;test-the-failover-mechanism&#34;&gt;Test the failover mechanism&lt;/h2&gt;

&lt;p&gt;To test the failover mechanism, you shut down the designated primary manager.
Issue a &lt;code&gt;Ctrl-C&lt;/code&gt; or &lt;code&gt;kill&lt;/code&gt; the current primary manager (&lt;code&gt;manager-1&lt;/code&gt;) to shut it down.&lt;/p&gt;

&lt;h3 id=&#34;wait-for-automated-failover&#34;&gt;Wait for automated failover&lt;/h3&gt;

&lt;p&gt;After a short time, the other instances detect the failure and a replica takes the &lt;em&gt;lead&lt;/em&gt; to become the primary manager.&lt;/p&gt;

&lt;p&gt;For example, look at &lt;code&gt;manager-2&lt;/code&gt;&amp;rsquo;s logs:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;user@manager-2 $ swarm manage -H :4000 &amp;lt;tls-config-flags&amp;gt; --replication --advertise 192.168.42.201:4000 consul://192.168.42.10:8500/nodes
INFO[0000] Listening for HTTP                            addr=:4000 proto=tcp
INFO[0000] Cluster leadership lost
INFO[0000] New leader elected: 192.168.42.200:4000
INFO[0038] New leader elected: 192.168.42.201:4000
INFO[0038] Cluster leadership acquired               &amp;lt;--- We have been elected as the new Primary Manager
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because the primary manager, &lt;code&gt;manager-1&lt;/code&gt;, failed right after it was elected, the replica with the address &lt;code&gt;192.168.42.201:4000&lt;/code&gt;, &lt;code&gt;manager-2&lt;/code&gt;, recognized the failure and attempted to take away the lead. Because &lt;code&gt;manager-2&lt;/code&gt; was fast enough, the process was effectively elected as the primary manager. As a result, &lt;code&gt;manager-2&lt;/code&gt; became the primary manager of the cluster.&lt;/p&gt;

&lt;p&gt;If we take a look at &lt;code&gt;manager-3&lt;/code&gt; we should see those &lt;code&gt;logs&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;user@manager-3 $ swarm manage -H :4000 &amp;lt;tls-config-flags&amp;gt; --replication --advertise 192.168.42.202:4000 consul://192.168.42.10:8500/nodes
INFO[0000] Listening for HTTP                            addr=:4000 proto=tcp
INFO[0000] Cluster leadership lost
INFO[0000] New leader elected: 192.168.42.200:4000
INFO[0036] New leader elected: 192.168.42.201:4000   &amp;lt;--- manager-2 sees the new Primary Manager
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At this point, we need to export the new &lt;code&gt;DOCKER_HOST&lt;/code&gt; value.&lt;/p&gt;

&lt;h3 id=&#34;switch-the-primary&#34;&gt;Switch the primary&lt;/h3&gt;

&lt;p&gt;To switch the &lt;code&gt;DOCKER_HOST&lt;/code&gt; to use &lt;code&gt;manager-2&lt;/code&gt; as the primary, you do the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;user@my-machine $ export DOCKER_HOST=192.168.42.201:4000 # Points to manager-2
user@my-machine $ docker info
Containers: 0
Images: 25
Storage Driver:
Role: Replica  &amp;lt;--------- manager-2 is a Replica
Primary: 192.168.42.200
Strategy: spread
Filters: affinity, health, constraint, port, dependency
Nodes: 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can use the &lt;code&gt;docker&lt;/code&gt; command on any Docker Swarm primary manager or any replica.&lt;/p&gt;

&lt;p&gt;If you like, you can use custom mechanisms to always point &lt;code&gt;DOCKER_HOST&lt;/code&gt; to the current primary manager. Then, you never lose contact with your Docker Swarm in the event of a failover.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>