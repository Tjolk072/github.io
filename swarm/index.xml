<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Swarms on Docker Docs</title>
    <link>https://docs.docker.com/v1.10/swarm/</link>
    <description>Recent content in Swarms on Docker Docs</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="https://docs.docker.com/v1.10/swarm/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Build a Swarm cluster for production</title>
      <link>https://docs.docker.com/v1.10/swarm/install-manual/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.docker.com/v1.10/swarm/install-manual/</guid>
      <description>

&lt;h1 id=&#34;build-a-swarm-cluster-for-production&#34;&gt;Build a Swarm cluster for production&lt;/h1&gt;

&lt;p&gt;This page teaches you to deploy a high-availability Docker Swarm cluster.
Although the example installation uses the Amazon Web Services (AWS) platform,
you can deploy an equivalent Docker Swarm cluster on many other platforms. In this example, you do the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Verify you have &lt;a href=&#34;#prerequisites&#34;&gt;prerequisites&lt;/a&gt; necessary to complete the example.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-1-add-network-security-rules&#34;&gt;Establish basic network security&lt;/a&gt; by creating a security group that restricts inbound traffic by port number, type, and origin.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-2-create-your-instances&#34;&gt;Create your hosts&lt;/a&gt; on your network by launching and configuring Elastic Cloud (EC2) instances.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-3-install-engine-on-each-node&#34;&gt;Install Docker Engine on each instance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-4-set-up-a-discovery-backend&#34;&gt;Create a discovery backend&lt;/a&gt; by running a consul container on one of the hosts.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-5-create-swarm-cluster&#34;&gt;Create a Swarm cluster&lt;/a&gt; by running two Swarm managers in a high-availability configuration.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-6-communicate-with-the-swarm&#34;&gt;Communicate with the Swarm&lt;/a&gt; and run a simple hello world application.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-7-test-swarm-failover&#34;&gt;Test the high-availability Swarm managers&lt;/a&gt; by causing a Swarm manager to fail.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For a gentler introduction to Swarm, try the &lt;a href=&#34;../v1.10/swarm/install-w-machine/&#34;&gt;Evaluate Swarm in a sandbox&lt;/a&gt; page.&lt;/p&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;An Amazon Web Services (AWS) account&lt;/li&gt;
&lt;li&gt;Familiarity with AWS features and tools, such as:

&lt;ul&gt;
&lt;li&gt;Elastic Cloud (EC2) Dashboard&lt;/li&gt;
&lt;li&gt;Virtual Private Cloud (VPC) Dashboard&lt;/li&gt;
&lt;li&gt;VPC Security groups&lt;/li&gt;
&lt;li&gt;Connecting to an EC2 instance using SSH&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;step-1-add-network-security-rules&#34;&gt;Step 1. Add network security rules&lt;/h2&gt;

&lt;p&gt;AWS uses a &amp;ldquo;security group&amp;rdquo; to allow specific types of network traffic on your
VPC network. The &lt;strong&gt;default&lt;/strong&gt; security group&amp;rsquo;s initial set of rules deny all
inbound traffic, allow all outbound traffic, and allow all traffic between
instances.&lt;/p&gt;

&lt;p&gt;You&amp;rsquo;re  going to add a couple of rules to allow inbound SSH connections and
inbound container images. This set of rules somewhat protects the Engine, Swarm,
and Consul ports. For a production environment, you would apply more restrictive
security measures. Do not leave Docker Engine ports unprotected.&lt;/p&gt;

&lt;p&gt;From your AWS home console, do the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Click &lt;strong&gt;VPC - Isolated Cloud Resources&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The VPC Dashboard opens.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Navigate to &lt;strong&gt;Security Groups&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Select the &lt;strong&gt;default&lt;/strong&gt; security group that&amp;rsquo;s associated with your default VPC.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Add the following two rules.&lt;/p&gt;

&lt;table&gt;
&lt;tr&gt;
  &lt;th&gt;Type&lt;/th&gt;
  &lt;th&gt;Protocol&lt;/th&gt;
  &lt;th&gt;Port Range&lt;/th&gt;
  &lt;th&gt;Source&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;SSH&lt;/td&gt;
  &lt;td&gt;TCP&lt;/td&gt;
  &lt;td&gt;22&lt;/td&gt;
  &lt;td&gt;0.0.0.0/0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;HTTP&lt;/td&gt;
  &lt;td&gt;TCP&lt;/td&gt;
  &lt;td&gt;80&lt;/td&gt;
  &lt;td&gt;0.0.0.0/0&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The SSH connection allows you to connect to the host while the HTTP is for container images.&lt;/p&gt;

&lt;h2 id=&#34;step-2-create-your-instances&#34;&gt;Step 2. Create your instances&lt;/h2&gt;

&lt;p&gt;In this step, you create five Linux hosts that are part of your default security
gorup. When complete, the example deployment contains three types of nodes:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Node Description&lt;/th&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Swarm primary and secondary managers&lt;/td&gt;
&lt;td&gt;&lt;code&gt;manager0&lt;/code&gt;,  &lt;code&gt;manager1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Swarm node&lt;/td&gt;
&lt;td&gt;&lt;code&gt;node0&lt;/code&gt;, &lt;code&gt;node1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Discovery backend&lt;/td&gt;
&lt;td&gt;&lt;code&gt;consul0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;To create the instances do the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Open the EC2 Dashboard and launch four EC2 instances, one at a time.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;During &lt;strong&gt;Step 1: Choose an Amazon Machine Image (AMI)&lt;/strong&gt;, pick the &lt;em&gt;Amazon Linux AMI&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;During &lt;strong&gt;Step 5: Tag Instance&lt;/strong&gt;, under &lt;strong&gt;Value&lt;/strong&gt;, give each instance one of these names:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;manager0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;manager1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;consul0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;During &lt;strong&gt;Step 6: Configure Security Group&lt;/strong&gt;, choose &lt;strong&gt;Select an existing security group&lt;/strong&gt; and pick the &amp;ldquo;default&amp;rdquo; security group.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Review and launch your instances.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;step-3-install-engine-on-each-node&#34;&gt;Step 3. Install Engine on each node&lt;/h2&gt;

&lt;p&gt;In this step, you install Docker Engine on each node. By installing Engine, you enable the Swarm manager to address the nodes via the Engine CLI and API.&lt;/p&gt;

&lt;p&gt;SSH to each node in turn and do the following.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Update the yum packages.&lt;/p&gt;

&lt;p&gt;Keep an eye out for the &amp;ldquo;y/n/abort&amp;rdquo; prompt:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo yum update
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Run the installation script.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -sSL https://get.docker.com/ | sh
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Configure and start Engine so it listens for Swarm nodes on port &lt;code&gt;2375&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker daemon -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Verify that Docker Engine is installed correctly:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker run hello-world
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output should display a &amp;ldquo;Hello World&amp;rdquo; message and other text without any
error messages.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Give the &lt;code&gt;ec2-user&lt;/code&gt; root privileges:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo usermod -aG docker ec2-user
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Enter &lt;code&gt;logout&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;troubleshooting&#34;&gt;Troubleshooting&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If entering a &lt;code&gt;docker&lt;/code&gt; command produces a message asking whether docker is
available on this host, it may be because the user doesn&amp;rsquo;t have root privileges.
If so, use &lt;code&gt;sudo&lt;/code&gt; or give the user root privileges.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For this example, don&amp;rsquo;t create an AMI image from one of your instances running
Docker Engine and then re-use it to create the other instances. Doing so will
produce errors.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If your host cannot reach Docker Hub, the &lt;code&gt;docker run&lt;/code&gt; commands that pull
container images may fail. In that case, check that your VPC is associated with
a security group with a rule that allows inbound traffic (e.g.,
HTTP/TCP/80/0.0.0.0/0). Also Check the &lt;a href=&#34;http://status.docker.com/&#34;&gt;Docker Hub status
page&lt;/a&gt; for service availability.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;step-4-set-up-a-discovery-backend&#34;&gt;Step 4. Set up a discovery backend&lt;/h2&gt;

&lt;p&gt;Here, you&amp;rsquo;re going to create a minimalist discovery backend. The Swarm managers
and nodes use this backend to authenticate themselves as members of the cluster.
The Swarm managers also use this information to identify which nodes are
available to run containers.&lt;/p&gt;

&lt;p&gt;To keep things simple, you are going to run a single consul daemon on the same
host as one of the Swarm managers.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;To start, copy the following launch command to a text file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d -p 8500:8500 --name=consul progrium/consul -server -bootstrap
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Use SSH to connect to the &lt;code&gt;manager0&lt;/code&gt; and &lt;code&gt;consul0&lt;/code&gt; instance.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ifconfig
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;From the output, copy the &lt;code&gt;eth0&lt;/code&gt; IP address from &lt;code&gt;inet addr&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Using SSH, connect to the &lt;code&gt;manager0&lt;/code&gt; and &lt;code&gt;etc0&lt;/code&gt; instance.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Paste the launch command you created in step 1. into the command line.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d -p 8500:8500 --name=consul progrium/consul -server -bootstrap
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Your Consul node is up and running, providing your cluster with a discovery
backend. To increase its reliability, you can create a high-availability cluster
using a trio of consul nodes using the link mentioned at the end of this page.
(Before creating a cluster of console nodes, update the VPC security group with
rules to allow inbound traffic on the required port numbers.)&lt;/p&gt;

&lt;h2 id=&#34;step-5-create-swarm-cluster&#34;&gt;Step 5. Create Swarm cluster&lt;/h2&gt;

&lt;p&gt;After creating the discovery backend, you can create the Swarm managers. In this step, you are going to create two Swarm managers in a high-availability configuration. The first manager you run becomes the Swarm&amp;rsquo;s &lt;em&gt;primary manager&lt;/em&gt;. Some documentation still refers to a primary manager as a &amp;ldquo;master&amp;rdquo;, but that term has been superseded. The second manager you run serves as a &lt;em&gt;replica&lt;/em&gt;. If the primary manager becomes unavailable, the cluster elects the replica as the primary manager.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;To create the primary manager in a high-availability Swarm cluster, use the following syntax:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d -p 4000:4000 swarm manage -H :4000 --replication --advertise &amp;lt;manager0_ip&amp;gt;:4000 consul://&amp;lt;consul_ip&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because this is particular manager is on the same &lt;code&gt;manager0&lt;/code&gt; and &lt;code&gt;consul0&lt;/code&gt;
instance as the consul node, replace both &lt;code&gt;&amp;lt;manager0_ip&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;consul_ip&amp;gt;&lt;/code&gt;
with the same IP address. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d -p 4000:4000 swarm manage -H :4000 --replication --advertise 172.30.0.161:4000 consul://172.30.0.161:8500
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Enter &lt;code&gt;docker ps&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;From the output, verify that both a swarm and a consul container are running.
Then, disconnect from the &lt;code&gt;manager0&lt;/code&gt; and &lt;code&gt;consul0&lt;/code&gt; instance.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Connect to the &lt;code&gt;manager1&lt;/code&gt; node and use &lt;code&gt;ifconfig&lt;/code&gt; to get its IP address.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ifconfig
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start the secondary Swarm manager using following command.&lt;/p&gt;

&lt;p&gt;Replacing &lt;code&gt;&amp;lt;manager1_ip&amp;gt;&lt;/code&gt; with the IP address from the previous command, for example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d swarm manage -H :4000 --replication --advertise &amp;lt;manager1_ip&amp;gt;:4000 consul://172.30.0.161:8500
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Enter &lt;code&gt;docker ps&lt;/code&gt;to verify that a Swarm container is running.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Connect to &lt;code&gt;node0&lt;/code&gt; and &lt;code&gt;node1&lt;/code&gt; in turn and join them to the cluster.&lt;/p&gt;

&lt;p&gt;a. Get the node IP addresses with the &lt;code&gt;ifconfig&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;b. Start a swarm container each using the following syntax:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d swarm join --advertise=&amp;lt;node_ip&amp;gt;:2375 consul://&amp;lt;consul_ip&amp;gt;:8500
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d swarm join --advertise=172.30.0.69:2375 consul://172.30.0.161:8500
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Your small Swarm cluster is up and running on multiple hosts, providing you with a high-availability virtual Docker Engine. To increase its reliability and capacity, you can add more Swarm managers, nodes, and a high-availability discovery backend.&lt;/p&gt;

&lt;h2 id=&#34;step-6-communicate-with-the-swarm&#34;&gt;Step 6. Communicate with the Swarm&lt;/h2&gt;

&lt;p&gt;You can communicate with the Swarm to get information about the managers and
nodes using the Swarm API, which is nearly the same as the standard Docker API.
In this example, you use SSL to connect to &lt;code&gt;manager0&lt;/code&gt; and &lt;code&gt;consul0&lt;/code&gt; host again.
Then, you address commands to the Swarm manager.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Get information about the master and nodes in the cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H :4000 info
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output gives the manager&amp;rsquo;s role as primary (&lt;code&gt;Role: primary&lt;/code&gt;) and
information about each of the nodes.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Run an application on the Swarm:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H :4000 run hello-world
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Check which Swarm node ran the application:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H :4000 ps
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;step-7-test-swarm-failover&#34;&gt;Step 7. Test Swarm failover&lt;/h2&gt;

&lt;p&gt;To see the replica instance take over, you&amp;rsquo;re going to shut down the primary
manager. Doing so kicks off an election, and the replica becomes the primary
manager. When you start the manager you shut down earlier, it becomes the
replica.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;SSH connection to the &lt;code&gt;manager0&lt;/code&gt; instance.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Get the container id or name of the swarm container:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker ps
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Shut down the primary master, replacing &lt;code&gt;&amp;lt;id_name&amp;gt;&lt;/code&gt; with the container id or name (for example, &amp;ldquo;8862717fe6d3&amp;rdquo; or &amp;ldquo;trusting_lamarr&amp;rdquo;).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker rm -f &amp;lt;id_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start the Swarm master. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d -p 4000:4000 swarm manage -H :4000 --replication --advertise 172.30.0.161:4000 consul://172.30.0.161:237
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Review the Engine&amp;rsquo;s daemon logs the logs, replacing &lt;code&gt;&amp;lt;id_name&amp;gt;&lt;/code&gt; with the new container id or name:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker logs &amp;lt;id_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output shows will show two entries like these ones:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;time=&amp;quot;2016-02-02T02:12:32Z&amp;quot; level=info msg=&amp;quot;Leader Election: Cluster leadership lost&amp;quot;
time=&amp;quot;2016-02-02T02:12:32Z&amp;quot; level=info msg=&amp;quot;New leader elected: 172.30.0.160:4000&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;To get information about the master and nodes in the cluster, enter:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H :4000 info
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can connect to the &lt;code&gt;master1&lt;/code&gt; node and run the &lt;code&gt;info&lt;/code&gt; and &lt;code&gt;logs&lt;/code&gt; commands.
They will display corresponding entries for the change in leadership.&lt;/p&gt;

&lt;h2 id=&#34;additional-resources&#34;&gt;Additional Resources&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://docs.docker.com/engine/installation/cloud/cloud-ex-aws/&#34;&gt;Installing Docker Engine on a cloud provider&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://goelzer.com/blog/2015/12/29/docker-swarmoverlay-networks-manual-method/&#34;&gt;Docker Swarm 1.0 with Multi-host Networking: Manual Setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/multi-manager-setup/&#34;&gt;High availability in Docker Swarm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/discovery/&#34;&gt;Discovery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hub.docker.com/r/progrium/consul/&#34;&gt;High-availability cluster using a trio of consul nodes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/swarm/networking/&#34;&gt;Networking&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Command line reference</title>
      <link>https://docs.docker.com/v1.10/swarm/reference/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.docker.com/v1.10/swarm/reference/</guid>
      <description>

&lt;h1 id=&#34;docker-swarm-command-line-reference&#34;&gt;Docker Swarm command line reference&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/reference/swarm/&#34;&gt;swarm&lt;/a&gt; — Run a Swarm container on Docker Engine&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/reference/create/&#34;&gt;create&lt;/a&gt; — Create a discovery token&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/reference/list/&#34;&gt;list&lt;/a&gt; — List the nodes in a Docker cluster&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/reference/manage/&#34;&gt;manage&lt;/a&gt; — Create a Swarm manager&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/reference/join/&#34;&gt;join&lt;/a&gt; — Create a Swarm node&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/reference/help/&#34;&gt;help&lt;/a&gt; — See a list of Swarm commands, or help for one command&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Configure Docker Swarm for TLS</title>
      <link>https://docs.docker.com/v1.10/swarm/configure-tls/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.docker.com/v1.10/swarm/configure-tls/</guid>
      <description>

&lt;h1 id=&#34;configure-docker-swarm-for-tls&#34;&gt;Configure Docker Swarm for TLS&lt;/h1&gt;

&lt;p&gt;In this procedure you create a two-node Swarm cluster, a Docker Engine CLI, a
Swarm Manager, and a Certificate Authority as shown below. All the Docker Engine
hosts (&lt;code&gt;client&lt;/code&gt;, &lt;code&gt;swarm&lt;/code&gt;, &lt;code&gt;node1&lt;/code&gt;, and &lt;code&gt;node2&lt;/code&gt;) have a copy of the
CA&amp;rsquo;s certificate as well as their own key-pair signed by the CA.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/tls-1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You will complete the following steps in this procedure:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#step-1-set-up-the-prerequisites&#34;&gt;Step 1: Set up the prerequisites&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-2-create-a-certificate-authority-ca-server&#34;&gt;Step 2: Create a Certificate Authority (CA) server&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-3-create-and-sign-keys&#34;&gt;Step 3: Create and sign keys&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-4-install-the-keys&#34;&gt;Step 4: Install the keys&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-5-configure-the-engine-daemon-for-tls&#34;&gt;Step 5: Configure the Engine daemon for TLS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-6-create-a-swarm-cluster&#34;&gt;Step 6: Create a Swarm cluster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-7-create-the-swarm-manager-using-tls&#34;&gt;Step 7: Create the Swarm Manager using TLS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-8-test-the-swarm-manager-configuration&#34;&gt;Step 8: Test the Swarm manager configuration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-9-configure-the-engine-cli-to-use-tls&#34;&gt;Step 9: Configure the Engine CLI to use TLS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;before-you-begin&#34;&gt;Before you begin&lt;/h3&gt;

&lt;p&gt;The article includes steps to create your own CA using OpenSSL. This is similar
to operating your own internal corporate CA and PKI. However, this &lt;code&gt;must not&lt;/code&gt;
be used as a guide to building a production-worthy internal CA and PKI. These
steps are included for demonstration purposes only - so that readers without
access to an existing CA and set of certificates can follow along and configure
Docker Swarm to use TLS.&lt;/p&gt;

&lt;h2 id=&#34;step-1-set-up-the-prerequisites&#34;&gt;Step 1: Set up the prerequisites&lt;/h2&gt;

&lt;p&gt;To complete this procedure you must stand up 5 (five) Linux servers. These
servers can be any mix of physical and virtual servers; they may be on premises
or in the public cloud. The following table lists each server name and its purpose.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Server name&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ca&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Acts as the Certificate Authority (CA) server.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;swarm&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Acts as the Swarm Manager.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;node1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Act as a Swarm node.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;node2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Act as a Swarm node.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;client&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Acts as a remote Docker Engine client&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Make sure that you have SSH access to all 5 servers and that they can communicate with each other using DNS name resolution. In particular:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Open TCP port 2376 between the Swarm Manager and Swarm nodes&lt;/li&gt;
&lt;li&gt;Open TCP port 3376 between the Docker Engine client and the Swarm Manager&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can choose different ports if these are already in use. This example assumes
you use these ports though.&lt;/p&gt;

&lt;p&gt;Each server must run an operating system compatible with Docker Engine. For
simplicity, the steps that follow assume all servers are running Ubuntu 14.04
LTS.&lt;/p&gt;

&lt;h2 id=&#34;step-2-create-a-certificate-authority-ca-server&#34;&gt;Step 2: Create a Certificate Authority (CA) server&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:If you already have access to a CA and certificates, and are comfortable working with them, you should skip this step and go to the next.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In this step, you configure a Linux server as a CA. You use this CA to create
and sign keys. This step included so that readers without access to an existing
CA (external or corpoate) and certificates can follow along and complete the
later steps that require installing and using certificates. It is &lt;code&gt;not&lt;/code&gt;
intended as a model for how to deploy production-worthy CA.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Logon to the terminal of your CA server and elevate to root.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo su
# 
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create a private key called &lt;code&gt;ca-priv-key.pem&lt;/code&gt; for the CA:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# openssl genrsa -out ca-priv-key.pem 2048
Generating RSA private key, 2048 bit long modulus
...........................................................+++
.....+++
e is 65537 (0x10001)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create a public key called &lt;code&gt;ca.pem&lt;/code&gt; for the CA.&lt;/p&gt;

&lt;p&gt;The public key is based on the private key created in the previous step.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# openssl req -config /usr/lib/ssl/openssl.cnf -new -key ca-priv-key.pem -x509 -days 1825 -out ca.pem
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter &#39;.&#39;, the field will be left blank.
-----
Country Name (2 letter code) [AU]:US
&amp;lt;output truncated&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You have now configured a CA server with a public and private keypair. You can inspect the contents of each key. To inspect the private key:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# openssl rsa -in ca-priv-key.pem -noout -text
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To inspect the public key (cert): `&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# openssl x509 -in ca.pem -noout -text`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following command shows the partial contents of the CA&amp;rsquo;s public key.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# openssl x509 -in ca.pem -noout -text
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number: 17432010264024107661 (0xf1eaf0f9f41eca8d)
    Signature Algorithm: sha256WithRSAEncryption
        Issuer: C=US, ST=CA, L=Sanfrancisco, O=Docker Inc
        Validity
            Not Before: Jan 16 18:28:12 2016 GMT
            Not After : Jan 13 18:28:12 2026 GMT
        Subject: C=US, ST=CA, L=San Francisco, O=Docker Inc
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                Public-Key: (2048 bit)
                Modulus:
                    00:d1:fe:6e:55:d4:93:fc:c9:8a:04:07:2d:ba:f0:
                    55:97:c5:2c:f5:d7:1d:6a:9b:f0:f0:55:6c:5d:90:
&amp;lt;output truncated&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Later, you&amp;rsquo;ll use this to certificate to sign keys for other servers in the
infrastructure.&lt;/p&gt;

&lt;h2 id=&#34;step-3-create-and-sign-keys&#34;&gt;Step 3: Create and sign keys&lt;/h2&gt;

&lt;p&gt;Now that you have a working CA, you need to create key pairs for the Swarm
Manager, Swarm nodes, and remote Docker Engine client. The commands and process
to create key pairs is identical for all servers.  You&amp;rsquo;ll create the following keys:&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;code&gt;ca-priv-key.pem&lt;/td&gt;
    &lt;td&gt;The CA&#39;s private key and must be kept secure. It is used later to sign new keys for the other nodes in the environment. Together with the &lt;code&gt;ca.pem&lt;/code&gt; file, this makes up the CA&#39;s key pair.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;code&gt;ca.pem&lt;/td&gt;
    &lt;td&gt;The CA&#39;s public key (also called certificate). This is installed on all nodes in the environment so that all nodes trust certificates signed by the CA. Together with the &lt;code&gt;ca-priv-key.pem&lt;/code&gt; file, this makes up the CA&#39;s key pair.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;code&gt;&lt;i&gt;node&lt;/i&gt;.csr&lt;/code&gt;&lt;/td&gt;
    &lt;td&gt;A certificate signing request (CSR). A CSR is effectively an application to the CA to create a new key pair for a particular node. The CA takes the information provided in the CSR and generates the public and private key pair for that node.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;code&gt;&lt;i&gt;node&lt;/i&gt;-priv.key&lt;/code&gt;&lt;/td&gt;
    &lt;td&gt;A private key signed by the CA. The node uses this key to authenticate itself with remote Docker Engines. Together with the &lt;code&gt;&lt;i&gt;node&lt;/i&gt;-cert.pem&lt;/code&gt; file, this makes up a node&#39;s key pair.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;code&gt;&lt;i&gt;node&lt;/i&gt;-cert.pem&lt;/code&gt;&lt;/td&gt;
    &lt;td&gt;A certificate signed by the CA. This is not used in this example. Together with the &lt;code&gt;&lt;i&gt;node&lt;/i&gt;-priv.key&lt;/code&gt; file, this makes up a node&#39;s key pair&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The commands below show how to create keys for all of your nodes. You perform this procedure in a working directory located on your CA server.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Logon to the terminal of your CA server and elevate to root.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo su
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create a private key &lt;code&gt;swarm-priv-key.pem&lt;/code&gt; for your Swarm Manager&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# openssl genrsa -out swarm-priv-key.pem 2048
Generating RSA private key, 2048 bit long modulus
............................................................+++
........+++
e is 65537 (0x10001)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Generate a certificate signing request (CSR) &lt;code&gt;swarm.csr&lt;/code&gt; using the private key you create in the previous step.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# openssl req -subj &amp;quot;/CN=swarm&amp;quot; -new -key swarm-priv-key.pem -out swarm.csr
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Remember, this is only for demonstration purposes. The process to create a
CSR will be slightly different in real-world production environments.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create the certificate &lt;code&gt;swarm-cert.pem&lt;/code&gt; based on the CSR created in the previous step.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# openssl x509 -req -days 1825 -in swarm.csr -CA ca.pem -CAkey ca-priv-key.pem -CAcreateserial -out swarm-cert.pem -extensions v3_req -extfile /usr/lib/ssl/openssl.cnf
&amp;lt;snip&amp;gt;
# openssl rsa -in swarm-priv-key.pem -out swarm-priv-key.pem
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You now have a keypair for the Swarm Manager.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Repeat the steps above for the remaining nodes in your infrastructure (&lt;code&gt;node1&lt;/code&gt;, &lt;code&gt;node2&lt;/code&gt;, and &lt;code&gt;client&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Remember to replace the &lt;code&gt;swarm&lt;/code&gt; specific values with the values relevant to the node you are creating the key pair for.&lt;/p&gt;

&lt;table&gt;
&lt;tr&gt;
&lt;th&gt;Server name&lt;/th&gt;
&lt;th&gt;Private key&lt;/th&gt;
&lt;th&gt;CSR&lt;/th&gt;
&lt;th&gt;Certificate&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;node1 &lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;node1-priv-key.pem&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;node1.csr&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;node1-cert.pem&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;node2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;node2-priv-key.pem&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;node2.csr&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;node2-cert.pem&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;client&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;client-priv-key.pem&lt;/td&gt;
&lt;td&gt;&lt;code&gt;client.csr&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;client-cert.pem&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Verify that your working directory contains the following files:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# ls -l
total 64
-rw-r--r-- 1 root   root   1679 Jan 16 18:27 ca-priv-key.pem
-rw-r--r-- 1 root   root   1229 Jan 16 18:28 ca.pem
-rw-r--r-- 1 root   root     17 Jan 18 09:56 ca.srl
-rw-r--r-- 1 root   root   1086 Jan 18 09:56 client-cert.pem
-rw-r--r-- 1 root   root    887 Jan 18 09:55 client.csr
-rw-r--r-- 1 root   root   1679 Jan 18 09:56 client-priv-key.pem
-rw-r--r-- 1 root   root   1082 Jan 18 09:44 node1-cert.pem
-rw-r--r-- 1 root   root    887 Jan 18 09:43 node1.csr
-rw-r--r-- 1 root   root   1675 Jan 18 09:44 node1-priv-key.pem
-rw-r--r-- 1 root   root   1082 Jan 18 09:49 node2-cert.pem
-rw-r--r-- 1 root   root    887 Jan 18 09:49 node2.csr
-rw-r--r-- 1 root   root   1675 Jan 18 09:49 node2-priv-key.pem
-rw-r--r-- 1 root   root   1082 Jan 18 09:42 swarm-cert.pem
-rw-r--r-- 1 root   root    887 Jan 18 09:41 swarm.csr
-rw-r--r-- 1 root   root   1679 Jan 18 09:42 swarm-priv-key.pem
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can inspect the contents of each of the keys. To inspect a private key:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openssl rsa -in &amp;lt;key-name&amp;gt; -noout -text
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To inspect a public key (cert):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openssl x509 -in &amp;lt;key-name&amp;gt; -noout -text
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following commands shows the partial contents of the Swarm Manager&amp;rsquo;s public
 &lt;code&gt;swarm-cert.pem&lt;/code&gt; key.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# openssl x509 -in ca.pem -noout -text
Certificate:
Data:
    Version: 3 (0x2)
    Serial Number: 9590646456311914051 (0x8518d2237ad49e43)
Signature Algorithm: sha256WithRSAEncryption
    Issuer: C=US, ST=CA, L=Sanfrancisco, O=Docker Inc
    Validity
        Not Before: Jan 18 09:42:16 2016 GMT
        Not After : Jan 15 09:42:16 2026 GMT
    Subject: CN=swarm

&amp;lt;output truncated&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-4-install-the-keys&#34;&gt;Step 4: Install the keys&lt;/h2&gt;

&lt;p&gt;In this step, you install the keys on the relevant servers in the
infrastructure. Each server needs three files:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A copy of the Certificate Authority&amp;rsquo;s public key (&lt;code&gt;ca.pem&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s own private key&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s own public key (cert)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The procedure below shows you how to copy these files from the CA server to each
server using &lt;code&gt;scp&lt;/code&gt;. As part of the copy procedure, you&amp;rsquo;ll rename each file as
follows on each node:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Original name&lt;/th&gt;
&lt;th&gt;Copied name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ca.pem&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ca.pem&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;&amp;lt;server&amp;gt;-cert.pem&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;cert.pem&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;&amp;lt;server&amp;gt;-priv-key.pem&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;key.pem&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Logon to the terminal of your CA server and elevate to root.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo su
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create a&lt;code&gt;~/.certs&lt;/code&gt; directory on the Swarm manager. Here we assume user account is ubuntu.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh ubuntu@swarm &#39;mkdir -p /home/ubuntu/.certs&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Copy the keys from the CA to the Swarm Manager server.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ scp ./ca.pem ubuntu@swarm:/home/ubuntu/.certs/ca.pem
$ scp ./swarm-cert.pem ubuntu@swarm:/home/ubuntu/.certs/cert.pem
$ scp ./swarm-priv-key.pem ubuntu@swarm:/home/ubuntu/.certs/key.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: You may need to provide authentication for the &lt;code&gt;scp&lt;/code&gt; commands to work. For example, AWS EC2 instances use certificate-based authentication. To copy the files to an EC2 instance associated with a public key called &lt;code&gt;nigel.pem&lt;/code&gt;, modify the &lt;code&gt;scp&lt;/code&gt; command as follows: &lt;code&gt;scp -i /path/to/nigel.pem ./ca.pem ubuntu@swarm:/home/ubuntu/.certs/ca.pem&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Repeat step 2 for each remaining  server in the infrastructure.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;node1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;client&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Verify your work.&lt;/p&gt;

&lt;p&gt;When the copying is complete, each machine should have the following keys.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/tls-2.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Each node in your infrastructure should have the following files in the
&lt;code&gt;/home/ubuntu/.certs/&lt;/code&gt; directory:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# ls -l /home/ubuntu/.certs/
total 16
-rw-r--r-- 1 ubuntu ubuntu 1229 Jan 18 10:03 ca.pem
-rw-r--r-- 1 ubuntu ubuntu 1082 Jan 18 10:06 cert.pem
-rw-r--r-- 1 ubuntu ubuntu 1679 Jan 18 10:06 key.pem
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;step-5-configure-the-engine-daemon-for-tls&#34;&gt;Step 5: Configure the Engine daemon for TLS&lt;/h2&gt;

&lt;p&gt;In the last step, you created and installed the necessary keys on each of your
Swarm nodes. In this step, you configure them to listen on the network and only
accept connections using TLS. Once you complete this step, your Swarm nodes will
listen on TCP port 2376, and only accept connections using TLS.&lt;/p&gt;

&lt;p&gt;On &lt;code&gt;node1&lt;/code&gt; and &lt;code&gt;node2&lt;/code&gt; (your Swarm nodes), do the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Open a terminal on &lt;code&gt;node1&lt;/code&gt; and elevate to root.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo su
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Edit Docker Engine configuration file.&lt;/p&gt;

&lt;p&gt;If you are following along with these instructions and using Ubuntu 14.04
LTS, the configuration file is &lt;code&gt;/etc/default/docker&lt;/code&gt;. The Docker Engine
configuration file may be different depending on the Linux distribution you
are using.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Add the following options to the &lt;code&gt;DOCKER_OPTS&lt;/code&gt; line.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; -H tcp://0.0.0.0:2376 --tlsverify --tlscacert=/home/ubuntu/.certs/ca.pem --tlscert=/home/ubuntu/.certs/cert.pem --tlskey=/home/ubuntu/.certs/key.pem
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Restart the Docker Engine daemon.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; $ service docker restart
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Repeat the procedure on &lt;code&gt;node2&lt;/code&gt; as well.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;step-6-create-a-swarm-cluster&#34;&gt;Step 6: Create a Swarm cluster&lt;/h2&gt;

&lt;p&gt;Next create a Swarm cluster. In this procedure you create a two-node Swarm
cluster using the default &lt;em&gt;hosted discovery&lt;/em&gt; backend. The default hosted
discovery backend uses Docker Hub and is not recommended for production use.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Logon to the terminal of your Swarm manager node.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create the cluster and export it&amp;rsquo;s unique ID to the &lt;code&gt;TOKEN&lt;/code&gt; environment variable.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo export TOKEN=$(docker run --rm swarm create)
Unable to find image &#39;swarm:latest&#39; locally
latest: Pulling from library/swarm
d681c900c6e3: Pulling fs layer
&amp;lt;snip&amp;gt;
986340ab62f0: Pull complete
a9975e2cc0a3: Pull complete
Digest: sha256:c21fd414b0488637b1f05f13a59b032a3f9da5d818d31da1a4ca98a84c0c781b
Status: Downloaded newer image for swarm:latest
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Join &lt;code&gt;node1&lt;/code&gt; to the cluster.&lt;/p&gt;

&lt;p&gt;Be sure to specify TCP port &lt;code&gt;2376&lt;/code&gt; and not &lt;code&gt;2375&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker run -d swarm join --addr=node1:2376 token://$TOKEN
7bacc98536ed6b4200825ff6f4004940eb2cec891e1df71c6bbf20157c5f9761
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Join &lt;code&gt;node2&lt;/code&gt; to the cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker run -d swarm join --addr=node2:2376 token://$TOKEN
db3f49d397bad957202e91f0679ff84f526e74d6c5bf1b6734d834f5edcbca6c
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;step-7-start-the-swarm-manager-using-tls&#34;&gt;Step 7: Start the Swarm Manager using TLS&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Launch a new container with TLS enables&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d -p 3376:3376 -v /home/ubuntu/.certs:/certs:ro swarm manage --tlsverify --tlscacert=/certs/ca.pem --tlscert=/certs/cert.pem --tlskey=/certs/key.pem --host=0.0.0.0:3376 token://$TOKEN
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The command above launches a new container based on the &lt;code&gt;swarm&lt;/code&gt; image
and it maps port &lt;code&gt;3376&lt;/code&gt; on the server to port &lt;code&gt;3376&lt;/code&gt; inside the
container. This mapping ensures that Docker Engine commands sent to the host
on port &lt;code&gt;3376&lt;/code&gt; are passed on to port &lt;code&gt;3376&lt;/code&gt; inside the container. The
container runs the Swarm &lt;code&gt;manage&lt;/code&gt; process with the &lt;code&gt;--tlsverify&lt;/code&gt;,
&lt;code&gt;--tlscacert&lt;/code&gt;, &lt;code&gt;--tlscert&lt;/code&gt; and &lt;code&gt;--tlskey&lt;/code&gt; options specified. These options
force TLS verification and specify the location of the Swarm manager&amp;rsquo;s TLS
keys.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Run a &lt;code&gt;docker ps&lt;/code&gt; command to verify that your Swarm manager container is up
and running.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker ps
CONTAINER ID   IMAGE               COMMAND                  CREATED          STATUS          PORTS                              NAMES
035dbf57b26e   swarm               &amp;quot;/swarm manage --tlsv&amp;quot;   7 seconds ago    Up 7 seconds    2375/tcp, 0.0.0.0:3376-&amp;gt;3376/tcp   compassionate_lovelace
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Your Swarm cluster is now configured to use TLS.&lt;/p&gt;

&lt;h2 id=&#34;step-8-test-the-swarm-manager-configuration&#34;&gt;Step 8: Test the Swarm manager configuration&lt;/h2&gt;

&lt;p&gt;Now that you have a Swarm cluster built and configured to use TLS, you&amp;rsquo;ll test that it works with a Docker Engine CLI.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Open a terminal onto your &lt;code&gt;client&lt;/code&gt; server.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Issue the &lt;code&gt;docker version&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;When issuing the command, you must pass it the location of the clients certifications.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker --tlsverify --tlscacert=/home/ubuntu/.certs/ca.pem --tlscert=/home/ubuntu/.certs/cert.pem --tlskey=/home/ubuntu/.certs/key.pem -H swarm:3376 version
Client:
 Version:      1.9.1
 API version:  1.21
 Go version:   go1.4.2
 Git commit:   a34a1d5
 Built:        Fri Nov 20 13:12:04 UTC 2015
 OS/Arch:      linux/amd64

Server:
 Version:      swarm/1.0.1
 API version:  1.21
 Go version:   go1.5.2
 Git commit:   744e3a3
 Built:
 OS/Arch:      linux/amd64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output above shows the &lt;code&gt;Server&lt;/code&gt; version as &amp;ldquo;swarm/1.0.1&amp;rdquo;. This means
that the command was successfully issued against the Swarm manager.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Verify that the same command does not work without TLS.&lt;/p&gt;

&lt;p&gt;This time, do not pass your certs to the Swarm manager.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker -H swarm:3376 version
:
 Version:      1.9.1
 API version:  1.21
 Go version:   go1.4.2
 Git commit:   a34a1d5
 Built:        Fri Nov 20 13:12:04 UTC 2015
 OS/Arch:      linux/amd64
Get http://swarm:3376/v1.21/version: malformed HTTP response &amp;quot;\x15\x03\x01\x00\x02\x02&amp;quot;.
* Are you trying to connect to a TLS-enabled daemon without TLS?
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output above shows that the command was rejected by the server. This is
because the server (Swarm manager) is configured to only accept connections
from authenticated clients using TLS.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;step-9-configure-the-engine-cli-to-use-tls&#34;&gt;Step 9: Configure the Engine CLI to use TLS&lt;/h2&gt;

&lt;p&gt;You can configure the Engine so that you don&amp;rsquo;t have to pass the TLS options when
you issue a command. To do this, you&amp;rsquo;ll configure the &lt;code&gt;Docker Engine host&lt;/code&gt; and
&lt;code&gt;TLS&lt;/code&gt; settings as defaults on your Docker Engine client.&lt;/p&gt;

&lt;p&gt;To do this, you place the client&amp;rsquo;s keys in your &lt;code&gt;~/.docker&lt;/code&gt; configuration folder. If you have other users on your system using the Engine command line, you&amp;rsquo;ll need to configure their account&amp;rsquo;s &lt;code&gt;~/.docker&lt;/code&gt; as well. The procedure below shows how to do this for the &lt;code&gt;ubuntu&lt;/code&gt; user on
your Docker Engine client.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Open a terminal onto your &lt;code&gt;client&lt;/code&gt; server.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If it doesn&amp;rsquo;t exist, create a &lt;code&gt;.docker&lt;/code&gt; directory in the &lt;code&gt;ubuntu&lt;/code&gt; user&amp;rsquo;s home directory.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir /home/ubuntu/.docker
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Copy the Docker Engine client&amp;rsquo;s keys from &lt;code&gt;/home/ubuntu/.certs&lt;/code&gt; to
&lt;code&gt;/home/ubuntu/.docker&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cp /home/ubuntu/.certs/{ca,cert,key}.pem /home/ubuntu/.docker
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Edit the account&amp;rsquo;s &lt;code&gt;~/.bash_profile&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Set the following variables:&lt;/p&gt;

&lt;table&gt;
&lt;tr&gt;
&lt;th&gt;Variable&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;DOCKER_HOST&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Sets the Docker host and TCP port to send all Engine commands to.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;DOCKER_TLS_VERIFY&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Tell&#39;s Engine to use TLS.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;DOCKER_CERT_PATH&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Specifies the location of TLS keys.&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    export DOCKER_HOST=tcp://swarm:3376
    export DOCKER_TLS_VERIFY=1
    export DOCKER_CERT_PATH=/home/ubuntu/.docker/
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Save and close the file.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Source the file to pick up the new variables.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    $ source ~/.bash_profile
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Verify that the procedure worked by issuing a &lt;code&gt;docker version&lt;/code&gt; command&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker version
Client:
 Version:      1.9.1
 API version:  1.21
 Go version:   go1.4.2
 Git commit:   a34a1d5
 Built:        Fri Nov 20 13:12:04 UTC 2015
 OS/Arch:      linux/amd64

Server:
 Version:      swarm/1.0.1
 API version:  1.21
 Go version:   go1.5.2
 Git commit:   744e3a3
 Built:
 OS/Arch:      linux/amd64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The server portion of the output above command shows that your Docker
client is issuing commands to the Swarm Manager and using TLS.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Congratulations! You have configured a Docker Swarm cluster to use TLS.&lt;/p&gt;

&lt;h2 id=&#34;related-information&#34;&gt;Related Information&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/secure-swarm-tls/&#34;&gt;Secure Docker Swarm with TLS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/engine/security/security/&#34;&gt;Docker security&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Deploy network infrastructure</title>
      <link>https://docs.docker.com/v1.10/swarm/swarm_at_scale/02-deploy-infra/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.docker.com/v1.10/swarm/swarm_at_scale/02-deploy-infra/</guid>
      <description>

&lt;h1 id=&#34;deploy-your-infrastructure&#34;&gt;Deploy your infrastructure&lt;/h1&gt;

&lt;p&gt;In this step, you create an AWS Virtual Private Cloud (VPC) to run your
application stack on. Before you continue, make sure you have taken the time to
&lt;a href=&#34;../v1.10/swarm/swarm_at_scale/01-about/&#34;&gt;learn the application architecture&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This example uses AWS but the AWS provider is only one example of an
infrastructure you can use. You can create the environment design on whatever
infrastructure you wish. For example, you could place the application on another
public cloud platform such as Azure or DigitalOcean, on premises in your data
center, or even in in a test environment on your laptop.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you are not deploying to AWS, or are not using the CloudFormation
template used in the instructions below, make sure your Docker hosts are running
a 3.16 or higher kernel. This kernel is required by Docker&amp;rsquo;s container
networking feature.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;overview-of-the-deployment-process&#34;&gt;Overview of the deployment process&lt;/h2&gt;

&lt;p&gt;To deploy on an AWS infrastructure, you first build a VPC and then apply apply
the &lt;a href=&#34;https://github.com/docker/swarm-microservice-demo-v1/blob/master/AWS/cloudformation.json&#34;&gt;CloudFormation template&lt;/a&gt; prepared for you. The template describes the hosts in the example&amp;rsquo;s stack. While you
could create the entire VPC and all instances via a CloudFormation template,
splitting the deployment into two steps lets you use the CloudFormation template
to build the stack on an &lt;em&gt;existing VPCs&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The diagram below shows the VPC infrastructure required to run the
CloudFormation template.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/cloud-formation-tmp.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The configuration is a single VPC with a single public subnet. The VPC
deployment relies on a &lt;a
href=&#34;https://raw.githubusercontent.com/docker/swarm-microservice-demo-v1/master/AWS/cloudformation.json&#34;&gt;cloudformation.json
template&lt;/a&gt; which specifies in the &lt;code&gt;us-west-1&lt;/code&gt; Region (N. California) or
&lt;code&gt;us-west-2&lt;/code&gt; (Oregon). The ability to create instances one of these regions is
&lt;strong&gt;required&lt;/strong&gt; for this particular CloudFormation template to work. If you want to
use a different region, edit the template before the import step.&lt;/p&gt;

&lt;p&gt;The VPC network address space is &lt;code&gt;192.168.0.0/16&lt;/code&gt; and single 24-bit public
subnet is carved out as 192.168.33.0/24. The subnet must be configured with a
default route to the internet via the VPC&amp;rsquo;s internet gateway. All six EC2
instances are deployed into this public subnet.&lt;/p&gt;

&lt;p&gt;Once the VPC is created, you deploy the EC2 instances using the
CloudFormation template located
&lt;a href=&#34;https://github.com/docker/swarm-microservice-demo-v1/blob/master/AWS/cloudformation.json&#34;&gt;in the &lt;code&gt;docker/swarm-microservice-demo-v1&lt;/code&gt; repository&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;p&gt;You&amp;rsquo;ll need to have an Amazon AWS account. This account can be personal or
through a corporate instance. The account must be able to deploy EC2 instances
in the &lt;code&gt;us-west-1&lt;/code&gt; region (N. California).&lt;/p&gt;

&lt;p&gt;Before starting through this procedure, make sure you have an existing EC2 key
pair in the &lt;code&gt;us-west-1&lt;/code&gt; region and that you have download its &lt;code&gt;.pem&lt;/code&gt; file.  If
you aren&amp;rsquo;t sure, login into AWS. Then, &lt;a
href=&#34;http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html&#34;
target=&#34;_blank&#34;&gt;follow the AWS documentation&lt;/a&gt; to ensure you have the key pair
and have downloaded the &lt;code&gt;.pem&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;Git clone the &lt;a href=&#34;https://github.com/docker/swarm-microservice-demo-v1&#34;
target=&#34;_blank&#34;&gt;example application&amp;rsquo;s GitHub repo&lt;/a&gt; to your local machine. If
you prefer, you can instead &lt;a href=&#34;https://github.com/docker/swarm-microservice-demo-v1/archive/master.zip&#34;&gt;download a &lt;code&gt;zip&lt;/code&gt;
file&lt;/a&gt;
and unzip the code in your local environment.&lt;/p&gt;

&lt;h2 id=&#34;step-1-build-and-configure-the-vpc&#34;&gt;Step 1. Build and configure the VPC&lt;/h2&gt;

&lt;p&gt;This step shows you using the VPC wizard on Amazon. If you prefer to build the
VPC manually, configure your VPC with the following values:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;VPC Network (CIDR)&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;192.168.0.0/16&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;VPC Name&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;swarm-scale&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Subnet network (CIDR)&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;192.168.33.0/24&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Availability Zone&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;N. California (us-west-1a or b)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Subnet name&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;publicSwarm&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;DNS resolution&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Subnet type&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Public (with route to the internet)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Availability Zone&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Any&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Auto-assign public IP&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Router&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;A single router with a route for &lt;em&gt;local&lt;/em&gt; traffic and default route for traffic to the internet&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Internet gateway&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;A single internet gateway used as default route for the subnet&amp;rsquo;s routing table&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;To build the VPC, with the wizard.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Go to the VPC dashboard.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Choose &lt;strong&gt;Start VPC Wizard&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Make sure &lt;strong&gt;VPC with a Single Public Subnet&lt;/strong&gt; is selected.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/vpc-create-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Click &lt;strong&gt;Select&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The browser displays the &lt;strong&gt;Step 2: VPC with a Single Public Subnet&lt;/strong&gt; dialog.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Complete the dialog as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/vpc-create-2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Click &lt;strong&gt;Create VPC&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;AWS works to build the VPC and then presents you with the &lt;strong&gt;VPC Successfully
Created&lt;/strong&gt; page.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Click &lt;strong&gt;OK&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Choose &lt;strong&gt;Subnets&lt;/strong&gt; from the &lt;strong&gt;VPC Dashboard&lt;/strong&gt; menu.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Locate your &lt;code&gt;publicSwarm&lt;/code&gt; subnet.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Choose &lt;strong&gt;Subnet Actions &amp;gt; Modify Auto-Assign Public IP&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/vpc-create-3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Select &lt;strong&gt;Enable auto-assign Public IP&lt;/strong&gt; and click &lt;strong&gt;Save&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the next step, you configure the remaining AWS settings by using a
CloudFormation template.&lt;/p&gt;

&lt;h2 id=&#34;step-2-build-the-network-stack&#34;&gt;Step 2. Build the network stack&lt;/h2&gt;

&lt;p&gt;In this step, you use CloudFormation template to build a stack on AWS. Before
you begin, make sure you have the prerequisites:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;access to a private key pair associated with your AWS account.&lt;/li&gt;
&lt;li&gt;a clone or download of the &lt;a
href=&#34;https://github.com/docker/swarm-microservice-demo-v1&#34; target=&#34;_blank&#34;&gt;the
example code&lt;/a&gt; on your local machine.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then, do the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Go to the AWS console and choose &lt;strong&gt;CloudFormation&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/vpc-create-4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Click &lt;strong&gt;Create Stack&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Under &lt;strong&gt;Choose a template&lt;/strong&gt; click the &lt;strong&gt;Choose file&lt;/strong&gt; button.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Browse to the download sample code and choose the  the &lt;code&gt;swarm-microservice-demo-v1/AWS/cloudformation.json&lt;/code&gt; CloudFormation template.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/vpc-create-5.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The system pre-populates most of the  &lt;strong&gt;Specify Details&lt;/strong&gt; dialog from the template.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Name the stack &lt;code&gt;VotingAppStack&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;You can name the stack something else if you want just make sure it is meaningful.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Select your key pair from the &lt;strong&gt;KeyName&lt;/strong&gt;  dropdown.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Select the &lt;code&gt;publicSwarm&lt;/code&gt; for the &lt;strong&gt;Subnetid&lt;/strong&gt; dropdown menu.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Select &lt;code&gt;swarm-scale&lt;/code&gt; from the &lt;strong&gt;Vpcid&lt;/strong&gt; dropdown menu.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Click &lt;strong&gt;Next&lt;/strong&gt; twice to reach the &lt;strong&gt;Review&lt;/strong&gt; page.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Check the values.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Template URL&lt;/strong&gt;,&lt;strong&gt;SubnetId&lt;/strong&gt; and &lt;strong&gt;VpcId&lt;/strong&gt; are always unique, so yours
will not match, but otherwise you should see the following:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/vpc-create-6.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Click &lt;strong&gt;Create&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;AWS displays the progress of your stack being created&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/create-stack.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;step-3-check-your-deployment&#34;&gt;Step 3. Check your deployment&lt;/h2&gt;

&lt;p&gt;When completed, the CloudFormation populates your VPC with six EC2 instances.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Instance&lt;/th&gt;
&lt;th&gt;Size&lt;/th&gt;
&lt;th&gt;Private IP Address&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;frontend01&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;t2.micro&lt;/td&gt;
&lt;td&gt;192.168.33.20&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;frontend02&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;t2.micro&lt;/td&gt;
&lt;td&gt;192.168.33.21&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;interlock&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;t2.micro&lt;/td&gt;
&lt;td&gt;192.168.33.12&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;manager&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;t2.micro&lt;/td&gt;
&lt;td&gt;192.168.33.11&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;store&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;m3.medium&lt;/td&gt;
&lt;td&gt;192.168.33.250&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;worker01&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;t2.micro&lt;/td&gt;
&lt;td&gt;192.168.33.200&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Navigate to the EC2 dashboard to view them running.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/vpc-create-7.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The underlying AWS infrastructure has this configuration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/aws-infrastructure.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;All instances are based on the &lt;code&gt;ami-56f59e36&lt;/code&gt; AMI. This is an Ubuntu 14.04 image
with a 3.13 kernel and 1.10.2 version of the Docker Engine installed. Each Engine
daemon was pre-configured via the &lt;code&gt;/etc/default/docker&lt;/code&gt; file using the following
&lt;code&gt;DOCKER_OPTS&lt;/code&gt; values.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--cluster-store=consul://192.168.33.11:8500 --cluster-advertise=eth0:2375 -H=tcp://0.0.0.0:2375 -H=unix:///var/run/docker.sock
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;next-step&#34;&gt;Next step&lt;/h2&gt;

&lt;p&gt;At this point your infrastructure stack is created successfully. You are ready
to progress to the next step and &lt;a href=&#34;../v1.10/swarm/swarm_at_scale/03-create-cluster/&#34;&gt;build the Swarm
cluster&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deploy the application</title>
      <link>https://docs.docker.com/v1.10/swarm/swarm_at_scale/04-deploy-app/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.docker.com/v1.10/swarm/swarm_at_scale/04-deploy-app/</guid>
      <description>

&lt;h1 id=&#34;deploy-the-application&#34;&gt;Deploy the application&lt;/h1&gt;

&lt;p&gt;You&amp;rsquo;ve &lt;a href=&#34;../v1.10/swarm/swarm_at_scale/03-create-cluster/&#34;&gt;built a Swarm cluster&lt;/a&gt; so now you are ready to
build and deploy the voting application itself.&lt;/p&gt;

&lt;h2 id=&#34;step-1-learn-about-the-images&#34;&gt;Step 1: Learn about the images&lt;/h2&gt;

&lt;p&gt;Some of the application&amp;rsquo;s containers are launched form existing images pulled
directly from Docker Hub. Other containers are launched from custom images you
must build. The list below shows which containers use custom images and which do
not:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Load balancer container: stock image (&lt;code&gt;ehazlett/interlock&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Redis containers: stock image (official &lt;code&gt;redis&lt;/code&gt; image)&lt;/li&gt;
&lt;li&gt;Postgres (PostgreSQL) containers: stock image (official &lt;code&gt;postgres&lt;/code&gt; image)&lt;/li&gt;
&lt;li&gt;Web containers: custom built image&lt;/li&gt;
&lt;li&gt;Worker containers: custom built image&lt;/li&gt;
&lt;li&gt;Results containers: custom built image&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All custom built images are built using Dockerfile&amp;rsquo;s pulled from the
&lt;a href=&#34;https://github.com/docker/swarm-microservice-demo-v1&#34;&gt;example application&amp;rsquo;s public GitHub
repository&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;If you haven&amp;rsquo;t already, &lt;code&gt;ssh&lt;/code&gt; into the Swarm &lt;code&gt;manager&lt;/code&gt; node.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Clone the &lt;a href=&#34;https://github.com/docker/swarm-microservice-demo-v1&#34;&gt;application&amp;rsquo;s GitHub repo&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/docker/swarm-microservice-demo-v1
sudo: unable to resolve host master
Cloning into &#39;swarm-microservice-demo-v1&#39;...
remote: Counting objects: 304, done.
remote: Compressing objects: 100% (17/17), done.
remote: Total 304 (delta 5), reused 0 (delta 0), pack-reused 287
Receiving objects: 100% (304/304), 2.24 MiB | 2.88 MiB/s, done.
Resolving deltas: 100% (132/132), done.
Checking connectivity... done.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command creates a new directory structure inside of your working
directory. The new directory contains all of the files and folders required
to build the voting application images.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;AWS&lt;/code&gt; directory contains the &lt;code&gt;cloudformation.json&lt;/code&gt; file used to deploy
the EC2 instances. The &lt;code&gt;Vagrant&lt;/code&gt; directory contains files and instructions
required to deploy the application using Vagrant. The &lt;code&gt;results-app&lt;/code&gt;,
&lt;code&gt;vote-worker&lt;/code&gt;, and &lt;code&gt;web-vote-app&lt;/code&gt; directories contain the Dockerfiles and
other files required to build the custom images for those particular
components of the application.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Change directory into the &lt;code&gt;swarm-microservice-demo-v1/web-vote-app&lt;/code&gt; directory.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd swarm-microservice-demo-v1/web-vote-app/
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;View the Dockerfile contents.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat Dockerfile  
# Using official python runtime base image
FROM python:2.7
# Set the application directory
WORKDIR /app
# Install our requirements.txt
ADD requirements.txt /app/requirements.txt
RUN pip install -r requirements.txt
# Copy our code from the current folder to /app inside the container
ADD . /app
# Make port 80 available for links and/or publish
EXPOSE 80
# Define our command to be run when launching the container
CMD [&amp;quot;python&amp;quot;, &amp;quot;app.py&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, the image is based on the official &lt;code&gt;Python:2.7&lt;/code&gt; tagged
image, adds a requirements file into the &lt;code&gt;/app&lt;/code&gt; directory, installs
requirements, copies files from the build context into the container,
exposes port &lt;code&gt;80&lt;/code&gt; and tells the container which command to run.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Spend time investigating the other parts of the application by viewing the &lt;code&gt;results-app/Dockefile&lt;/code&gt; and the &lt;code&gt;vote-worker/Dockerfile&lt;/code&gt; in the application.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;step-2-build-custom-images&#34;&gt;Step 2. Build custom images&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;If you haven&amp;rsquo;t already, &lt;code&gt;ssh&lt;/code&gt; into the Swarm &lt;code&gt;manager&lt;/code&gt; node.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Make sure you have DOCKER_HOST set&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export DOCKER_HOST=&amp;quot;tcp://192.168.33.11:3375&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Change to the root of your &lt;code&gt;swarm-microservice-demo-v1&lt;/code&gt; clone.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Build the &lt;code&gt;web-votes-app&lt;/code&gt; image both the front end nodes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;frontend01&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H tcp://192.168.33.20:2375 build -t web-vote-app ./web-vote-app
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;frontend02&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H tcp://192.168.33.21:2375 build -t web-vote-app ./web-vote-app
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These commands build the  &lt;code&gt;web-vote-app&lt;/code&gt; image on the &lt;code&gt;frontend01&lt;/code&gt; and
&lt;code&gt;frontend02&lt;/code&gt; nodes. To accomplish the operation, each command copies the
contents of the &lt;code&gt;swarm-microservice-demo-v1/web-vote-app&lt;/code&gt; sub-directory from the
&lt;code&gt;manager&lt;/code&gt; node to each frontend node. The command then instructs the
Docker daemon on each frontend node to build the image and store it locally.&lt;/p&gt;

&lt;p&gt;You&amp;rsquo;ll notice this example uses a &lt;code&gt;-H&lt;/code&gt; flag to pull an image to specific
host. This is to help you conceptualize the architecture for this sample. In
a production deployment, you&amp;rsquo;d omit this option and rely on the Swarm
manager to distribute the image. The manager would pull the image to every
node; so that any node can step in to run the image as needed.&lt;/p&gt;

&lt;p&gt;It may take a minute or so for each image to build. Wait for the builds to finish.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Build &lt;code&gt;vote-worker&lt;/code&gt; image on the &lt;code&gt;worker01&lt;/code&gt; node&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H tcp://192.168.33.200:2375 build -t vote-worker ./vote-worker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It may take a minute or so for the image to build. Wait for the build to
finish.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Build the &lt;code&gt;results-app&lt;/code&gt; on the &lt;code&gt;store&lt;/code&gt; node&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H tcp://192.168.33.250:2375 build -t results-app ./results-app
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Each of the &lt;em&gt;custom images&lt;/em&gt; required by the application is now built and stored
locally on the nodes that will use them.&lt;/p&gt;

&lt;h2 id=&#34;step-3-pull-images-from-docker-hub&#34;&gt;Step 3. Pull images from Docker Hub&lt;/h2&gt;

&lt;p&gt;For performance reasons, it is always better to pull any required Docker Hub
images locally on each instance that needs them. This ensures that containers
based on those images can start quickly.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Log into the Swarm &lt;code&gt;manager&lt;/code&gt; node.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Pull the &lt;code&gt;redis&lt;/code&gt; image to your frontend nodes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;frontend01&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H tcp://192.168.33.20:2375 pull redis
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;frontend02&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H tcp://192.168.33.21:2375 pull redis
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Pull the &lt;code&gt;postgres&lt;/code&gt; image to the &lt;code&gt;store&lt;/code&gt; node&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H tcp://192.168.33.250:2375 pull postgres
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Pull the &lt;code&gt;ehazlett/interlock&lt;/code&gt; image to the &lt;code&gt;interlock&lt;/code&gt; node&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H tcp://192.168.33.12:2375 pull ehazlett/interlock
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Each node in the cluster, as well as the &lt;code&gt;interlock&lt;/code&gt; node, now has the required images stored locally as shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/interlock.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now that all images are built, pulled, and stored locally, the next step is to start the application.&lt;/p&gt;

&lt;h2 id=&#34;step-4-start-the-voting-application&#34;&gt;Step 4. Start the voting application&lt;/h2&gt;

&lt;p&gt;In the following steps, your launch several containers to the voting application.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;If you haven&amp;rsquo;t already, &lt;code&gt;ssh&lt;/code&gt; into the Swarm &lt;code&gt;manager&lt;/code&gt; node.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start the &lt;code&gt;interlock&lt;/code&gt; container on the &lt;code&gt;interlock&lt;/code&gt; node&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H tcp://192.168.33.12:2375 run --restart=unless-stopped -p 80:80 --name interlock -d ehazlett/interlock --swarm-url tcp://192.168.33.11:3375 --plugin haproxy start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command is issued against the &lt;code&gt;interlock&lt;/code&gt; instance and maps port 80 on the instance to port 80 inside the container. This allows the container to load balance connections coming in over port 80 (HTTP). The command also applies the &lt;code&gt;--restart=unless-stopped&lt;/code&gt; policy to the container, telling Docker to restart the container if it exits unexpectedly.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Verify the container is running.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H tcp://192.168.33.12:2375 ps
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start a &lt;code&gt;redis&lt;/code&gt; container on your front end nodes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;frontend01&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run --restart=unless-stopped --env=&amp;quot;constraint:node==frontend01&amp;quot; -p 6379:6379 --name redis01 --net mynet -d redis
$ docker -H tcp://192.168.33.20:2375 ps
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;frontend02&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run --restart=unless-stopped --env=&amp;quot;constraint:node==frontend02&amp;quot; -p 6379:6379 --name redis02 --net mynet -d redis
$ docker -H tcp://192.168.33.21:2375 ps
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These two commands are issued against the Swarm cluster. The commands specify &lt;em&gt;node constraints&lt;/em&gt;, forcing Swarm to start the contaienrs on &lt;code&gt;frontend01&lt;/code&gt; and &lt;code&gt;frontend02&lt;/code&gt;. Port 6379 on each instance is mapped to port 6379 inside of each container for debugging purposes. The command also applies the &lt;code&gt;--restart=unless-stopped&lt;/code&gt; policy to the containers and attaches them to the &lt;code&gt;mynet&lt;/code&gt; overlay network.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start a &lt;code&gt;web-vote-app&lt;/code&gt; container the frontend nodes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;frontend01&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run --restart=unless-stopped --env=&amp;quot;constraint:node==frontend01&amp;quot; -d -p 5000:80 -e WEB_VOTE_NUMBER=&#39;01&#39; --name frontend01 --net mynet --hostname votingapp.local web-vote-app
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;frontend02&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run --restart=unless-stopped --env=&amp;quot;constraint:node==frontend02&amp;quot; -d -p 5000:80 -e WEB_VOTE_NUMBER=&#39;02&#39; --name frontend02 --net mynet --hostname votingapp.local web-vote-app
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These two commands are issued against the Swarm cluster. The commands
specify &lt;em&gt;node constraints&lt;/em&gt;, forcing Swarm to start the contaienrs on
&lt;code&gt;frontend01&lt;/code&gt; and &lt;code&gt;frontend02&lt;/code&gt;. Port &lt;code&gt;5000&lt;/code&gt; on each node is mapped to port
&lt;code&gt;80&lt;/code&gt; inside of each container. This allows connections to come in to each
node on port &lt;code&gt;5000&lt;/code&gt; and be forwarded to port &lt;code&gt;80&lt;/code&gt; inside of each container.&lt;/p&gt;

&lt;p&gt;Both containers are attached to the &lt;code&gt;mynet&lt;/code&gt; overlay network and both
containers are given the &lt;code&gt;votingapp-local&lt;/code&gt; hostname. The
&lt;code&gt;--restart=unless-stopped&lt;/code&gt; policy is also applied to these containers.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start the &lt;code&gt;postgres&lt;/code&gt; container on the &lt;code&gt;store&lt;/code&gt; node&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run --restart=unless-stopped --env=&amp;quot;constraint:node==store&amp;quot; --name pg -e POSTGRES_PASSWORD=pg8675309 --net mynet -p 5432:5432 -d postgres
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command is issued against the Swarm cluster and starts the container on
&lt;code&gt;store&lt;/code&gt;. It maps port 5432 on the &lt;code&gt;store&lt;/code&gt; node to port 5432 inside the
container and attaches the container to the &lt;code&gt;mynet&lt;/code&gt; overlay network. The
command also inserts the database password into the container via the
&lt;code&gt;POSTGRES_PASSWORD&lt;/code&gt; environment variable and applies the
&lt;code&gt;--restart=unless-stopped&lt;/code&gt; policy to the container.&lt;/p&gt;

&lt;p&gt;Sharing passwords like this is not recommended for production use cases.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start the &lt;code&gt;worker01&lt;/code&gt; container on the &lt;code&gt;worker01&lt;/code&gt; node&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run --restart=unless-stopped --env=&amp;quot;constraint:node==worker01&amp;quot; -d -e WORKER_NUMBER=&#39;01&#39; -e FROM_REDIS_HOST=1 -e TO_REDIS_HOST=2 --name worker01 --net mynet vote-worker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command is issued against the Swarm manager and uses a constraint to
start the container on the &lt;code&gt;worker01&lt;/code&gt; node. It passes configuration data
into the container via environment variables, telling the worker container
to clear the queues on &lt;code&gt;frontend01&lt;/code&gt; and &lt;code&gt;frontend02&lt;/code&gt;. It adds the container
to the &lt;code&gt;mynet&lt;/code&gt; overlay network and applies the &lt;code&gt;--restart=unless-stopped&lt;/code&gt;
policy to the container.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start the &lt;code&gt;results-app&lt;/code&gt; container on the &lt;code&gt;store&lt;/code&gt; node&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run --restart=unless-stopped --env=&amp;quot;constraint:node==store&amp;quot; -p 80:80 -d --name results-app --net mynet results-app
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command starts the results-app container on the &lt;code&gt;store&lt;/code&gt; node by means
of a &lt;em&gt;node constraint&lt;/em&gt;. It maps port 80 on the &lt;code&gt;store&lt;/code&gt; node to port 80
inside the container. It adds the container to the &lt;code&gt;mynet&lt;/code&gt; overlay network
and applies the &lt;code&gt;--restart=unless-stopped&lt;/code&gt; policy to the container.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The application is now fully deployed as shown in the diagram below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/fully-deployed.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;step-5-test-the-application&#34;&gt;Step 5. Test the application&lt;/h2&gt;

&lt;p&gt;Now that the application is deployed and running, it&amp;rsquo;s time to test it. To do
this, you configure a DNS mapping on the machine where you are running your web
browser. This maps the &amp;ldquo;votingapp.local&amp;rdquo; DNS name to the public IP address of
the &lt;code&gt;interlock&lt;/code&gt; node.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Configure the DNS name resolution on your local machine for browsing.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;On Windows machines this is done by adding &lt;code&gt;votingapp.local &amp;lt;interlock-public-ip&amp;gt;&lt;/code&gt; to the &lt;code&gt;C:\Windows\System32\Drivers\etc\hosts file&lt;/code&gt;. Modifying this file requires administrator privileges. To open the file with administrator privileges, right-click &lt;code&gt;C:\Windows\System32\notepad.exe&lt;/code&gt; and select &lt;code&gt;Run as administrator&lt;/code&gt;. Once Notepad is open, click &lt;code&gt;file&lt;/code&gt; &amp;gt; &lt;code&gt;open&lt;/code&gt; and open the file and make the edit.&lt;/li&gt;
&lt;li&gt;On OSX machines this is done by adding &lt;code&gt;votingapp.local &amp;lt;interlock-public-ip&amp;gt;&lt;/code&gt; to &lt;code&gt;/private/etc/hosts&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;On most Linux machines this is done by adding &lt;code&gt;votingapp.local &amp;lt;interlock-public-ip&amp;gt;&lt;/code&gt; to &lt;code&gt;/etc/hosts&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Be sure to replace &lt;code&gt;&amp;lt;interlock-public-ip&amp;gt;&lt;/code&gt; with the public IP address of
your &lt;code&gt;interlock&lt;/code&gt; node. You can find the &lt;code&gt;interlock&lt;/code&gt; node&amp;rsquo;s Public IP by
selecting your &lt;code&gt;interlock&lt;/code&gt; EC2 Instance from within the AWS EC2 console.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Verify the mapping worked with a &lt;code&gt;ping&lt;/code&gt; command from your local machine.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ping votingapp.local
Pinging votingapp.local [54.183.164.230] with 32 bytes of data:
Reply from 54.183.164.230: bytes=32 time=164ms TTL=42
Reply from 54.183.164.230: bytes=32 time=163ms TTL=42
Reply from 54.183.164.230: bytes=32 time=169ms TTL=42
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Point your web browser to &lt;a href=&#34;http://votingapp.local&#34;&gt;http://votingapp.local&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/vote-app-test.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Notice the text at the bottom of the web page. This shows which web
container serviced the request. In the diagram above, this is &lt;code&gt;frontend02&lt;/code&gt;.
If you refresh your web browser you should see this change as the Interlock
load balancer shares incoming requests across both web containers.&lt;/p&gt;

&lt;p&gt;To see more detailed load balancer data from the Interlock service, point your web browser to &lt;a href=&#34;http://stats:interlock@votingapp.local/haproxy?stats&#34;&gt;http://stats:interlock@votingapp.local/haproxy?stats&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/proxy-test.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Cast your vote. It is recommended to choose &amp;ldquo;Dogs&amp;rdquo; ;-)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;To see the results of the poll, you can point your web browser at the public IP of the &lt;code&gt;store&lt;/code&gt; node&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/poll-results.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;

&lt;p&gt;Congratulations. You have successfully walked through manually deploying a
microservice-based application to a Swarm cluster. Of course, not every
deployment goes smoothly. Now that you&amp;rsquo;ve learned how to successfully deploy an
application at scale, you should learn &lt;a href=&#34;../v1.10/swarm/swarm_at_scale/05-troubleshoot/&#34;&gt;what to consider when troubleshooting
large applications running on a Swarm cluster&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Discovery</title>
      <link>https://docs.docker.com/v1.10/swarm/discovery/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.docker.com/v1.10/swarm/discovery/</guid>
      <description>

&lt;h1 id=&#34;docker-swarm-discovery&#34;&gt;Docker Swarm Discovery&lt;/h1&gt;

&lt;p&gt;Docker Swarm comes with multiple discovery backends. You use a hosted discovery service with Docker Swarm. The service maintains a list of IPs in your swarm.
This page describes the different types of hosted discovery available to you. These are:&lt;/p&gt;

&lt;h2 id=&#34;using-a-distributed-key-value-store&#34;&gt;Using a distributed key/value store&lt;/h2&gt;

&lt;p&gt;The recommended way to do node discovery in Swarm is Docker&amp;rsquo;s libkv project. The libkv project is an abstraction layer over existing distributed key/value stores.  As of this writing, the project supports:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Consul 0.5.1 or higher&lt;/li&gt;
&lt;li&gt;Etcd 2.0 or higher&lt;/li&gt;
&lt;li&gt;ZooKeeper 3.4.5 or higher&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For details about libkv and a detailed technical overview of the supported backends, refer to the &lt;a href=&#34;https://github.com/docker/libkv&#34;&gt;libkv project&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;using-a-hosted-discovery-key-store&#34;&gt;Using a hosted discovery key store&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;On each node, start the Swarm agent.&lt;/p&gt;

&lt;p&gt;The node IP address doesn&amp;rsquo;t have to be public as long as the swarm manager can access it. In a large cluster, the nodes joining swarm may trigger request spikes to discovery. For example, a large number of nodes are added by a script, or recovered from a network partition. This may result in discovery failure. You can use &lt;code&gt;--delay&lt;/code&gt; option to specify a delay limit. Swarm join will add a random delay less than this limit to reduce pressure to discovery.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Etcd&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm join --advertise=&amp;lt;node_ip:2375&amp;gt; etcd://&amp;lt;etcd_addr1&amp;gt;,&amp;lt;etcd_addr2&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Consul&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm join --advertise=&amp;lt;node_ip:2375&amp;gt; consul://&amp;lt;consul_addr&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;ZooKeeper&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm join --advertise=&amp;lt;node_ip:2375&amp;gt; zk://&amp;lt;zookeeper_addr1&amp;gt;,&amp;lt;zookeeper_addr2&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start the Swarm manager on any machine or your laptop.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Etcd&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm manage -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; etcd://&amp;lt;etcd_addr1&amp;gt;,&amp;lt;etcd_addr2&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Consul&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm manage -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; consul://&amp;lt;consul_addr&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;ZooKeeper&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm manage -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; zk://&amp;lt;zookeeper_addr1&amp;gt;,&amp;lt;zookeeper_addr2&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Use the regular Docker commands.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; info
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; run ...
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; ps
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; logs ...
...
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Try listing the nodes in your cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Etcd&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm list etcd://&amp;lt;etcd_addr1&amp;gt;,&amp;lt;etcd_addr2&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&amp;lt;node_ip:2375&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Consul&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm list consul://&amp;lt;consul_addr&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&amp;lt;node_ip:2375&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;ZooKeeper&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm list zk://&amp;lt;zookeeper_addr1&amp;gt;,&amp;lt;zookeeper_addr2&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&amp;lt;node_ip:2375&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;use-tls-with-distributed-key-value-discovery&#34;&gt;Use TLS with distributed key/value discovery&lt;/h3&gt;

&lt;p&gt;You can securely talk to the distributed k/v store using TLS. To connect
securely to the store, you must generate the certificates for a node when you
&lt;code&gt;join&lt;/code&gt; it to the swarm. You can only use with Consul and Etcd. The following example illustrates this with Consul:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm join \
    --advertise=&amp;lt;node_ip:2375&amp;gt; \
    --discovery-opt kv.cacertfile=/path/to/mycacert.pem \
    --discovery-opt kv.certfile=/path/to/mycert.pem \
    --discovery-opt kv.keyfile=/path/to/mykey.pem \
    consul://&amp;lt;consul_addr&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This works the same way for the Swarm &lt;code&gt;manage&lt;/code&gt; and &lt;code&gt;list&lt;/code&gt; commands.&lt;/p&gt;

&lt;h2 id=&#34;a-static-file-or-list-of-nodes&#34;&gt;A static file or list of nodes&lt;/h2&gt;

&lt;p&gt;You can use a static file or list of nodes for your discovery backend. The file must be stored on a host that is accessible from the Swarm manager. You can also pass a node list as an option when you start Swarm.&lt;/p&gt;

&lt;p&gt;Both the static file and the &lt;code&gt;nodes&lt;/code&gt; option support a IP address ranges. To specify a range supply a pattern, for example, &lt;code&gt;10.0.0.[10:200]&lt;/code&gt; refers to nodes starting from &lt;code&gt;10.0.0.10&lt;/code&gt; to &lt;code&gt;10.0.0.200&lt;/code&gt;.  For example for the &lt;code&gt;file&lt;/code&gt; discovery method.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    $ echo &amp;quot;10.0.0.[11:100]:2375&amp;quot;   &amp;gt;&amp;gt; /tmp/my_cluster
    $ echo &amp;quot;10.0.1.[15:20]:2375&amp;quot;    &amp;gt;&amp;gt; /tmp/my_cluster
    $ echo &amp;quot;192.168.1.2:[2:20]375&amp;quot;  &amp;gt;&amp;gt; /tmp/my_cluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or with node discovery:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    swarm manage -H &amp;lt;swarm_ip:swarm_port&amp;gt; &amp;quot;nodes://10.0.0.[10:200]:2375,10.0.1.[2:250]:2375&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;to-create-a-file&#34;&gt;To create a file&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Edit the file and add line for each of your nodes.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo &amp;lt;node_ip1:2375&amp;gt; &amp;gt;&amp;gt; /opt/my_cluster
echo &amp;lt;node_ip2:2375&amp;gt; &amp;gt;&amp;gt; /opt/my_cluster
echo &amp;lt;node_ip3:2375&amp;gt; &amp;gt;&amp;gt; /opt/my_cluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This example creates a file named &lt;code&gt;/tmp/my_cluster&lt;/code&gt;. You can use any name you like.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start the Swarm manager on any machine.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm manage -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; file:///tmp/my_cluster
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Use the regular Docker commands.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; info
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; run ...
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; ps
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; logs ...
...
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;List the nodes in your cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ swarm list file:///tmp/my_cluster
&amp;lt;node_ip1:2375&amp;gt;
&amp;lt;node_ip2:2375&amp;gt;
&amp;lt;node_ip3:2375&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;to-use-a-node-list&#34;&gt;To use a node list&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Start the manager on any machine or your laptop.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm manage -H &amp;lt;swarm_ip:swarm_port&amp;gt; nodes://&amp;lt;node_ip1:2375&amp;gt;,&amp;lt;node_ip2:2375&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm manage -H &amp;lt;swarm_ip:swarm_port&amp;gt; &amp;lt;node_ip1:2375&amp;gt;,&amp;lt;node_ip2:2375&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Use the regular Docker commands.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker -H &amp;lt;swarm_ip:swarm_port&amp;gt; info
docker -H &amp;lt;swarm_ip:swarm_port&amp;gt; run ...
docker -H &amp;lt;swarm_ip:swarm_port&amp;gt; ps
docker -H &amp;lt;swarm_ip:swarm_port&amp;gt; logs ...
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;List the nodes in your cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ swarm list file:///tmp/my_cluster
&amp;lt;node_ip1:2375&amp;gt;
&amp;lt;node_ip2:2375&amp;gt;
&amp;lt;node_ip3:2375&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;docker-hub-as-a-hosted-discovery-service&#34;&gt;Docker Hub as a hosted discovery service&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: The Docker Hub Hosted Discovery Service &lt;strong&gt;is not recommended&lt;/strong&gt; for production use. It&amp;rsquo;s intended to be used for testing/development. See the  discovery backends for production use.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This example uses the hosted discovery service on Docker Hub. Using
Docker Hub&amp;rsquo;s hosted discovery service requires that each node in the
swarm is connected to the public internet. To create your swarm:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Create a cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ swarm create
6856663cdefdec325839a4b7e1de38e8 # &amp;lt;- this is your unique &amp;lt;cluster_id&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create each node and join them to the cluster.&lt;/p&gt;

&lt;p&gt;On each of your nodes, start the swarm agent. The &lt;node_ip&gt; doesn&amp;rsquo;t have to be public (eg. 192.168.0.X) but the the swarm manager must be able to access it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ swarm join --advertise=&amp;lt;node_ip:2375&amp;gt; token://&amp;lt;cluster_id&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start the Swarm manager.&lt;/p&gt;

&lt;p&gt;This can be on any machine or even your laptop.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ swarm manage -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; token://&amp;lt;cluster_id&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Use regular Docker commands to interact with your swarm.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; info
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; run ...
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; ps
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; logs ...
...
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;List the nodes in your cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm list token://&amp;lt;cluster_id&amp;gt;
&amp;lt;node_ip:2375&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;contribute-a-new-discovery-backend&#34;&gt;Contribute a new discovery backend&lt;/h2&gt;

&lt;p&gt;You can contribute a new discovery backend to Swarm. For information on how to
do this, see &lt;a
href=&#34;https://github.com/docker/docker/tree/master/pkg/discovery&#34;&gt;
github.com/docker/docker/pkg/discovery&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;docker-swarm-documentation-index&#34;&gt;Docker Swarm documentation index&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/&#34;&gt;Docker Swarm overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/scheduler/strategy/&#34;&gt;Scheduler strategies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/scheduler/filter/&#34;&gt;Scheduler filters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/swarm-api/&#34;&gt;Swarm API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Docker Swarm</title>
      <link>https://docs.docker.com/v1.10/swarm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.docker.com/v1.10/swarm/</guid>
      <description>

&lt;h1 id=&#34;docker-swarm&#34;&gt;Docker Swarm&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/overview/&#34;&gt;Docker Swarm overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/get-swarm/&#34;&gt;How to get Docker Swarm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/install-w-machine/&#34;&gt;Evaluate Swarm in a sandbox&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/plan-for-production/&#34;&gt;Plan for Swarm in production&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/install-manual/&#34;&gt;Build a Swarm cluster for production&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/swarm_at_scale/&#34;&gt;Try Swarm at scale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/secure-swarm-tls/&#34;&gt;Overview Swarm with TLS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/configure-tls/&#34;&gt;Configure Docker Swarm for TLS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/discovery/&#34;&gt;Docker Swarm Discovery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/multi-manager-setup/&#34;&gt;High availability in Docker Swarm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/networking/&#34;&gt;Swarm and container networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/scheduler/&#34;&gt;Advanced Scheduling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/provision-with-machine/&#34;&gt;Provision a Swarm cluster with Docker Machine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/swarm-api/&#34;&gt;Docker Swarm API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Docker Swarm API</title>
      <link>https://docs.docker.com/v1.10/swarm/swarm-api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.docker.com/v1.10/swarm/swarm-api/</guid>
      <description>

&lt;h1 id=&#34;docker-swarm-api&#34;&gt;Docker Swarm API&lt;/h1&gt;

&lt;p&gt;The Docker Swarm API is mostly compatible with the &lt;a href=&#34;https://docs.docker.com/reference/api/docker_remote_api/&#34;&gt;Docker Remote API&lt;/a&gt;. This document is an overview of the differences between the Swarm API and the Docker Remote API.&lt;/p&gt;

&lt;h2 id=&#34;missing-endpoints&#34;&gt;Missing endpoints&lt;/h2&gt;

&lt;p&gt;Some endpoints have not yet been implemented and will return a 404 error.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;POST &amp;quot;/images/create&amp;quot; : &amp;quot;docker import&amp;quot; flow not implement
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;endpoints-which-behave-differently&#34;&gt;Endpoints which behave differently&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;GET &amp;quot;/containers/{name:.*}/json&amp;quot;&lt;/code&gt;: New field &lt;code&gt;Node&lt;/code&gt; added:&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;&amp;quot;Node&amp;quot;: {
	&amp;quot;Id&amp;quot;: &amp;quot;ODAI:IC6Q:MSBL:TPB5:HIEE:6IKC:VCAM:QRNH:PRGX:ERZT:OK46:PMFX&amp;quot;,
	&amp;quot;Ip&amp;quot;: &amp;quot;0.0.0.0&amp;quot;,
	&amp;quot;Addr&amp;quot;: &amp;quot;http://0.0.0.0:4243&amp;quot;,
	&amp;quot;Name&amp;quot;: &amp;quot;vagrant-ubuntu-saucy-64&amp;quot;,
    },
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;GET &amp;quot;/containers/{name:.*}/json&amp;quot;&lt;/code&gt;: &lt;code&gt;HostIP&lt;/code&gt; replaced by the the actual Node&amp;rsquo;s IP if &lt;code&gt;HostIP&lt;/code&gt; is &lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;GET &amp;quot;/containers/json&amp;quot;&lt;/code&gt;: Node&amp;rsquo;s name prepended to the container name.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;GET &amp;quot;/containers/json&amp;quot;&lt;/code&gt;: &lt;code&gt;HostIP&lt;/code&gt; replaced by the the actual Node&amp;rsquo;s IP if &lt;code&gt;HostIP&lt;/code&gt; is &lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;GET &amp;quot;/containers/json&amp;quot;&lt;/code&gt; : Containers started from the &lt;code&gt;swarm&lt;/code&gt; official image are hidden by default, use &lt;code&gt;all=1&lt;/code&gt; to display them.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;GET &amp;quot;/images/json&amp;quot;&lt;/code&gt; : Use &amp;lsquo;--filter node=&amp;lt;Node name&amp;gt;&amp;rsquo; to show images of the specific node.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;POST &amp;quot;/containers/create&amp;quot;&lt;/code&gt;: &lt;code&gt;CpuShares&lt;/code&gt; in &lt;code&gt;HostConfig&lt;/code&gt; sets the number of CPU cores allocated to the container.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;registry-authentication&#34;&gt;Registry Authentication&lt;/h1&gt;

&lt;p&gt;During container create calls, the swarm API will optionally accept a X-Registry-Config header.
If provided, this header will be passed down to the engine if the image must be pulled
to complete the create operation.&lt;/p&gt;

&lt;p&gt;The following two examples demonstrate how to utilize this using the existing docker CLI&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;CLI usage example using username/password:&lt;/p&gt;

&lt;p&gt;```bash&lt;/p&gt;

&lt;h1 id=&#34;calculate-the-header&#34;&gt;Calculate the header&lt;/h1&gt;

&lt;p&gt;REPO_USER=yourusername
read -s PASSWORD
HEADER=$(echo &amp;ldquo;{\&amp;ldquo;username\&amp;rdquo;:\&amp;ldquo;${REPO_USER}\&amp;ldquo;,\&amp;ldquo;password\&amp;rdquo;:\&amp;ldquo;${PASSWORD}\&amp;ldquo;}&amp;rdquo;|base64 -w 0 )
unset PASSWORD
echo HEADER=$HEADER&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;then-add-the-following-to-your-docker-config-json&#34;&gt;Then add the following to your ~/.docker/config.json&lt;/h1&gt;

&lt;p&gt;&amp;ldquo;HttpHeaders&amp;rdquo;: {
    &amp;ldquo;X-Registry-Auth&amp;rdquo;: &amp;ldquo;&lt;HEADER string from above&gt;&amp;ldquo;
}&lt;/p&gt;

&lt;h1 id=&#34;now-run-a-private-image-against-swarm&#34;&gt;Now run a private image against swarm:&lt;/h1&gt;

&lt;p&gt;docker run --rm -it yourprivateimage:latest&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
* CLI usage example using registry tokens: (Requires engine 1.10 with new auth token support)

    ```bash
REPO=yourrepo/yourimage
REPO_USER=yourusername
read -s PASSWORD
AUTH_URL=https://auth.docker.io/token
TOKEN=$(curl -s -u &amp;quot;${REPO_USER}:${PASSWORD}&amp;quot; &amp;quot;${AUTH_URL}?scope=repository:${REPO}:pull&amp;amp;service=registry.docker.io&amp;quot; |
    jq -r &amp;quot;.token&amp;quot;)
HEADER=$(echo &amp;quot;{\&amp;quot;registrytoken\&amp;quot;:\&amp;quot;${TOKEN}\&amp;quot;}&amp;quot;|base64 -w 0 )
echo HEADER=$HEADER

# Update the docker config as above, but the token will expire quickly...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;docker-swarm-documentation-index&#34;&gt;Docker Swarm documentation index&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/swarm/&#34;&gt;Docker Swarm overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/swarm/discovery/&#34;&gt;Discovery options&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/swarm/scheduler/strategy/&#34;&gt;Scheduler strategies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/swarm/scheduler/filter/&#34;&gt;Scheduler filters&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Evaluate Swarm in a sandbox</title>
      <link>https://docs.docker.com/v1.10/swarm/install-w-machine/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.docker.com/v1.10/swarm/install-w-machine/</guid>
      <description>

&lt;h1 id=&#34;evaluate-swarm-in-a-sandbox&#34;&gt;Evaluate Swarm in a sandbox&lt;/h1&gt;

&lt;p&gt;This getting started example shows you how to create a Docker Swarm, the
native clustering tool for Docker.&lt;/p&gt;

&lt;p&gt;You&amp;rsquo;ll use Docker Toolbox to install Docker Machine and some other tools on your computer. Then you&amp;rsquo;ll use Docker Machine to provision a set of Docker Engine hosts. Lastly, you&amp;rsquo;ll use Docker client to connect to the hosts, where you&amp;rsquo;ll create a discovery token, create a cluster of one Swarm manager and nodes, and manage the cluster.&lt;/p&gt;

&lt;p&gt;When you finish, you&amp;rsquo;ll have a Docker Swarm up and running in VirtualBox on your
local Mac or Windows computer. You can use this Swarm as personal development
sandbox.&lt;/p&gt;

&lt;p&gt;To use Docker Swarm on Linux, see &lt;a href=&#34;../v1.10/swarm/install-manual/&#34;&gt;Build a Swarm cluster for production&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;install-docker-toolbox&#34;&gt;Install Docker Toolbox&lt;/h2&gt;

&lt;p&gt;Download and install &lt;a href=&#34;https://www.docker.com/docker-toolbox&#34;&gt;Docker Toolbox&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The toolbox installs a handful of tools on your local Windows or Mac OS X computer. In this exercise, you use three of those tools:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Docker Machine: To deploy virtual machines that run Docker Engine.&lt;/li&gt;
&lt;li&gt;VirtualBox: To host the virtual machines deployed from Docker Machine.&lt;/li&gt;
&lt;li&gt;Docker Client: To connect from your local computer to the Docker Engines on the VMs and issue docker commands to create the Swarm.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following sections provide more information on each of these tools. The rest of the document uses the abbreviation, VM, for virtual machine.&lt;/p&gt;

&lt;h2 id=&#34;create-three-vms-running-docker-engine&#34;&gt;Create three VMs running Docker Engine&lt;/h2&gt;

&lt;p&gt;Here, you use Docker Machine to provision three VMs running Docker Engine.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Open a terminal on your computer. Use Docker Machine to list any VMs in VirtualBox.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine ls
NAME         ACTIVE   DRIVER       STATE     URL                         SWARM
default    *        virtualbox   Running   tcp://192.168.99.100:2376
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Optional: To conserve system resources, stop any virtual machines you are not using. For example, to stop the VM named &lt;code&gt;default&lt;/code&gt;, enter:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine stop default
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create and run a VM named &lt;code&gt;manager&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine create -d virtualbox manager
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create and run a VM named &lt;code&gt;agent1&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine create -d virtualbox agent1
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create and run a VM named &lt;code&gt;agent2&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine create -d virtualbox agent2
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Each create command checks for a local copy of the &lt;em&gt;latest&lt;/em&gt; VM image, called boot2docker.iso. If it isn&amp;rsquo;t available, Docker Machine downloads the image from Docker Hub. Then, Docker Machine uses boot2docker.iso to create a VM that automatically runs Docker Engine.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Troubleshooting: If your computer or hosts cannot reach Docker Hub, the
&lt;code&gt;docker-machine&lt;/code&gt; or &lt;code&gt;docker run&lt;/code&gt; commands that pull images may fail. In that
case, check the &lt;a href=&#34;http://status.docker.com/&#34;&gt;Docker Hub status page&lt;/a&gt; for
service availability. Then, check whether your computer is connected to the Internet.  Finally, check whether VirtualBox&amp;rsquo;s network settings allow your hosts to connect to the Internet.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;create-a-swarm-discovery-token&#34;&gt;Create a Swarm discovery token&lt;/h2&gt;

&lt;p&gt;Here you use the discovery backend hosted on Docker Hub to create a unique discovery token for your cluster. This discovery backend is only for low-volume development and testing purposes, not for production. Later on, when you run the Swarm manager and nodes, they register with the discovery backend as members of the cluster that&amp;rsquo;s associated with the unique token. The discovery backend maintains an up-to-date list of cluster members and shares that list with the Swarm manager. The Swarm manager uses this list to assign tasks to the nodes.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Connect the Docker Client on your computer to the Docker Engine running on &lt;code&gt;manager&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ eval $(docker-machine env manager)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The client will send the &lt;code&gt;docker&lt;/code&gt; commands in the following steps to the Docker Engine on on &lt;code&gt;manager&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create a unique id for the Swarm cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run --rm swarm create
.
.
.
Status: Downloaded newer image for swarm:latest
0ac50ef75c9739f5bfeeaf00503d4e6e
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;docker run&lt;/code&gt; command gets the latest &lt;code&gt;swarm&lt;/code&gt; image and runs it as a container. The &lt;code&gt;create&lt;/code&gt; argument makes the Swarm container connect to the Docker Hub discovery service and get a unique Swarm ID, also known as a &amp;ldquo;discovery token&amp;rdquo;. The token appears in the output, it is not saved to a file on the host. The &lt;code&gt;--rm&lt;/code&gt; option automatically cleans up the container and removes the file system when the container exits.&lt;/p&gt;

&lt;p&gt;The discovery service keeps unused tokens for approximately one week.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Copy the discovery token from the last line of the previous output to a safe place.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;create-the-swarm-manager-and-nodes&#34;&gt;Create the Swarm manager and nodes&lt;/h2&gt;

&lt;p&gt;Here, you connect to each of the hosts and create a Swarm manager or node.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;List the VMs to check that they&amp;rsquo;re set up and running. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine ls
NAME      ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER   ERRORS
agent1    -        virtualbox   Running   tcp://192.168.99.102:2376           v1.9.1
agent2    -        virtualbox   Running   tcp://192.168.99.103:2376           v1.9.1
manager   *        virtualbox   Running   tcp://192.168.99.100:2376           v1.9.1
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Your client should still be pointing to Docker Engine on &lt;code&gt;manager&lt;/code&gt;. Use the following syntax to run a Swarm container that functions as the primary manager on &lt;code&gt;manager&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d -p &amp;lt;your_selected_port&amp;gt;:3376 -t -v /var/lib/boot2docker:/certs:ro swarm manage -H 0.0.0.0:3376 --tlsverify --tlscacert=/certs/ca.pem --tlscert=/certs/server.pem --tlskey=/certs/server-key.pem token://&amp;lt;cluster_id&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d -p 3376:3376 -t -v /var/lib/boot2docker:/certs:ro swarm manage -H 0.0.0.0:3376 --tlsverify --tlscacert=/certs/ca.pem --tlscert=/certs/server.pem --tlskey=/certs/server-key.pem token://0ac50ef75c9739f5bfeeaf00503d4e6e
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;-p&lt;/code&gt; option maps a port 3376 on the container to port 3376 on the host. The &lt;code&gt;-v&lt;/code&gt; option mounts the directory containing TLS certificates (&lt;code&gt;/var/lib/boot2docker&lt;/code&gt; for the &lt;code&gt;manager&lt;/code&gt; VM) into the container running Swarm manager in read-only mode.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Connect Docker Client to &lt;code&gt;agent1&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ eval $(docker-machine env agent1)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Use the following syntax to run a Swarm container that functions as an agent on &lt;code&gt;agent1&lt;/code&gt;. Replace &lt;code&gt;&amp;lt;node_ip&amp;gt;&lt;/code&gt; with the IP address of the VM from above, or use the &lt;code&gt;docker-machine ip&lt;/code&gt; command.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d swarm join --addr=&amp;lt;node_ip&amp;gt;:&amp;lt;node_port&amp;gt; token://&amp;lt;cluster_id&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d swarm join --addr=$(docker-machine ip agent1):2376 token://0ac50ef75c9739f5bfeeaf00503d4e6e
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Connect Docker Client to &lt;code&gt;agent2&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ eval $(docker-machine env agent2)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Run a Swarm container as an agent on &lt;code&gt;agent2&lt;/code&gt;. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d swarm join --addr=$(docker-machine ip agent2):2376 token://0ac50ef75c9739f5bfeeaf00503d4e6e
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;manage-your-swarm&#34;&gt;Manage your Swarm&lt;/h2&gt;

&lt;p&gt;Here, you connect to the cluster and review information about the Swarm manager and nodes. You tell the Swarm to run a container and check which node did the work.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Connect the Docker Client to the Swarm by updating the &lt;code&gt;DOCKER_HOST&lt;/code&gt; environment variable.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ DOCKER_HOST=&amp;lt;manager_ip&amp;gt;:&amp;lt;your_selected_port&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We use the &lt;code&gt;docker-machine ip&lt;/code&gt; command, and we selected port 3376 for the Swarm manager.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ DOCKER_HOST=$(docker-machine ip manager):3376
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because Docker Swarm uses the standard Docker API, you can connect to it using Docker Client and other tools such as Docker Compose, Dokku, Jenkins, and Krane, among others.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Get information about the Swarm.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker info
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, the output displays information about the two agent nodes and the one manager node in the Swarm.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Check the images currently running on your Swarm.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker ps
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Run a container on the Swarm.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run hello-world
Hello from Docker.
.
.
.
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;docker ps&lt;/code&gt; command to find out which node the container ran on. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker ps -a
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                      PORTS               NAMES
0b0628349187        hello-world         &amp;quot;/hello&amp;quot;                 20 minutes ago      Exited (0) 11 seconds ago                       agent1
.
.
.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this case, the Swarm ran &lt;code&gt;hello-world&lt;/code&gt; on &lt;code&gt;agent1&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;By default, Docker Swarm uses the &amp;ldquo;spread&amp;rdquo; strategy to choose which node runs a container. When you run multiple containers, the spread strategy assigns each container to the node with the fewest containers.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;where-to-go-next&#34;&gt;Where to go next&lt;/h2&gt;

&lt;p&gt;At this point, you&amp;rsquo;ve done the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Created a Swarm discovery token.&lt;/li&gt;
&lt;li&gt;Created Swarm nodes using Docker Machine.&lt;/li&gt;
&lt;li&gt;Managed a Swarm cluster and run containers on it.&lt;/li&gt;
&lt;li&gt;Learned Swarm-related concepts and terminology.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, Docker Swarm has many other aspects and capabilities.
For more information, visit &lt;a href=&#34;https://www.docker.com/docker-swarm&#34;&gt;the Swarm landing page&lt;/a&gt; or read the &lt;a href=&#34;https://docs.docker.com/swarm/&#34;&gt;Swarm documentation&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Filters</title>
      <link>https://docs.docker.com/v1.10/swarm/scheduler/filter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.docker.com/v1.10/swarm/scheduler/filter/</guid>
      <description>

&lt;h1 id=&#34;swarm-filters&#34;&gt;Swarm filters&lt;/h1&gt;

&lt;p&gt;Filters tell Docker Swarm scheduler which nodes to use when creating and running
a container.&lt;/p&gt;

&lt;h2 id=&#34;configure-the-available-filters&#34;&gt;Configure the available filters&lt;/h2&gt;

&lt;p&gt;Filters are divided into two categories, node filters and container configuration
filters. Node filters operate on characteristics of the Docker host or on the
configuration of the Docker daemon. Container configuration filters operate on
characteristics of containers, or on the availability of images on a host.&lt;/p&gt;

&lt;p&gt;Each filter has a name that identifies it. The node filters are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;constraint&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;health&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The container configuration filters are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;affinity&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dependency&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;port&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When you start a Swarm manager with the &lt;code&gt;swarm manage&lt;/code&gt; command, all the filters
are enabled. If you want to limit the filters available to your Swarm, specify a subset
of filters by passing the &lt;code&gt;--filter&lt;/code&gt; flag and the name:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ swarm manage --filter=health --filter=dependency
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Container configuration filters match all containers, including stopped
containers, when applying the filter. To release a node used by a container, you
must remove the container from the node.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;node-filters&#34;&gt;Node filters&lt;/h2&gt;

&lt;p&gt;When creating a container or building an image, you use a &lt;code&gt;constraint&lt;/code&gt; or
&lt;code&gt;health&lt;/code&gt; filter to select a subset of nodes to consider for scheduling.&lt;/p&gt;

&lt;h3 id=&#34;use-a-constraint-filter&#34;&gt;Use a constraint filter&lt;/h3&gt;

&lt;p&gt;Node constraints can refer to Docker&amp;rsquo;s default tags or to custom labels. Default
tags are sourced from &lt;code&gt;docker info&lt;/code&gt;. Often, they relate to properties of the Docker
host. Currently, the default tags include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;node&lt;/code&gt; to refer to the node by ID or name&lt;/li&gt;
&lt;li&gt;&lt;code&gt;storagedriver&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;executiondriver&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kernelversion&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;operatingsystem&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Custom node labels you apply when you start the &lt;code&gt;docker daemon&lt;/code&gt;, for example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker daemon --label com.example.environment=&amp;quot;production&amp;quot; --label
com.example.storage=&amp;quot;ssd&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, when you start a container on the cluster, you can set constraints using
these default tags or custom labels. The Swarm scheduler looks for matching node
on the cluster and starts the container there. This approach has several
practical applications:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Schedule based on specific host properties, for example,&lt;code&gt;storage=ssd&lt;/code&gt; schedules
containers on specific hardware.&lt;/li&gt;
&lt;li&gt;Force containers to run in a given location, for example region=us-east`.&lt;/li&gt;
&lt;li&gt;Create logical cluster partitions by splitting a cluster into
sub-clusters with different properties, for example &lt;code&gt;environment=production&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;example-node-constraints&#34;&gt;Example node constraints&lt;/h4&gt;

&lt;p&gt;To specify custom label for a node, pass a list of &lt;code&gt;--label&lt;/code&gt;
options at &lt;code&gt;docker&lt;/code&gt; startup time. For instance, to start &lt;code&gt;node-1&lt;/code&gt; with the
&lt;code&gt;storage=ssd&lt;/code&gt; label:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker daemon --label storage=ssd
$ swarm join --advertise=192.168.0.42:2375 token://XXXXXXXXXXXXXXXXXX
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You might start a different &lt;code&gt;node-2&lt;/code&gt; with &lt;code&gt;storage=disk&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker daemon --label storage=disk
$ swarm join --advertise=192.168.0.43:2375 token://XXXXXXXXXXXXXXXXXX
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once the nodes are joined to a cluster, the Swarm master pulls their respective
tags.  Moving forward, the master takes the tags into account when scheduling
new containers.&lt;/p&gt;

&lt;p&gt;Continuing the previous example, assuming your cluster with &lt;code&gt;node-1&lt;/code&gt; and
&lt;code&gt;node-2&lt;/code&gt;, you can run a MySQL server container on the cluster.  When you run the
container, you can use a &lt;code&gt;constraint&lt;/code&gt; to ensure the database gets good I/O
performance. You do this by filtering for nodes with flash drives:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt;  run -d -P -e constraint:storage==ssd --name db mysql
f8b693db9cd6

$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt;  ps
CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NODE        NAMES
f8b693db9cd6        mysql:latest        &amp;quot;mysqld&amp;quot;            Less than a second ago   running             192.168.0.42:49178-&amp;gt;3306/tcp    node-1      db
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this example, the master selected all nodes that met the &lt;code&gt;storage=ssd&lt;/code&gt;
constraint and applied resource management on top of them.   Only &lt;code&gt;node-1&lt;/code&gt; was
selected because it&amp;rsquo;s the only host running flash.&lt;/p&gt;

&lt;p&gt;Suppose you want to run an Nginx frontend in a cluster. In this case, you wouldn&amp;rsquo;t want flash drives because the frontend mostly writes logs to disk.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d -P -e constraint:storage==disk --name frontend nginx
963841b138d8

$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; ps
CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NODE        NAMES
963841b138d8        nginx:latest        &amp;quot;nginx&amp;quot;             Less than a second ago   running             192.168.0.43:49177-&amp;gt;80/tcp      node-2      frontend
f8b693db9cd6        mysql:latest        &amp;quot;mysqld&amp;quot;            Up About a minute        running             192.168.0.42:49178-&amp;gt;3306/tcp    node-1      db
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The scheduler selected &lt;code&gt;node-2&lt;/code&gt; since it was started with the &lt;code&gt;storage=disk&lt;/code&gt; label.&lt;/p&gt;

&lt;p&gt;Finally, build args can be used to apply node constraints to a &lt;code&gt;docker build&lt;/code&gt;.
Again, you&amp;rsquo;ll avoid flash drives.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir sinatra
$ cd sinatra
$ echo &amp;quot;FROM ubuntu:14.04&amp;quot; &amp;gt; Dockerfile
$ echo &amp;quot;MAINTAINER Kate Smith &amp;lt;ksmith@example.com&amp;gt;&amp;quot; &amp;gt;&amp;gt; Dockerfile
$ echo &amp;quot;RUN apt-get update &amp;amp;&amp;amp; apt-get install -y ruby ruby-dev&amp;quot; &amp;gt;&amp;gt; Dockerfile
$ echo &amp;quot;RUN gem install sinatra&amp;quot; &amp;gt;&amp;gt; Dockerfile
$ docker build --build-arg=constraint:storage==disk -t ouruser/sinatra:v2 .
Sending build context to Docker daemon 2.048 kB
Step 1 : FROM ubuntu:14.04
 ---&amp;gt; a5a467fddcb8
Step 2 : MAINTAINER Kate Smith &amp;lt;ksmith@example.com&amp;gt;
 ---&amp;gt; Running in 49e97019dcb8
 ---&amp;gt; de8670dcf80e
Removing intermediate container 49e97019dcb8
Step 3 : RUN apt-get update &amp;amp;&amp;amp; apt-get install -y ruby ruby-dev
 ---&amp;gt; Running in 26c9fbc55aeb
 ---&amp;gt; 30681ef95fff
Removing intermediate container 26c9fbc55aeb
Step 4 : RUN gem install sinatra
 ---&amp;gt; Running in 68671d4a17b0
 ---&amp;gt; cd70495a1514
Removing intermediate container 68671d4a17b0
Successfully built cd70495a1514

$ docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
dockerswarm/swarm   master              8c2c56438951        2 days ago          795.7 MB
ouruser/sinatra     v2                  cd70495a1514        35 seconds ago      318.7 MB
ubuntu              14.04               a5a467fddcb8        11 days ago         187.9 MB
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;use-the-health-filter&#34;&gt;Use the health filter&lt;/h3&gt;

&lt;p&gt;The node &lt;code&gt;health&lt;/code&gt; filter prevents the scheduler form running containers
on unhealthy nodes. A node is considered unhealthy if the node is down or it
can&amp;rsquo;t communicate with the cluster store.&lt;/p&gt;

&lt;h2 id=&#34;container-filters&#34;&gt;Container filters&lt;/h2&gt;

&lt;p&gt;When creating a container, you can use three types of container filters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#use-an-affinity-filter&#34;&gt;&lt;code&gt;affinity&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#use-a-depedency-filter&#34;&gt;&lt;code&gt;dependency&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#use-a-port-filter&#34;&gt;&lt;code&gt;port&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;use-an-affinity-filter&#34;&gt;Use an affinity filter&lt;/h3&gt;

&lt;p&gt;Use an &lt;code&gt;affinity&lt;/code&gt; filter to create &amp;ldquo;attractions&amp;rdquo; between containers. For
example, you can run a container and instruct Swarm to schedule it next to
another container based on these affinities:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;container name or id&lt;/li&gt;
&lt;li&gt;an image on the host&lt;/li&gt;
&lt;li&gt;a custom label applied to the container&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These affinities ensure that containers run on the same network node
&amp;mdash; without you having to know what each node is running.&lt;/p&gt;

&lt;h4 id=&#34;example-name-affinity&#34;&gt;Example name affinity&lt;/h4&gt;

&lt;p&gt;You can schedule a new container to run next to another based on a container
name or ID. For example, you can start a container called &lt;code&gt;frontend&lt;/code&gt; running
&lt;code&gt;nginx&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt;  run -d -p 80:80 --name frontend nginx
 87c4376856a8


$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; ps
CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NODE        NAMES
87c4376856a8        nginx:latest        &amp;quot;nginx&amp;quot;             Less than a second ago   running             192.168.0.42:80-&amp;gt;80/tcp         node-1      frontend
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, using &lt;code&gt;-e affinity:container==frontend&lt;/code&gt; value to schedule a second
container to locate and run next to the container named &lt;code&gt;frontend&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --name logger -e affinity:container==frontend logger
 87c4376856a8

$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; ps
CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NODE        NAMES
87c4376856a8        nginx:latest        &amp;quot;nginx&amp;quot;             Less than a second ago   running             192.168.0.42:80-&amp;gt;80/tcp         node-1      frontend
963841b138d8        logger:latest       &amp;quot;logger&amp;quot;            Less than a second ago   running                                             node-1      logger
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because of &lt;code&gt;name&lt;/code&gt; affinity, the  &lt;code&gt;logger&lt;/code&gt; container ends up on &lt;code&gt;node-1&lt;/code&gt; along
with the &lt;code&gt;frontend&lt;/code&gt; container. Instead of the &lt;code&gt;frontend&lt;/code&gt; name you could have
supplied its ID as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --name logger -e affinity:container==87c4376856a8
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;example-image-affinity&#34;&gt;Example image affinity&lt;/h4&gt;

&lt;p&gt;You can schedule a container to run only on nodes where a specific image is
already pulled. For example, suppose you pull a &lt;code&gt;redis&lt;/code&gt; image to two hosts and a
&lt;code&gt;mysql&lt;/code&gt; image to a third.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker -H node-1:2375 pull redis
$ docker -H node-2:2375 pull mysql
$ docker -H node-3:2375 pull redis
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Only &lt;code&gt;node-1&lt;/code&gt; and &lt;code&gt;node-3&lt;/code&gt; have the &lt;code&gt;redis&lt;/code&gt; image. Specify a &lt;code&gt;-e
affinity:image==redis&lt;/code&gt; filter to schedule several additional containers to run
on these nodes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --name redis1 -e affinity:image==redis redis
$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --name redis2 -e affinity:image==redis redis
$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --name redis3 -e affinity:image==redis redis
$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --name redis4 -e affinity:image==redis redis
$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --name redis5 -e affinity:image==redis redis
$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --name redis6 -e affinity:image==redis redis
$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --name redis7 -e affinity:image==redis redis
$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --name redis8 -e affinity:image==redis redis

$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; ps
CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NODE        NAMES
87c4376856a8        redis:latest        &amp;quot;redis&amp;quot;             Less than a second ago   running                                             node-1      redis1
1212386856a8        redis:latest        &amp;quot;redis&amp;quot;             Less than a second ago   running                                             node-1      redis2
87c4376639a8        redis:latest        &amp;quot;redis&amp;quot;             Less than a second ago   running                                             node-3      redis3
1234376856a8        redis:latest        &amp;quot;redis&amp;quot;             Less than a second ago   running                                             node-1      redis4
86c2136253a8        redis:latest        &amp;quot;redis&amp;quot;             Less than a second ago   running                                             node-3      redis5
87c3236856a8        redis:latest        &amp;quot;redis&amp;quot;             Less than a second ago   running                                             node-3      redis6
87c4376856a8        redis:latest        &amp;quot;redis&amp;quot;             Less than a second ago   running                                             node-3      redis7
963841b138d8        redis:latest        &amp;quot;redis&amp;quot;             Less than a second ago   running                                             node-1      redis8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see here, the containers were only scheduled on nodes that had the
&lt;code&gt;redis&lt;/code&gt; image. Instead of the image name, you could have specified the image ID.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker images
REPOSITORY                         TAG                       IMAGE ID            CREATED             VIRTUAL SIZE
redis                              latest                    06a1f75304ba        2 days ago          111.1 MB

$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --name redis1 -e affinity:image==06a1f75304ba redis
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;example-label-affinity&#34;&gt;Example label affinity&lt;/h4&gt;

&lt;p&gt;A label affinity allows you to filter based on a custom container label. For
example, you can run a &lt;code&gt;nginx&lt;/code&gt; container and apply the
&lt;code&gt;com.example.type=frontend&lt;/code&gt; custom label.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d -p 80:80 --label com.example.type=frontend nginx
 87c4376856a8

$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; ps  --filter &amp;quot;label=com.example.type=frontend&amp;quot;
CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NODE        NAMES
87c4376856a8        nginx:latest        &amp;quot;nginx&amp;quot;             Less than a second ago   running             192.168.0.42:80-&amp;gt;80/tcp         node-1      trusting_yonath
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, use &lt;code&gt;-e affinity:com.example.type==frontend&lt;/code&gt; to schedule a container next
to the container with the &lt;code&gt;com.example.type==frontend&lt;/code&gt; label.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d -e affinity:com.example.type==frontend logger
 87c4376856a8

$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; ps
CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NODE        NAMES
87c4376856a8        nginx:latest        &amp;quot;nginx&amp;quot;             Less than a second ago   running             192.168.0.42:80-&amp;gt;80/tcp         node-1      trusting_yonath
963841b138d8        logger:latest       &amp;quot;logger&amp;quot;            Less than a second ago   running                                             node-1      happy_hawking
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;logger&lt;/code&gt; container ends up on &lt;code&gt;node-1&lt;/code&gt; because its affinity with the
&lt;code&gt;com.example.type==frontend&lt;/code&gt; label.&lt;/p&gt;

&lt;h3 id=&#34;use-a-dependency-filter&#34;&gt;Use a dependency filter&lt;/h3&gt;

&lt;p&gt;A container dependency filter co-schedules dependent containers on the same node.
Currently, dependencies are declared as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--volumes-from=dependency&lt;/code&gt; (shared volumes)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--link=dependency:alias&lt;/code&gt; (links)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--net=container:dependency&lt;/code&gt; (shared network stacks)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Swarm attempts to co-locate the dependent container on the same node. If it
cannot be done (because the dependent container doesn&amp;rsquo;t exist, or because the
node doesn&amp;rsquo;t have enough resources), it will prevent the container creation.&lt;/p&gt;

&lt;p&gt;The combination of multiple dependencies are honored if possible. For
instance, if you specify &lt;code&gt;--volumes-from=A --net=container:B&lt;/code&gt;,  the scheduler
attempts to co-locate the container on the same node as &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt;. If those
containers are running on different nodes, Swarm does not schedule the container.&lt;/p&gt;

&lt;h3 id=&#34;use-a-port-filter&#34;&gt;Use a port filter&lt;/h3&gt;

&lt;p&gt;When the &lt;code&gt;port&lt;/code&gt; filter is enabled, a container&amp;rsquo;s port configuration is used as a
unique constraint. Docker Swarm selects a node where a particular port is
available and unoccupied by another container or process. Required ports may be
specified by mapping a host port, or using the host networking an exposing a
port using the container configuration.&lt;/p&gt;

&lt;h4 id=&#34;example-in-bridge-mode&#34;&gt;Example in bridge mode&lt;/h4&gt;

&lt;p&gt;By default, containers run on Docker&amp;rsquo;s bridge network. To use the &lt;code&gt;port&lt;/code&gt; filter
with the bridge network, you run a container as follows.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d -p 80:80 nginx
87c4376856a8

$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; ps
CONTAINER ID    IMAGE               COMMAND         PORTS                       NODE        NAMES
87c4376856a8    nginx:latest        &amp;quot;nginx&amp;quot;         192.168.0.42:80-&amp;gt;80/tcp     node-1      prickly_engelbart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Docker Swarm selects a node where port &lt;code&gt;80&lt;/code&gt; is available and unoccupied by another
container or process, in this case &lt;code&gt;node-1&lt;/code&gt;. Attempting to run another container
that uses the host port &lt;code&gt;80&lt;/code&gt; results in Swarm selecting a different node,
because port &lt;code&gt;80&lt;/code&gt; is already occupied on &lt;code&gt;node-1&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d -p 80:80 nginx
963841b138d8

$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; ps
CONTAINER ID        IMAGE          COMMAND        PORTS                           NODE        NAMES
963841b138d8        nginx:latest   &amp;quot;nginx&amp;quot;        192.168.0.43:80-&amp;gt;80/tcp         node-2      dreamy_turing
87c4376856a8        nginx:latest   &amp;quot;nginx&amp;quot;        192.168.0.42:80-&amp;gt;80/tcp         node-1      prickly_engelbart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Again, repeating the same command will result in the selection of &lt;code&gt;node-3&lt;/code&gt;,
since port &lt;code&gt;80&lt;/code&gt; is neither available on &lt;code&gt;node-1&lt;/code&gt; nor &lt;code&gt;node-2&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d -p 80:80 nginx
963841b138d8

$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; ps
CONTAINER ID   IMAGE               COMMAND        PORTS                           NODE        NAMES
f8b693db9cd6   nginx:latest        &amp;quot;nginx&amp;quot;        192.168.0.44:80-&amp;gt;80/tcp         node-3      stoic_albattani
963841b138d8   nginx:latest        &amp;quot;nginx&amp;quot;        192.168.0.43:80-&amp;gt;80/tcp         node-2      dreamy_turing
87c4376856a8   nginx:latest        &amp;quot;nginx&amp;quot;        192.168.0.42:80-&amp;gt;80/tcp         node-1      prickly_engelbart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, Docker Swarm will refuse to run another container that requires port
&lt;code&gt;80&lt;/code&gt;, because it is not available on any node in the cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d -p 80:80 nginx
2014/10/29 00:33:20 Error response from daemon: no resources available to schedule container
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each container occupies port &lt;code&gt;80&lt;/code&gt; on its residing node when the container
is created and releases the port when the container is deleted. A container in &lt;code&gt;exited&lt;/code&gt;
state still owns the port. If &lt;code&gt;prickly_engelbart&lt;/code&gt; on &lt;code&gt;node-1&lt;/code&gt; is stopped but not
deleted, trying to start another container on &lt;code&gt;node-1&lt;/code&gt; that requires port &lt;code&gt;80&lt;/code&gt; would fail
because port &lt;code&gt;80&lt;/code&gt; is associated with &lt;code&gt;prickly_engelbart&lt;/code&gt;. To increase running
instances of nginx, you can either restart &lt;code&gt;prickly_engelbart&lt;/code&gt;, or start another container
after deleting &lt;code&gt;prickly_englbart&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&#34;node-port-filter-with-host-networking&#34;&gt;Node port filter with host networking&lt;/h4&gt;

&lt;p&gt;A container running with &lt;code&gt;--net=host&lt;/code&gt; differs from the default
&lt;code&gt;bridge&lt;/code&gt; mode as the &lt;code&gt;host&lt;/code&gt; mode does not perform any port binding. Instead,
host mode requires that you  explicitly expose one or more port numbers.  You
expose a port using &lt;code&gt;EXPOSE&lt;/code&gt; in the &lt;code&gt;Dockerfile&lt;/code&gt; or &lt;code&gt;--expose&lt;/code&gt; on the command
line. Swarm makes use of this information in conjunction with the &lt;code&gt;host&lt;/code&gt; mode to
choose an available node for a new container.&lt;/p&gt;

&lt;p&gt;For example, the following commands start &lt;code&gt;nginx&lt;/code&gt; on 3-node cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --expose=80 --net=host nginx
640297cb29a7
$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --expose=80 --net=host nginx
7ecf562b1b3f
$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --expose=80 --net=host nginx
09a92f582bc2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Port binding information is not available through the &lt;code&gt;docker ps&lt;/code&gt; command because
all the nodes were started with the &lt;code&gt;host&lt;/code&gt; network.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; ps
CONTAINER ID        IMAGE               COMMAND                CREATED                  STATUS              PORTS               NAMES
640297cb29a7        nginx:1             &amp;quot;nginx -g &#39;daemon of   Less than a second ago   Up 30 seconds                           box3/furious_heisenberg
7ecf562b1b3f        nginx:1             &amp;quot;nginx -g &#39;daemon of   Less than a second ago   Up 28 seconds                           box2/ecstatic_meitner
09a92f582bc2        nginx:1             &amp;quot;nginx -g &#39;daemon of   46 seconds ago           Up 27 seconds                           box1/mad_goldstine
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Swarm refuses the operation when trying to instantiate the 4th container.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$  docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --expose=80 --net=host nginx
FATA[0000] Error response from daemon: unable to find a node with port 80/tcp available in the Host mode
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, port binding to the different value, for example  &lt;code&gt;81&lt;/code&gt;, is still allowed.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$  docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d -p 81:80 nginx:latest
832f42819adc
$  docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; ps
CONTAINER ID        IMAGE               COMMAND                CREATED                  STATUS                  PORTS                                 NAMES
832f42819adc        nginx:1             &amp;quot;nginx -g &#39;daemon of   Less than a second ago   Up Less than a second   443/tcp, 192.168.136.136:81-&amp;gt;80/tcp   box3/thirsty_hawking
640297cb29a7        nginx:1             &amp;quot;nginx -g &#39;daemon of   8 seconds ago            Up About a minute                                             box3/furious_heisenberg
7ecf562b1b3f        nginx:1             &amp;quot;nginx -g &#39;daemon of   13 seconds ago           Up About a minute                                             box2/ecstatic_meitner
09a92f582bc2        nginx:1             &amp;quot;nginx -g &#39;daemon of   About a minute ago       Up About a minute                                             box1/mad_goldstine
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;how-to-write-filter-expressions&#34;&gt;How to write filter expressions&lt;/h2&gt;

&lt;p&gt;To apply a node &lt;code&gt;constraint&lt;/code&gt; or container &lt;code&gt;affinity&lt;/code&gt; filters you must set
environment variables on the container using filter expressions, for example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --name redis1 -e affinity:image==~redis redis
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each expression must be in the form:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;filter-type&amp;gt;:&amp;lt;key&amp;gt;&amp;lt;operator&amp;gt;&amp;lt;value&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;&amp;lt;filter-type&amp;gt;&lt;/code&gt; is either the &lt;code&gt;affinity&lt;/code&gt; or the &lt;code&gt;container&lt;/code&gt; keyword. It
identifies the type filter you intend to use.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;&amp;lt;key&amp;gt;&lt;/code&gt; is an alpha-numeric and must start with a letter or underscore. The
&lt;code&gt;&amp;lt;key&amp;gt;&lt;/code&gt; corresponds to one of the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the &lt;code&gt;container&lt;/code&gt; keyword&lt;/li&gt;
&lt;li&gt;the &lt;code&gt;node&lt;/code&gt; keyword&lt;/li&gt;
&lt;li&gt;a default tag (node constraints)&lt;/li&gt;
&lt;li&gt;a custom metadata label (nodes or containers).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;code&gt;&amp;lt;operator&amp;gt;&lt;/code&gt;is either &lt;code&gt;==&lt;/code&gt; or &lt;code&gt;!=&lt;/code&gt;. By default, expression operators are
hard enforced. If an expression is not met exactly , the manager does not
schedule the container. You can use a &lt;code&gt;~&lt;/code&gt;(tilde) to create a &amp;ldquo;soft&amp;rdquo; expression.
The scheduler tries to match a soft expression. If the expression is not met,
the scheduler discards the filter and schedules the container according to the
scheduler&amp;rsquo;s strategy.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;&amp;lt;value&amp;gt;&lt;/code&gt; is an alpha-numeric string, dots, hyphens, and underscores making
up one of the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A globbing pattern, for example, &lt;code&gt;abc*&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;A regular expression in the form of &lt;code&gt;/regexp/&lt;/code&gt;. See
&lt;a href=&#34;https://github.com/google/re2/wiki/Syntax&#34;&gt;re2 syntax&lt;/a&gt; for the supported
regex syntax.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following examples illustrate some possible expressions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;constraint:node==node1&lt;/code&gt; matches node &lt;code&gt;node1&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;constraint:node!=node1&lt;/code&gt; matches all nodes, except &lt;code&gt;node1&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;constraint:region!=us*&lt;/code&gt; matches all nodes outside with a &lt;code&gt;region&lt;/code&gt; tag prefixed with &lt;code&gt;us&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;constraint:node==/node[12]/&lt;/code&gt; matches nodes &lt;code&gt;node1&lt;/code&gt; and &lt;code&gt;node2&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;constraint:node==/node\d/&lt;/code&gt; matches all nodes with &lt;code&gt;node&lt;/code&gt; + 1 digit.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;constraint:node!=/node-[01]/&lt;/code&gt; matches all nodes, except &lt;code&gt;node-0&lt;/code&gt; and &lt;code&gt;node-1&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;constraint:node!=/foo\[bar\]/&lt;/code&gt; matches all nodes, except &lt;code&gt;foo[bar]&lt;/code&gt;. You can see the use of escape characters here.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;constraint:node==/(?i)node1/&lt;/code&gt; matches node &lt;code&gt;node1&lt;/code&gt; case-insensitive. So &lt;code&gt;NoDe1&lt;/code&gt; or &lt;code&gt;NODE1&lt;/code&gt; also match.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;affinity:image==~redis&lt;/code&gt; tries to match for nodes running container with a &lt;code&gt;redis&lt;/code&gt; image&lt;/li&gt;
&lt;li&gt;&lt;code&gt;constraint:region==~us*&lt;/code&gt; searches for nodes in the cluster belongs to the&lt;/li&gt;
&lt;li&gt;&lt;code&gt;us&lt;/code&gt; region&lt;/li&gt;
&lt;li&gt;&lt;code&gt;affinity:container!=~redis*&lt;/code&gt; schedule a new &lt;code&gt;redis5&lt;/code&gt; container to a node
without a container that satisfies &lt;code&gt;redis*&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;related-information&#34;&gt;Related information&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/&#34;&gt;Docker Swarm overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/discovery/&#34;&gt;Discovery options&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/scheduler/strategy/&#34;&gt;Scheduler strategies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/swarm-api/&#34;&gt;Swarm API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>High availability in Swarm</title>
      <link>https://docs.docker.com/v1.10/swarm/multi-manager-setup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.docker.com/v1.10/swarm/multi-manager-setup/</guid>
      <description>

&lt;h1 id=&#34;high-availability-in-docker-swarm&#34;&gt;High availability in Docker Swarm&lt;/h1&gt;

&lt;p&gt;In Docker Swarm, the &lt;strong&gt;swarm manager&lt;/strong&gt; is responsible for the entire cluster and manages the resources of multiple &lt;em&gt;Docker hosts&lt;/em&gt; at scale. If the swarm manager dies, you must create a new one and deal with an interruption of service.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;High Availability&lt;/em&gt; feature allows a Docker Swarm to gracefully handle the failover of a manager instance. Using this feature, you can create a single &lt;strong&gt;primary manager&lt;/strong&gt; instance and multiple &lt;strong&gt;replica&lt;/strong&gt; instances.&lt;/p&gt;

&lt;p&gt;A primary manager is the main point of contact with the Docker Swarm cluster. You can also create and talk to replica instances that will act as backups. Requests issued on a replica are automatically proxied to the primary manager. If the primary manager fails, a replica takes away the lead. In this way, you always keep a point of contact with the cluster.&lt;/p&gt;

&lt;h2 id=&#34;setup-primary-and-replicas&#34;&gt;Setup primary and replicas&lt;/h2&gt;

&lt;p&gt;This section explains how to set up Docker Swarm using multiple &lt;strong&gt;managers&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&#34;assumptions&#34;&gt;Assumptions&lt;/h3&gt;

&lt;p&gt;You need either a &lt;code&gt;Consul&lt;/code&gt;, &lt;code&gt;etcd&lt;/code&gt;, or &lt;code&gt;Zookeeper&lt;/code&gt; cluster. This procedure is written assuming a &lt;code&gt;Consul&lt;/code&gt; server running on address &lt;code&gt;192.168.42.10:8500&lt;/code&gt;. All hosts will have a Docker Engine configured to listen on port 2375.  We will be configuring the Managers to operate on port 4000. The sample swarm configuration has three machines:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;manager-1&lt;/code&gt; on &lt;code&gt;192.168.42.200&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;manager-2&lt;/code&gt; on &lt;code&gt;192.168.42.201&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;manager-3&lt;/code&gt; on &lt;code&gt;192.168.42.202&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;create-the-primary-manager&#34;&gt;Create the primary manager&lt;/h3&gt;

&lt;p&gt;You use the &lt;code&gt;swarm manage&lt;/code&gt; command with the &lt;code&gt;--replication&lt;/code&gt; and &lt;code&gt;--advertise&lt;/code&gt; flags to create a primary manager.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  user@manager-1 $ swarm manage -H :4000 &amp;lt;tls-config-flags&amp;gt; --replication --advertise 192.168.42.200:4000 consul://192.168.42.10:8500/nodes
  INFO[0000] Listening for HTTP addr=:4000 proto=tcp
  INFO[0000] Cluster leadership acquired
  INFO[0000] New leader elected: 192.168.42.200:4000
  [...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The  &lt;code&gt;--replication&lt;/code&gt; flag tells swarm that the manager is part of a a multi-manager configuration and that this primary manager competes with other manager instances for the primary role. The primary manager has the authority to manage cluster, replicate logs, and replicate events happening inside the cluster.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;--advertise&lt;/code&gt; option specifies the primary manager address. Swarm uses this address to advertise to the cluster when the node is elected as the primary. As you see in the command&amp;rsquo;s output, the address you provided now appears to be the one of the elected Primary manager.&lt;/p&gt;

&lt;h3 id=&#34;create-two-replicas&#34;&gt;Create two replicas&lt;/h3&gt;

&lt;p&gt;Now that you have a primary manager, you can create replicas.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;user@manager-2 $ swarm manage -H :4000 &amp;lt;tls-config-flags&amp;gt; --replication --advertise 192.168.42.201:4000 consul://192.168.42.10:8500/nodes
INFO[0000] Listening for HTTP                            addr=:4000 proto=tcp
INFO[0000] Cluster leadership lost
INFO[0000] New leader elected: 192.168.42.200:4000
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command creates a replica manager on &lt;code&gt;192.168.42.201:4000&lt;/code&gt; which is looking at &lt;code&gt;192.168.42.200:4000&lt;/code&gt; as the primary manager.&lt;/p&gt;

&lt;p&gt;Create an additional, third &lt;em&gt;manager&lt;/em&gt; instance:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;user@manager-3 $ swarm manage -H :4000 &amp;lt;tls-config-flags&amp;gt; --replication --advertise 192.168.42.202:4000 consul://192.168.42.10:8500/nodes
INFO[0000] Listening for HTTP                            addr=:4000 proto=tcp
INFO[0000] Cluster leadership lost
INFO[0000] New leader elected: 192.168.42.200:4000
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once you have established your primary manager and the replicas, create &lt;strong&gt;swarm agents&lt;/strong&gt; as you normally would.&lt;/p&gt;

&lt;h3 id=&#34;list-machines-in-the-cluster&#34;&gt;List machines in the cluster&lt;/h3&gt;

&lt;p&gt;Typing &lt;code&gt;docker info&lt;/code&gt; should give you an output similar to the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;user@my-machine $ export DOCKER_HOST=192.168.42.200:4000 # Points to manager-1
user@my-machine $ docker info
Containers: 0
Images: 25
Storage Driver:
Role: Primary  &amp;lt;--------- manager-1 is the Primary manager
Primary: 192.168.42.200
Strategy: spread
Filters: affinity, health, constraint, port, dependency
Nodes: 3
 swarm-agent-0: 192.168.42.100:2375
  └ Containers: 0
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 2.053 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=3.13.0-49-generic, operatingsystem=Ubuntu 14.04.2 LTS, storagedriver=aufs
 swarm-agent-1: 192.168.42.101:2375
  └ Containers: 0
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 2.053 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=3.13.0-49-generic, operatingsystem=Ubuntu 14.04.2 LTS, storagedriver=aufs
 swarm-agent-2: 192.168.42.102:2375
  └ Containers: 0
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 2.053 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=3.13.0-49-generic, operatingsystem=Ubuntu 14.04.2 LTS, storagedriver=aufs
Execution Driver:
Kernel Version:
Operating System:
CPUs: 3
Total Memory: 6.158 GiB
Name:
ID:
Http Proxy:
Https Proxy:
No Proxy:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This information shows that &lt;code&gt;manager-1&lt;/code&gt; is the current primary and supplies the address to use to contact this primary.&lt;/p&gt;

&lt;h2 id=&#34;test-the-failover-mechanism&#34;&gt;Test the failover mechanism&lt;/h2&gt;

&lt;p&gt;To test the failover mechanism, you shut down the designated primary manager.
Issue a &lt;code&gt;Ctrl-C&lt;/code&gt; or &lt;code&gt;kill&lt;/code&gt; the current primary manager (&lt;code&gt;manager-1&lt;/code&gt;) to shut it down.&lt;/p&gt;

&lt;h3 id=&#34;wait-for-automated-failover&#34;&gt;Wait for automated failover&lt;/h3&gt;

&lt;p&gt;After a short time, the other instances detect the failure and a replica takes the &lt;em&gt;lead&lt;/em&gt; to become the primary manager.&lt;/p&gt;

&lt;p&gt;For example, look at &lt;code&gt;manager-2&lt;/code&gt;&amp;rsquo;s logs:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;user@manager-2 $ swarm manage -H :4000 &amp;lt;tls-config-flags&amp;gt; --replication --advertise 192.168.42.201:4000 consul://192.168.42.10:8500/nodes
INFO[0000] Listening for HTTP                            addr=:4000 proto=tcp
INFO[0000] Cluster leadership lost
INFO[0000] New leader elected: 192.168.42.200:4000
INFO[0038] New leader elected: 192.168.42.201:4000
INFO[0038] Cluster leadership acquired               &amp;lt;--- We have been elected as the new Primary Manager
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because the primary manager, &lt;code&gt;manager-1&lt;/code&gt;, failed right after it was elected, the replica with the address &lt;code&gt;192.168.42.201:4000&lt;/code&gt;, &lt;code&gt;manager-2&lt;/code&gt;, recognized the failure and attempted to take away the lead. Because &lt;code&gt;manager-2&lt;/code&gt; was fast enough, the process was effectively elected as the primary manager. As a result, &lt;code&gt;manager-2&lt;/code&gt; became the primary manager of the cluster.&lt;/p&gt;

&lt;p&gt;If we take a look at &lt;code&gt;manager-3&lt;/code&gt; we should see those &lt;code&gt;logs&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;user@manager-3 $ swarm manage -H :4000 &amp;lt;tls-config-flags&amp;gt; --replication --advertise 192.168.42.202:4000 consul://192.168.42.10:8500/nodes
INFO[0000] Listening for HTTP                            addr=:4000 proto=tcp
INFO[0000] Cluster leadership lost
INFO[0000] New leader elected: 192.168.42.200:4000
INFO[0036] New leader elected: 192.168.42.201:4000   &amp;lt;--- manager-2 sees the new Primary Manager
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At this point, we need to export the new &lt;code&gt;DOCKER_HOST&lt;/code&gt; value.&lt;/p&gt;

&lt;h3 id=&#34;switch-the-primary&#34;&gt;Switch the primary&lt;/h3&gt;

&lt;p&gt;To switch the &lt;code&gt;DOCKER_HOST&lt;/code&gt; to use &lt;code&gt;manager-2&lt;/code&gt; as the primary, you do the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;user@my-machine $ export DOCKER_HOST=192.168.42.201:4000 # Points to manager-2
user@my-machine $ docker info
Containers: 0
Images: 25
Storage Driver:
Role: Replica  &amp;lt;--------- manager-2 is a Replica
Primary: 192.168.42.200
Strategy: spread
Filters: affinity, health, constraint, port, dependency
Nodes: 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can use the &lt;code&gt;docker&lt;/code&gt; command on any Docker Swarm primary manager or any replica.&lt;/p&gt;

&lt;p&gt;If you like, you can use custom mechanisms to always point &lt;code&gt;DOCKER_HOST&lt;/code&gt; to the current primary manager. Then, you never lose contact with your Docker Swarm in the event of a failover.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to get Swarm</title>
      <link>https://docs.docker.com/v1.10/swarm/get-swarm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.docker.com/v1.10/swarm/get-swarm/</guid>
      <description>

&lt;h1 id=&#34;how-to-get-docker-swarm&#34;&gt;How to get Docker Swarm&lt;/h1&gt;

&lt;p&gt;You can create a Docker Swarm cluster using the &lt;code&gt;swarm&lt;/code&gt; executable image from a
container or using an executable &lt;code&gt;swarm&lt;/code&gt; binary you install on your system. This
page introduces the two methods and discusses their pros and cons.&lt;/p&gt;

&lt;h2 id=&#34;create-a-cluster-with-an-interactive-container&#34;&gt;Create a cluster with an interactive container&lt;/h2&gt;

&lt;p&gt;You can use the Docker Swarm official image to create a cluster. The image is
built by Docker and updated regularly through an automated build. To use the
image, you run it a container via the Engine &lt;code&gt;docker run&lt;/code&gt; command. The image has
multiple options and subcommands you can use to create and manage a Swarm cluster.&lt;/p&gt;

&lt;p&gt;The first time you use any image, Docker Engine checks to see if you already have the image in your environment. By default Docker runs the &lt;code&gt;swarm:latest&lt;/code&gt; version but you can also specify a tag other than &lt;code&gt;latest&lt;/code&gt;. If you have an image locally but a newer one exists on Docker Hub, Engine downloads it.&lt;/p&gt;

&lt;h3 id=&#34;run-the-swarm-image-from-a-container&#34;&gt;Run the Swarm image from a container&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Open a terminal on a host running Engine.&lt;/p&gt;

&lt;p&gt;If you are using Mac or Windows, then you must make sure you have started an Docker Engine host running and pointed your terminal environment to it with the Docker Machine commands. If you aren&amp;rsquo;t sure, you can verify:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine ls
NAME      ACTIVE   URL          STATE     URL                         SWARM   DOCKER    ERRORS
default   *       virtualbox   Running   tcp://192.168.99.100:2376           v1.9.1    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This shows an environment running an Engine host on the &lt;code&gt;default&lt;/code&gt; instance.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;swarm&lt;/code&gt; image to execute a command.&lt;/p&gt;

&lt;p&gt;The easiest command is to get the help for the image. This command shows all the options that are available with the image.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run swarm --help
Unable to find image &#39;swarm:latest&#39; locally
latest: Pulling from library/swarm
d681c900c6e3: Pull complete
188de6f24f3f: Pull complete
90b2ffb8d338: Pull complete
237af4efea94: Pull complete
3b3fc6f62107: Pull complete
7e6c9135b308: Pull complete
986340ab62f0: Pull complete
a9975e2cc0a3: Pull complete
Digest: sha256:c21fd414b0488637b1f05f13a59b032a3f9da5d818d31da1a4ca98a84c0c781b
Status: Downloaded newer image for swarm:latest
Usage: swarm [OPTIONS] COMMAND [arg...]

A Docker-native clustering system

Version: 1.0.1 (744e3a3)

Options:
  --debug           debug mode [$DEBUG]
  --log-level, -l &amp;quot;info&amp;quot;    Log level (options: debug, info, warn, error, fatal, panic)
  --help, -h            show help
  --version, -v         print the version

Commands:
  create, c Create a cluster
  list, l   List nodes in a cluster
  manage, m Manage a docker cluster
  join, j   join a docker cluster
  help, h   Shows a list of commands or help for one command

Run &#39;swarm COMMAND --help&#39; for more information on a command.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this example, the &lt;code&gt;swarm&lt;/code&gt; image did not exist on the Engine host, so the
Engine downloaded it. After it downloaded, the image executed the &lt;code&gt;help&lt;/code&gt;
subcommand to display the help text. After displaying the help, the &lt;code&gt;swarm&lt;/code&gt;
image exits and returns your to your terminal command line.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;List the running containers on your Engine host.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Swarm is no longer running. The &lt;code&gt;swarm&lt;/code&gt; image exits after you issue it a command.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;why-use-the-image&#34;&gt;Why use the image?&lt;/h3&gt;

&lt;p&gt;Using a Swarm container has three key benefits over other methods:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You don&amp;rsquo;t need to install a binary on the system to use the image.&lt;/li&gt;
&lt;li&gt;The single command &lt;code&gt;docker run&lt;/code&gt; command gets and run the most recent version of the image every time.&lt;/li&gt;
&lt;li&gt;The container isolates Swarm from your host environment. You don&amp;rsquo;t need to perform or maintain shell paths and environments.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Running the Swarm image is the recommended way to create and manage your Swarm cluster. All of Docker&amp;rsquo;s documentation and tutorials use this method.&lt;/p&gt;

&lt;h2 id=&#34;run-a-swarm-binary&#34;&gt;Run a Swarm binary&lt;/h2&gt;

&lt;p&gt;Before you run a Swarm binary directly on a host operating system (OS), you compile the binary from the source code or get a trusted copy from another location. Then you run the Swarm binary.&lt;/p&gt;

&lt;p&gt;To compile Swarm from source code, refer to the instructions in
&lt;a href=&#34;http://github.com/docker/swarm/blob/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;why-use-the-binary&#34;&gt;Why use the binary?&lt;/h3&gt;

&lt;p&gt;Using a Swarm binary this way has one key benefit over other methods: If you are
a developer who contributes to the Swarm project, you can test your code changes
without &amp;ldquo;containerizing&amp;rdquo; the binary before you run it.&lt;/p&gt;

&lt;p&gt;Running a Swarm binary on the host OS has disadvantages:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Compilation from source is a burden.&lt;/li&gt;
&lt;li&gt;The binary doesn&amp;rsquo;t have the benefits that
Docker containers provide, such as isolation.&lt;/li&gt;
&lt;li&gt;Most Docker documentation and tutorials don&amp;rsquo;t show this method of running swarm.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lastly, because the Swarm nodes don&amp;rsquo;t use Engine, you can&amp;rsquo;t use Docker-based
software tools, such as Docker Engine CLI at the node level.&lt;/p&gt;

&lt;h2 id=&#34;related-information&#34;&gt;Related information&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://hub.docker.com/_/swarm/&#34;&gt;Docker Swarm official image&lt;/a&gt; repository on Docker Hub&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/provision-with-machine/&#34;&gt;Provision a Swarm with Docker Machine&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Learn the application architecture</title>
      <link>https://docs.docker.com/v1.10/swarm/swarm_at_scale/01-about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.docker.com/v1.10/swarm/swarm_at_scale/01-about/</guid>
      <description>

&lt;h1 id=&#34;learn-the-application-architecture&#34;&gt;Learn the application architecture&lt;/h1&gt;

&lt;p&gt;On this page, you learn about the Swarm at scale example.  Make sure you have
read through &lt;a href=&#34;../v1.10/swarm/swarm_at_scale/&#34;&gt;the introduction&lt;/a&gt; to get an idea of the skills and time
required first.&lt;/p&gt;

&lt;h2 id=&#34;learn-the-example-back-story&#34;&gt;Learn the example back story&lt;/h2&gt;

&lt;p&gt;Your company is a pet food company that has bought an commercial during the
Superbowl. The commercial drives viewers to a web survey that asks users to vote
&amp;ndash; cats or dogs. You are developing the web survey.&lt;/p&gt;

&lt;p&gt;Your survey must ensure that millions of people can vote concurrently without
your website becoming unavailable. You don&amp;rsquo;t need real-time results, a company
press release announces the results. However, you do need confidence that every
vote is counted.&lt;/p&gt;

&lt;h2 id=&#34;understand-the-application-architecture&#34;&gt;Understand the application architecture&lt;/h2&gt;

&lt;p&gt;The voting application is a dockerized microservice application. It uses a
parallel web frontend that sends jobs to asynchronous background workers. The
application&amp;rsquo;s design can accommodate arbitrarily large scale. The diagram below
shows the high level architecture of the application.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/app-architecture.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The application is fully dockerized with all services running inside of
containers.&lt;/p&gt;

&lt;p&gt;The frontend consists of an Interlock load balancer with &lt;em&gt;N&lt;/em&gt; frontend web
servers and associated queues. The load balancer can handle an arbitrary number
of web containers behind it (&lt;code&gt;frontend01&lt;/code&gt;- &lt;code&gt;frontendN&lt;/code&gt;). The web containers run
a simple Python Flask application. Each web container accepts votes and queues
them to a Redis container on the same node. Each web container and Redis queue
pair operates independently.&lt;/p&gt;

&lt;p&gt;The load balancer together with the independent pairs allows the entire
application to scale to an arbitrary size as needed to meet demand.&lt;/p&gt;

&lt;p&gt;Behind the frontend is a worker tier which runs on separate nodes. This tier:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;scans the Redis containers&lt;/li&gt;
&lt;li&gt;dequeues votes&lt;/li&gt;
&lt;li&gt;deduplicates votes to prevent double voting&lt;/li&gt;
&lt;li&gt;commits the results to a Postgres container running on a separate node&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Just like the front end, the worker tier can also scale arbitrarily. The worker
count and frontend count are independent from each other.&lt;/p&gt;

&lt;h2 id=&#34;swarm-cluster-architecture&#34;&gt;Swarm Cluster Architecture&lt;/h2&gt;

&lt;p&gt;To support the application, the design calls for a Swarm cluster with a single
Swarm manager and four nodes as shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/swarm-cluster-arch.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;All four nodes in the cluster are running the Docker daemon, as is the Swarm
manager and the Interlock load balancer. The Swarm manager exists on a Docker
Engine host that is part of the cluster and is considered out of band for
the application. The Interlock load balancer could be placed inside of the
cluster, but for this demonstration it is not.&lt;/p&gt;

&lt;p&gt;A container network is overlayed on top of the Swarm cluster using the container
overlay feature of Docker engine. The dockerized microservices are deployed to
this network.  After completing the example and deploying your application, this
is what your environment should look like.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/final-result.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As the previous diagram shows, each node in the cluster runs the following containers:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;frontend01&lt;/code&gt;:

&lt;ul&gt;
&lt;li&gt;Container: Python flask web app (frontend01)&lt;/li&gt;
&lt;li&gt;Container: Redis (redis01)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;frontend02&lt;/code&gt;:

&lt;ul&gt;
&lt;li&gt;Container: Python flask web app (frontend02)&lt;/li&gt;
&lt;li&gt;Container: Redis (redis02)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;worker01&lt;/code&gt;: vote worker app (worker01)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;store&lt;/code&gt;:

&lt;ul&gt;
&lt;li&gt;Container: Postgres (pg)&lt;/li&gt;
&lt;li&gt;Container: results app (results-app)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After you deploy the application, you&amp;rsquo;ll configure your local system so that you
can test the application from your local browser. In production, of course, this
step wouldn&amp;rsquo;t be needed.&lt;/p&gt;

&lt;h2 id=&#34;the-network-infrastructure&#34;&gt;The network infrastructure&lt;/h2&gt;

&lt;p&gt;The example assumes you are deploying the application to a Docker Swarm cluster
running on top of Amazon Web Services (AWS). AWS is an example only. There is
nothing about this application or deployment that requires it. You could deploy
the application to a Docker Swarm cluster running on; a different cloud provider
such as Microsoft Azure, on premises in your own physical data center, or in a
development environment on your laptop.&lt;/p&gt;

&lt;h2 id=&#34;next-step&#34;&gt;Next step&lt;/h2&gt;

&lt;p&gt;Now that you understand the application architecture, you need to deploy a
network configuration that can support it. In the next step, you use AWS to
&lt;a href=&#34;../v1.10/swarm/swarm_at_scale/02-deploy-infra/&#34;&gt;deploy network infrastructure&lt;/a&gt; for use in this sample.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Overview Docker Swarm with TLS</title>
      <link>https://docs.docker.com/v1.10/swarm/secure-swarm-tls/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.docker.com/v1.10/swarm/secure-swarm-tls/</guid>
      <description>

&lt;h1 id=&#34;overview-swarm-with-tls&#34;&gt;Overview Swarm with TLS&lt;/h1&gt;

&lt;p&gt;All nodes in a Swarm cluster must bind their Docker daemons to a network port.
This has obvious security implications. These implications are compounded when
the network in question is untrusted such as the internet. To mitigate these
risks, Docker Swarm and the Docker Engine daemon support Transport Layer Security
(TLS).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: TLS is the successor to SSL (Secure Sockets Layer) and the two
terms are often used interchangeably. Docker uses TLS, this
term is used throughout this article.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;learn-the-tls-concepts&#34;&gt;Learn the TLS concepts&lt;/h2&gt;

&lt;p&gt;Before going further, it is important to understand the basic concepts of TLS
and public key infrastructure (PKI).&lt;/p&gt;

&lt;p&gt;Public key infrastructure is a combination of security-related technologies,
policies, and procedures, that are used to create and manage digital
certificates. These certificates and infrastructure secure digital
communication using mechanisms such as authentication and encryption.&lt;/p&gt;

&lt;p&gt;The following analogy may be useful. It is common practice that passports are
used to verify an individual&amp;rsquo;s identity. Passports usually contain a photograph
and biometric information that identify the owner. A passport also lists the
country that issued it, as well as &lt;em&gt;valid from&lt;/em&gt; and &lt;em&gt;valid to&lt;/em&gt; dates. Digital
certificates are very similar. The text below is an extract from a a digital
certificate:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Certificate:
Data:
    Version: 3 (0x2)
    Serial Number: 9590646456311914051 (0x8518d2237ad49e43)
Signature Algorithm: sha256WithRSAEncryption
    Issuer: C=US, ST=CA, L=Sanfrancisco, O=Docker Inc
    Validity
        Not Before: Jan 18 09:42:16 2016 GMT
        Not After : Jan 15 09:42:16 2026 GMT
    Subject: CN=swarm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This certificate identifies a computer called &lt;strong&gt;swarm&lt;/strong&gt;. The certificate is valid between January 2016 and January 2026 and was issued by Docker Inc based in the state of California in the US.&lt;/p&gt;

&lt;p&gt;Just as passports authenticate individuals as they board flights and clear
customs, digital certificates authenticate computers on a network.&lt;/p&gt;

&lt;p&gt;Public key infrastructure (PKI) is the combination of technologies, policies,
and procedures that work behind the scenes to enable digital certificates. Some
of the technologies, policies and procedures provided by PKI include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Services to securely request certificates&lt;/li&gt;
&lt;li&gt;Procedures to authenticate the entity requesting the certificate&lt;/li&gt;
&lt;li&gt;Procedures to determine the entity&amp;rsquo;s eligibility for the certificate&lt;/li&gt;
&lt;li&gt;Technologies and processes to issue certificates&lt;/li&gt;
&lt;li&gt;Technologies and processes to revoke certificates&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;how-does-docker-engine-authenticate-using-tls&#34;&gt;How does Docker Engine authenticate using TLS&lt;/h2&gt;

&lt;p&gt;In this section, you&amp;rsquo;ll learn how Docker Engine and Swarm use PKI and
certificates to increase security.&lt;/p&gt;

&lt;!--[metadata]&gt;Need to know about encryption too&lt;![end-metadata]--&gt;

&lt;p&gt;You can configure both the Docker Engine CLI and the Engine daemon to require
TLS for authentication.  Configuring TLS means that all communications between
the Engine CLI and the Engine daemon must be accompanied with, and signed by a
trusted digital certificate. The Engine CLI must provide its digital certificate
before the Engine daemon will accept incoming commands from it.&lt;/p&gt;

&lt;p&gt;The Engine daemon must also trust the certificate that the Engine CLI uses.
This trust is usually established by way of a trusted third party. The Engine
CLI and daemon in the diagram below are configured to require TLS
authentication.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/trust-diagram.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The trusted third party in this diagram is the the Certificate Authority (CA)
server. Like the country in the passport example, a CA creates, signs, issues,
revokes certificates. Trust is established by installing the CA&amp;rsquo;s root
certificate on the host running the Engine daemon. The Engine CLI then requests
its own certificate from the CA server, which the CA server signs and issues to
the client.&lt;/p&gt;

&lt;p&gt;The Engine CLI  sends its certificate to the Engine daemon before issuing
commands. The daemon inspects the certificate, and because daemon trusts the CA,
the daemon automatically trusts any certificates signed by the CA. Assuming the
certificate is in order (the certificate has not expired or been revoked etc.)
the Engine daemon accepts commands from this trusted Engine CLI.&lt;/p&gt;

&lt;p&gt;The Docker Engine CLI is simply a client that uses the Docker Remote API to
communicate with the Engine daemon. Any client that uses this Docker Remote API can use
TLS. For example, other Engine clients such as Docker Universal Control Plane
(UCP) have TLS support built-in. Other, third party products, that use Docker&amp;rsquo;s
Remote API, can also be configured this way.&lt;/p&gt;

&lt;h2 id=&#34;tls-modes-with-docker-and-swarm&#34;&gt;TLS modes with Docker and Swarm&lt;/h2&gt;

&lt;p&gt;Now that you know how certificates are used by Docker Engine for authentication,
it&amp;rsquo;s important to be aware of the three TLS configurations possible with Docker
Engine and its clients:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;External 3rd party CA&lt;/li&gt;
&lt;li&gt;Internal corporate CA&lt;/li&gt;
&lt;li&gt;Self-signed certificates&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These configurations are differentiated by the type of entity acting as the  Certificate Authority (CA).&lt;/p&gt;

&lt;h3 id=&#34;external-3rd-party-ca&#34;&gt;External 3rd party CA&lt;/h3&gt;

&lt;p&gt;An external CA is a trusted 3rd party company that provides a means of creating,
issuing, revoking, and otherwise managing certificates. They are &lt;em&gt;trusted&lt;/em&gt; in
the sense that they have to fulfill specific conditions and maintain high levels
of security and business practices to win your business. You also have to
install the external CA&amp;rsquo;s root certificates for you computers and services to
&lt;em&gt;trust&lt;/em&gt; them.&lt;/p&gt;

&lt;p&gt;When you use an external 3rd party CA, they create, sign, issue, revoke and
otherwise manage your certificates. They normally charge a fee for these
services, but are considered an enterprise-class scalable solution that
provides a high degree of trust.&lt;/p&gt;

&lt;h3 id=&#34;internal-corporate-ca&#34;&gt;Internal corporate CA&lt;/h3&gt;

&lt;p&gt;Many organizations choose to implement their own Certificate Authorities and
PKI. Common examples are using OpenSSL and Microsoft Active Directory. In this
case, your company is its own Certificate Authority with all the work it
entails. The benefit is, as your own CA, you have more control over your PKI.&lt;/p&gt;

&lt;p&gt;Running your own CA and PKI requires you to provide all of the services offered
by external 3rd party CAs. These include creating, issuing, revoking, and
otherwise managing certificates. Doing all of this yourself has its own costs
and overheads. However, for a large corporation, it still may reduce costs in
comparison to using an external 3rd party service.&lt;/p&gt;

&lt;p&gt;Assuming you operate and manage your own internal CAs and PKI properly, an
internal, corporate CA  can be a highly scalable and highly secure option.&lt;/p&gt;

&lt;h3 id=&#34;self-signed-certificates&#34;&gt;Self-signed certificates&lt;/h3&gt;

&lt;p&gt;As the name suggests, self-signed certificates are certificates that are signed
with their own private key rather than a trusted CA. This is a low cost and
simple to use option. If you implement and manage self-signed certificates
correctly, they can be better than using no certificates.&lt;/p&gt;

&lt;p&gt;Because self-signed certificates lack of a full-blown PKI, they do not scale
well and lack many of the advantages offered by the other options. One of their
disadvantages is you cannot revoke self-signed certificates. Due to this, and
other limitations, self-signed certificates are considered the least secure of
the three options. Self-signed certificates are not recommended for public
facing production workloads exposed to untrusted networks.&lt;/p&gt;

&lt;h2 id=&#34;related-information&#34;&gt;Related information&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/configure-tls/&#34;&gt;Configure Docker Swarm for TLS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/engine/security/security/&#34;&gt;Docker security&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Plan for Swarm in production</title>
      <link>https://docs.docker.com/v1.10/swarm/plan-for-production/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.docker.com/v1.10/swarm/plan-for-production/</guid>
      <description>

&lt;h1 id=&#34;plan-for-swarm-in-production&#34;&gt;Plan for Swarm in production&lt;/h1&gt;

&lt;p&gt;This article provides guidance to help you plan, deploy, and manage Docker
Swarm clusters in business critical production environments. The following high
 level topics are covered:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#security&#34;&gt;Security&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#high-availability&#34;&gt;High Availability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#performance&#34;&gt;Performance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cluster-ownership&#34;&gt;Cluster ownership&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;security&#34;&gt;Security&lt;/h2&gt;

&lt;p&gt;There are many aspects to securing a Docker Swarm cluster. This section covers:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Authentication using TLS&lt;/li&gt;
&lt;li&gt;Network access control&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These topics are not exhaustive. They form part of a wider security architecture
that includes: security patching, strong password policies, role based access
control, technologies such as SELinux and AppArmor, strict auditing, and more.&lt;/p&gt;

&lt;h3 id=&#34;configure-swarm-for-tls&#34;&gt;Configure Swarm for TLS&lt;/h3&gt;

&lt;p&gt;All nodes in a Swarm cluster must bind their Docker Engine daemons to a network
port. This brings with it all of the usual network related security
implications such as man-in-the-middle attacks. These risks are compounded when
 the network in question is untrusted such as the internet. To mitigate these
risks, Swarm and the Engine support Transport Layer Security(TLS) for
authentication.&lt;/p&gt;

&lt;p&gt;The Engine daemons, including the Swarm manager, that are configured to use TLS
will only accept commands from Docker Engine clients that sign their
communications. The Engine and Swarm support external 3rd party Certificate
Authorities (CA) as well as internal corporate CAs.&lt;/p&gt;

&lt;p&gt;The default Engine and Swarm ports for TLS are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Engine daemon: 2376/tcp&lt;/li&gt;
&lt;li&gt;Swarm manager: 3376/tcp&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more information on configuring Swarm for TLS, see the &lt;a href=&#34;../v1.10/swarm/secure-swarm-tls/&#34;&gt;Overview Docker Swarm with TLS&lt;/a&gt; page.&lt;/p&gt;

&lt;h3 id=&#34;network-access-control&#34;&gt;Network access control&lt;/h3&gt;

&lt;p&gt;Production networks are complex, and usually locked down so that only allowed
traffic can flow on the network. The list below shows the network ports that
the different components of a Swam cluster listen on. You should use these to
configure your firewalls and other network access control lists.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Swarm manager.&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Inbound 80/tcp (HTTP)&lt;/strong&gt;. This is allows &lt;code&gt;docker pull&lt;/code&gt; commands to work. If you will be pulling from Docker Hub you will need to allow connections on port 80 from the internet.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inbound 2375/tcp&lt;/strong&gt;. This allows Docker Engine CLI commands direct to the Engine daemon.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inbound 3375/tcp&lt;/strong&gt;. This allows Engine CLI commands to the Swarm manager.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inbound 22/tcp&lt;/strong&gt;. This allows remote management via SSH&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Service Discovery&lt;/strong&gt;:

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Inbound 80/tcp (HTTP)&lt;/strong&gt;. This is allows &lt;code&gt;docker pull&lt;/code&gt; commands to work. If you will be pulling from Docker Hub you will need to allow connections on port 80 from the internet.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inbound *Discovery service port&lt;/strong&gt;*. This needs setting to the port that the backend discovery service listens on (consul, etcd, or zookeeper).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inbound 22/tcp&lt;/strong&gt;. This allows remote management via SSH&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Swarm nodes&lt;/strong&gt;:

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Inbound 80/tcp (HTTP)&lt;/strong&gt;. This is allows &lt;code&gt;docker pull&lt;/code&gt; commands to work. If you will be pulling from Docker Hub you will need to allow connections on port 80 from the internet.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inbound 2375/tcp&lt;/strong&gt;. This allows Engine CLI commands direct to the Docker daemon.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inbound 22/tcp&lt;/strong&gt;. This allows remote management via SSH.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Custom, cross-host container networks&lt;/strong&gt;:

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Inbound 7946/tcp&lt;/strong&gt; Allows for discovering other container networks.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inbound 7946/udp&lt;/strong&gt; Allows for discovering other container networks.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inbound &lt;store-port&gt;/tcp&lt;/strong&gt; Network key-value store service port.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;4789/udp&lt;/strong&gt; For the container overlay network.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If your firewalls and other network devices are connection state aware, they
will allow responses to established TCP connections. If your devices are not
state aware, you will need to open up ephemeral ports from 32768-65535. For
added security you can configure the ephemeral port rules to only allow
connections from interfaces on known Swarm devices.&lt;/p&gt;

&lt;p&gt;If your Swarm cluster is configured for TLS, replace &lt;code&gt;2375&lt;/code&gt; with &lt;code&gt;2376&lt;/code&gt;, and
&lt;code&gt;3375&lt;/code&gt; with &lt;code&gt;3376&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The ports listed above are just for Swarm cluster operations  such as; cluster
creation, cluster management, and scheduling of containers against the cluster.
 You may need to open additional network ports for application-related
communications.&lt;/p&gt;

&lt;p&gt;It is possible for different components of a Swarm cluster to exist on separate
networks. For example, many organizations operate separate management and
production networks. Some Docker Engine clients may exist on a management
network, while Swarm managers, discovery service instances, and nodes might
exist on one or more production networks. To offset against network failures,
you can deploy Swarm managers, discovery services, and nodes across multiple
production networks. In all of these cases you can use the list of ports above
to assist the work of your network infrastructure teams to efficiently and
securely configure your network.&lt;/p&gt;

&lt;h2 id=&#34;high-availability-ha&#34;&gt;High Availability (HA)&lt;/h2&gt;

&lt;p&gt;All production environments should be highly available, meaning they are
continuously operational over long periods of time. To achieve high
availability, an environment must the survive failures of its individual
component parts.&lt;/p&gt;

&lt;p&gt;The following sections discuss some technologies and best practices that can
enable you to build resilient, highly-available Swarm clusters. You can then use
these cluster to run your most demanding production applications and workloads.&lt;/p&gt;

&lt;h3 id=&#34;swarm-manager-ha&#34;&gt;Swarm manager HA&lt;/h3&gt;

&lt;p&gt;The Swarm manager is responsible for accepting all commands coming in to a Swarm
cluster, and scheduling resources against the cluster. If the Swarm manager
becomes unavailable, some cluster operations cannot be performed until the Swarm
manager becomes available again. This is unacceptable in large-scale business
critical scenarios.&lt;/p&gt;

&lt;p&gt;Swarm provides HA features to mitigate against possible failures of the Swarm
manager. You can use Swarm&amp;rsquo;s HA feature to configure multiple Swarm managers for
a single cluster. These Swarm managers operate in an active/passive formation
with a single Swarm manager being the &lt;em&gt;primary&lt;/em&gt;, and all others being
&lt;em&gt;secondaries&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Swarm secondary managers operate as &lt;em&gt;warm standby&amp;rsquo;s&lt;/em&gt;, meaning they run in the
background of the primary Swarm manager. The secondary Swarm managers are online
and accept commands issued to the cluster, just as the primary Swarm manager.
However, any commands received by the secondaries are forwarded to the primary
where they are executed. Should the primary Swarm manager fail, a new primary is
elected from the surviving secondaries.&lt;/p&gt;

&lt;p&gt;When creating HA Swarm managers, you should take care to distribute them over as
many &lt;em&gt;failure domains&lt;/em&gt; as possible. A failure domain is a network section that
can be negatively affected if a critical device or service experiences problems.
For example, if your cluster is running in the Ireland Region of Amazon Web
Services (eu-west-1) and you configure three Swarm managers (1 x primary, 2 x
secondary), you should place one in each availability zone as shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm2.staticflickr.com/1657/24581727611_0a076b79de_b.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In this configuration, the Swarm cluster can survive the loss of any two
availability zones. For your applications to survive such failures, they must be
architected across as many failure domains as well.&lt;/p&gt;

&lt;p&gt;For Swarm clusters serving high-demand, line-of-business applications, you
should have 3 or more Swarm managers. This configuration allows you to take one
manager down for maintenance, suffer an unexpected failure, and still continue
to manage and operate the cluster.&lt;/p&gt;

&lt;h3 id=&#34;discovery-service-ha&#34;&gt;Discovery service HA&lt;/h3&gt;

&lt;p&gt;The discovery service is a key component of a Swarm cluster. If the discovery
service becomes unavailable, this can prevent certain cluster operations. For
example, without a working discovery service, operations such as adding new
nodes to the cluster and making queries against the cluster configuration fail.
This is not acceptable in business critical production environments.&lt;/p&gt;

&lt;p&gt;Swarm supports four backend discovery services:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Hosted (not for production use)&lt;/li&gt;
&lt;li&gt;Consul&lt;/li&gt;
&lt;li&gt;etcd&lt;/li&gt;
&lt;li&gt;Zookeeper&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Consul, etcd, and Zookeeper are all suitable for production, and should be
configured for high availability. You should use each service&amp;rsquo;s existing tools
and best practices to configure these for HA.&lt;/p&gt;

&lt;p&gt;For Swarm clusters serving high-demand, line-of-business applications, it is
recommended to have 5 or more discovery service instances. This due to the
replication/HA technologies they use (such as Paxos/Raft) requiring a strong
quorum. Having 5 instances allows you to take one down for maintenance, suffer
an unexpected failure, and still be able to achieve a strong quorum.&lt;/p&gt;

&lt;p&gt;When creating a highly available Swarm discovery service, you should take care
to distribute each discovery service instance over as many failure domains as
possible. For example, if your cluster is running in the Ireland Region of
Amazon Web Services (eu-west-1) and you configure three discovery service
 instances, you should place one in each availability zone.&lt;/p&gt;

&lt;p&gt;The diagram below shows a Swarm cluster configured for HA. It has three Swarm
managers and three discovery service instances spread over three failure
domains (availability zones). It also has Swarm nodes balanced across all three
 failure domains. The loss of two availability zones in the configuration shown
 below does not cause the Swarm cluster to go down.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm2.staticflickr.com/1675/24380252320_999687d2bb_b.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It is possible to share the same Consul, etcd, or Zookeeper containers between
the Swarm discovery and Engine container networks. However, for best
performance and availability you should deploy dedicated instances &amp;ndash; a
discovery instance for Swarm and another for your container networks.&lt;/p&gt;

&lt;h3 id=&#34;multiple-clouds&#34;&gt;Multiple clouds&lt;/h3&gt;

&lt;p&gt;You can architect and build Swarm clusters that stretch across multiple cloud
providers, and even across public cloud and on premises infrastructures. The
diagram below shows an example Swarm cluster stretched across AWS and Azure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm2.staticflickr.com/1493/24676269945_d19daf856c_b.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;While such architectures may appear to provide the ultimate in availability,
there are several factors to consider. Network latency can be problematic, as
can partitioning. As such, you should seriously consider technologies that
provide reliable, high speed, low latency connections into these cloud
platforms &amp;ndash; technologies such as AWS Direct Connect and Azure
ExpressRoute.&lt;/p&gt;

&lt;p&gt;If you are considering a production deployment across multiple infrastructures
like this, make sure you have good test coverage over your entire system.&lt;/p&gt;

&lt;h3 id=&#34;isolated-production-environments&#34;&gt;Isolated production environments&lt;/h3&gt;

&lt;p&gt;It is possible to run multiple environments, such as development, staging, and
production, on a single Swarm cluster. You accomplish this by tagging Swarm
nodes and using constraints to filter containers onto nodes tagged as
&lt;code&gt;production&lt;/code&gt; or &lt;code&gt;staging&lt;/code&gt; etc. However, this is not recommended. The recommended
approach is to air-gap production environments, especially high performance
business critical production environments.&lt;/p&gt;

&lt;p&gt;For example, many companies not only deploy dedicated isolated infrastructures
for production &amp;ndash; such as networks, storage, compute and other systems.
They also deploy separate management systems and policies. This results in
things like users having separate accounts for logging on to production systems
etc. In these types of environments, it is mandatory to deploy dedicated
production Swarm clusters that operate on the production hardware infrastructure
and follow thorough production management, monitoring, audit and other policies.&lt;/p&gt;

&lt;h3 id=&#34;operating-system-selection&#34;&gt;Operating system selection&lt;/h3&gt;

&lt;p&gt;You should give careful consideration to the operating system that your Swarm
infrastructure relies on. This consideration is vital for production
environments.&lt;/p&gt;

&lt;p&gt;It is not unusual for a company to use one operating system in development
environments, and a different one in production. A common example of this is to
use CentOS in development environments, but then to use Red Hat Enterprise Linux
(RHEL) in production. This decision is often a balance between cost and support.
CentOS Linux can be downloaded and used for free, but commercial support options
are few and far between. Whereas RHEL has an associated support and license
cost, but comes with world class commercial support from Red Hat.&lt;/p&gt;

&lt;p&gt;When choosing the production operating system to use with your Swarm clusters,
you should choose one that closely matches what you have used in development and
staging environments. Although containers abstract much of the underlying OS,
some things are mandatory. For example, Docker container networks require Linux
kernel 3.16 or higher. Operating a 4.x kernel in development and staging and
then 3.14 in production will certainly cause issues.&lt;/p&gt;

&lt;p&gt;You should also consider procedures and channels for deploying and potentially
patching your production operating systems.&lt;/p&gt;

&lt;h2 id=&#34;performance&#34;&gt;Performance&lt;/h2&gt;

&lt;p&gt;Performance is critical in environments that support business critical line of
business applications. The following sections discuss some technologies and
best practices that can help you build high performance Swarm clusters.&lt;/p&gt;

&lt;h3 id=&#34;container-networks&#34;&gt;Container networks&lt;/h3&gt;

&lt;p&gt;Docker Engine container networks are overlay networks and can be created across
multiple Engine hosts. For this reason, a container network requires a key-value
(KV) store to maintain network configuration and state. This KV store can be
shared in common with the one used by the Swarm cluster discovery service.
However, for best performance and fault isolation, you should deploy individual
KV store instances for container networks and Swarm discovery. This is
especially so in demanding business critical production environments.&lt;/p&gt;

&lt;p&gt;Engine container networks also require version 3.16 or higher of the Linux
kernel. Higher kernel versions are usually preferred, but carry an increased
risk of instability because of the newness of the kernel. Where possible, you
should use a kernel version that is already approved for use in your production
environment. If you do not have a 3.16 or higher Linux kernel version approved
for production, you should begin the process of getting one as early as
possible.&lt;/p&gt;

&lt;h3 id=&#34;scheduling-strategies&#34;&gt;Scheduling strategies&lt;/h3&gt;

&lt;!-- NIGEL: This reads like an explanation of specific scheduling strategies rather than guidance on which strategy to pick for production or with consideration of a production architecture choice.  For example, is spread a problem in a multiple clouds or random not good for XXX type application for YYY reason?

Or perhaps there is nothing to consider when it comes to scheduling strategy and network / HA architecture, application, os choice etc. that good?

--&gt;

&lt;p&gt;Scheduling strategies are how Swarm decides which nodes on a cluster to start
containers on. Swarm supports the following strategies:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;spread&lt;/li&gt;
&lt;li&gt;binpack&lt;/li&gt;
&lt;li&gt;random (not for production use)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can also write your own.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Spread&lt;/strong&gt; is the default strategy. It attempts to balance the number of
containers evenly across all nodes in the cluster. This is a good choice for
high performance clusters, as it spreads container workload across all
resources in the cluster. These resources include CPU, RAM, storage, and
network bandwidth.&lt;/p&gt;

&lt;p&gt;If your Swarm nodes are balanced across multiple failure domains, the spread
strategy evenly balance containers across those failure domains. However,
spread on its own is not aware of the roles of any of those containers, so has
no inteligence to spread multiple instances of the same service across failure
domains. To achieve this you should use tags and constraints.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;binpack&lt;/strong&gt; strategy runs as many containers as possible on a node,
effectively filling it up, before scheduling containers on the next node.&lt;/p&gt;

&lt;p&gt;This means that binpack does not use all cluster resources until the cluster
fills up. As a result, applications running on Swarm clusters that operate the
binpack strategy might not perform as well as those that operate the spread
strategy. However, binpack is a good choice for minimizing infrastructure
requirements and cost. For example, imagine you have a 10-node cluster where
each node has 16 CPUs and 128GB of RAM. However, your container workload across
 the entire cluster is only using the equivalent of 6 CPUs and 64GB RAM. The
spread strategy would balance containers across all nodes in the cluster.
However, the binpack strategy would fit all containers on a single node,
potentially allowing you turn off the additional nodes and save on cost.&lt;/p&gt;

&lt;h2 id=&#34;ownership-of-swarm-clusters&#34;&gt;Ownership of Swarm clusters&lt;/h2&gt;

&lt;p&gt;The question of ownership is vital in production environments. It is therefore
vital that you consider and agree on all of the following when planning,
documenting, and deploying your production Swarm clusters.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Who&amp;rsquo;s budget does the production Swarm infrastructure come out of?&lt;/li&gt;
&lt;li&gt;Who owns the accounts that can administer and manage the production Swarm
cluster?&lt;/li&gt;
&lt;li&gt;Who is responsible for monitoring the production Swarm infrastructure?&lt;/li&gt;
&lt;li&gt;Who is responsible for patching and upgrading the production Swarm
infrastructure?&lt;/li&gt;
&lt;li&gt;On-call responsibilities and escalation procedures?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The above is not a complete list, and the answers to the questions will vary
depending on how your organization&amp;rsquo;s and team&amp;rsquo;s are structured. Some companies
are along way down the DevOps route, while others are not. Whatever situation
your company is in, it is important that you factor all of the above into the
planning, deployment, and ongoing management of your production Swarm clusters.&lt;/p&gt;

&lt;h2 id=&#34;related-information&#34;&gt;Related information&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/swarm_at_scale/&#34;&gt;Try Swarm at scale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/networking/&#34;&gt;Swarm and container networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../v1.10/swarm/multi-manager-setup/&#34;&gt;High availability in Docker Swarm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.docker.com/products/docker-universal-control-plane&#34;&gt;Universal Control plane&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>